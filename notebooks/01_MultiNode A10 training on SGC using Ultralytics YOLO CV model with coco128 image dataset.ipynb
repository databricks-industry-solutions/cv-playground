{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5da57367-ba6b-4f58-9304-ecf5ba3ec2bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%markdown\n",
    "# Ultralytics YOLO Training on Databricks Serverless GPU Compute (SGC)\n",
    "\n",
    "This notebook demonstrates how to set up and run distributed YOLO model training using Databricks Serverless GPU Compute with proper resource management and MLflow integration.\n",
    "\n",
    "## ðŸš€ Overview\n",
    "\n",
    "This implementation leverages:\n",
    "- **Databricks Serverless GPU Compute (SGC)** for scalable, on-demand GPU resources\n",
    "- **Ultralytics YOLO v11** for state-of-the-art object detection\n",
    "- **Distributed Training** with PyTorch DDP and NCCL backend\n",
    "- **MLflow** for experiment tracking and model management\n",
    "- **Unity Catalog Volumes** for persistent data storage\n",
    "\n",
    "## ðŸ“‹ Prerequisites\n",
    "\n",
    "### Environment Requirements\n",
    "- Databricks Runtime with GPU support\n",
    "- Access to Serverless GPU Compute\n",
    "- Unity Catalog enabled workspace\n",
    "- MLflow experiment tracking permissions\n",
    "\n",
    "### Required Packages\n",
    "```python\n",
    "ultralytics==8.3.204\n",
    "mlflow>=3.0\n",
    "nvidia-ml-py==13.580.82  # GPU monitoring\n",
    "pyrsmi==0.2.0           # AMD GPU support (optional)\n",
    "threadpoolctl==3.1.0\n",
    "```\n",
    "\n",
    "## ðŸ—ï¸ Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Databricks SGC Cluster                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  GPU 0    â”‚  GPU 1    â”‚  GPU 2    â”‚  ...    â”‚  GPU N-1    â”‚\n",
    "â”‚  Rank 0   â”‚  Rank 1   â”‚  Rank 2   â”‚  ...    â”‚  Rank N-1   â”‚\n",
    "â”‚  (Master) â”‚           â”‚           â”‚         â”‚             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Unity Catalog Volume                     â”‚\n",
    "â”‚  /Volumes/catalog/schema/volume/                           â”‚\n",
    "â”‚  â”œâ”€â”€ data/           # Training datasets                   â”‚\n",
    "â”‚  â”œâ”€â”€ raw_model/      # Pre-trained models                 â”‚\n",
    "â”‚  â””â”€â”€ training_runs/  # Output artifacts                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      MLflow Tracking                       â”‚\n",
    "â”‚  â€¢ Experiment logging                                      â”‚\n",
    "â”‚  â€¢ Model versioning                                        â”‚\n",
    "â”‚  â€¢ Metrics & artifacts                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ðŸ”§ Setup Instructions\n",
    "\n",
    "### 1. Environment Setup\n",
    "\n",
    "First, use A10 SGC with Env version 4, then install required packages and restart Python runtime:\n",
    "\n",
    "```python\n",
    "%pip install -U mlflow>=3.0\n",
    "%pip install ultralytics==8.3.204\n",
    "%pip install nvidia-ml-py==13.580.82\n",
    "dbutils.library.restartPython()\n",
    "```\n",
    "\n",
    "### 2. Unity Catalog Configuration\n",
    "\n",
    "Create necessary catalog structure:\n",
    "\n",
    "```sql\n",
    "CREATE CATALOG IF NOT EXISTS your_catalog;\n",
    "CREATE SCHEMA IF NOT EXISTS your_catalog.computer_vision;\n",
    "CREATE VOLUME IF NOT EXISTS your_catalog.computer_vision.yolo;\n",
    "```\n",
    "\n",
    "### 3. Data Preparation\n",
    "\n",
    "**Supported Dataset Formats:**\n",
    "- COCO format (recommended)\n",
    "- YOLO format\n",
    "- Custom annotations\n",
    "\n",
    "**Directory Structure:**\n",
    "```\n",
    "/Volumes/catalog/schema/volume/\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â””â”€â”€ dataset_name/\n",
    "â”‚       â”œâ”€â”€ images/\n",
    "â”‚       â”‚   â”œâ”€â”€ train/\n",
    "â”‚       â”‚   â””â”€â”€ val/\n",
    "â”‚       â”œâ”€â”€ labels/\n",
    "â”‚       â”‚   â”œâ”€â”€ train/\n",
    "â”‚       â”‚   â””â”€â”€ val/\n",
    "â”‚       â””â”€â”€ data.yaml\n",
    "â”œâ”€â”€ raw_model/\n",
    "â”‚   â””â”€â”€ yolo11n.pt\n",
    "â””â”€â”€ training_runs/\n",
    "```\n",
    "\n",
    "### 4. MLflow Configuration\n",
    "\n",
    "Set up experiment tracking:\n",
    "\n",
    "```python\n",
    "experiment_name = \"/Users/your.email@company.com/YOLO_Experiments\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
    "```\n",
    "\n",
    "## ðŸš€ Distributed Training\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Automatic GPU Detection**: Dynamically scales across available GPUs\n",
    "2. **Fault Tolerance**: Proper cleanup with try/finally blocks\n",
    "3. **Resource Management**: Prevents NCCL process group leaks\n",
    "4. **MLflow Integration**: Automatic logging of metrics, models, and artifacts\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "```python\n",
    "@distributed(gpus=8, gpu_type='A10', remote=True)\n",
    "def train_fn(world_size=None, parent_run_id=None):\n",
    "    try:\n",
    "        # Setup distributed environment\n",
    "        rank, world_size, device = setup()\n",
    "        \n",
    "        # Initialize YOLO model\n",
    "        model = YOLO(\"yolo11n.pt\")\n",
    "        \n",
    "        # Start training with optimized parameters\n",
    "        model.train(\n",
    "            task=\"detect\",\n",
    "            batch=16,                    # Adjust based on GPU memory\n",
    "            device=[LOCAL_RANK],\n",
    "            data=data_yaml_path,\n",
    "            epochs=100,\n",
    "            project=training_output_path,\n",
    "            exist_ok=True,\n",
    "            # Data augmentation\n",
    "            fliplr=1,                   # Horizontal flip probability\n",
    "            flipud=1,                   # Vertical flip probability  \n",
    "            perspective=0.001,          # Perspective transformation\n",
    "            degrees=0.45                # Rotation degrees\n",
    "        )\n",
    "        \n",
    "        # Validation and export (rank 0 only)\n",
    "        if RANK in (0, -1):\n",
    "            model.val()\n",
    "            model.export()\n",
    "            \n",
    "    finally:\n",
    "        # Critical: Always cleanup to prevent resource leaks\n",
    "        cleanup()\n",
    "```\n",
    "\n",
    "## ðŸ“Š Monitoring & Debugging\n",
    "\n",
    "### Environment Variables\n",
    "\n",
    "```python\n",
    "# Debugging options\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"  # Set to \"1\" for debugging\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"         # NCCL communication logs\n",
    "os.environ[\"NCCL_DEBUG_SUBSYS\"] = \"ALL\"   # Detailed NCCL debugging\n",
    "```\n",
    "\n",
    "### MLflow System Metrics\n",
    "\n",
    "Enable comprehensive system monitoring:\n",
    "\n",
    "```python\n",
    "os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
    "```\n",
    "\n",
    "This captures:\n",
    "- GPU utilization and memory\n",
    "- CPU usage and memory\n",
    "- Network I/O\n",
    "- Disk I/O\n",
    "\n",
    "## ðŸ” Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **NCCL Process Group Warning**\n",
    "   ```\n",
    "   ProcessGroupNCCL.cpp:1479] Warning: destroy_process_group() was not called\n",
    "   ```\n",
    "   **Solution**: Ensure `cleanup()` is called in finally block\n",
    "\n",
    "2. **CUDA Out of Memory**\n",
    "   ```\n",
    "   RuntimeError: CUDA out of memory\n",
    "   ```\n",
    "   **Solution**: Reduce batch size or use gradient accumulation\n",
    "\n",
    "3. **Distributed Training Hangs**\n",
    "   **Solution**: Check NCCL environment variables and network connectivity\n",
    "\n",
    "### Debug Commands\n",
    "\n",
    "```python\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Verify distributed setup\n",
    "print(f\"Rank: {RANK}, Local Rank: {LOCAL_RANK}\")\n",
    "print(f\"World Size: {dist.get_world_size() if dist.is_initialized() else 'Not initialized'}\")\n",
    "```\n",
    "\n",
    "## ðŸ“š References\n",
    "\n",
    "- [Ultralytics YOLO Documentation](https://docs.ultralytics.com/)\n",
    "- [Databricks Serverless GPU Compute](https://docs.databricks.com/en/compute/serverless-gpu.html)\n",
    "- [PyTorch Distributed Training](https://pytorch.org/docs/stable/distributed.html)\n",
    "- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)\n",
    "- [Unity Catalog Volumes](https://docs.databricks.com/en/catalog/volumes.html)\n",
    "\n",
    "## ðŸ·ï¸ Model Versioning\n",
    "\n",
    "This setup automatically:\n",
    "- Logs training metrics to MLflow\n",
    "- Saves model artifacts to Unity Catalog\n",
    "- Creates model signatures for deployment\n",
    "- Tracks hyperparameters and dataset versions\n",
    "\n",
    "## ðŸš¦ Best Practices\n",
    "\n",
    "1. **Resource Management**: Always use try/finally for cleanup\n",
    "2. **Data Location**: Use Unity Catalog Volumes for overall governance\n",
    "3. **Batch Size**: Start with smaller batches and scale up\n",
    "4. **Monitoring**: Enable MLflow system metrics\n",
    "5. **Checkpointing**: Save intermediate results for long training runs (under /tmp/)\n",
    "6. **Validation**: Run validation on rank 0 only to avoid conflicts\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: Run the cells below to start your YOLO training pipeline! ðŸŽ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8b9120-1b2e-4f47-85b8-26f4ee90bda4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import serverless_gpu\n",
    "%pip install -U mlflow>=3.0\n",
    "%pip install threadpoolctl==3.1.0\n",
    "%pip install ultralytics==8.3.204\n",
    "%pip install nvidia-ml-py==13.580.82 # for later mlflow GPU monitoring\n",
    "%pip install pyrsmi==0.2.0 # for later mlflow AMD GPU monitoring if you have AMD\n",
    "\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae4f83f4-ba4c-4aac-b4f2-3444f106388d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from serverless_gpu import distributed\n",
    "import mlflow\n",
    "\n",
    "\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import mlflow\n",
    "import torch.distributed as dist\n",
    "from ultralytics import settings\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from ultralytics.utils import RANK, LOCAL_RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42531dbd-3ea0-4570-ba97-35dc3c03058d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%cat /tmp/Ultralytics/settings.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c531b751-2f78-4dc6-96dc-309b003cb196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup I/O Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d14e30-c744-4232-bf5f-8ad34d9f26e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists yyang;\n",
    "create schema if not exists yyang.computer_vision;\n",
    "create volume if not exists yyang.computer_vision.yolo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c934f1e5-273f-4448-ad06-984749ce9d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "project_location = '/Volumes/yyang/computer_vision/yolo/'\n",
    "os.makedirs(f'{project_location}/training_runs/', exist_ok=True)\n",
    "os.makedirs(f'{project_location}/data/', exist_ok=True)\n",
    "os.makedirs(f'{project_location}/raw_model/', exist_ok=True)\n",
    "\n",
    "# volume folder in UC.\n",
    "volume_project_location = f'{project_location}/training_results/'\n",
    "os.makedirs(volume_project_location, exist_ok=True)\n",
    "\n",
    "# or alternatively, ephemeral /tmp/ project location on VM\n",
    "tmp_project_location = \"/tmp/training_results/\"\n",
    "os.makedirs(tmp_project_location, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcb57faa-8cd4-4efe-bc23-88aebc793fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Image Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d365dc7c-dfda-4b2d-a892-551c92acad16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('/Workspace/' + dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get().rsplit('/', 1)[0])\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d1e829-1767-401e-8fcb-6a405b2afb75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# curl -L https://github.com/ultralytics/ultralytics/raw/main/ultralytics/cfg/datasets/coco8.yaml -o coco8.yaml\n",
    "curl -L https://github.com/ultralytics/ultralytics/raw/main/ultralytics/cfg/datasets/coco128.yaml -o coco128.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9613b8d3-aa50-4c4b-90e6-a45581976941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# cat ./coco8.yaml\n",
    "cat ./coco128.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "359206f2-de4f-4cc5-acb5-aae5bc8cfb90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "__REMEMBER: change below cell path for your data.yaml as input to YOLO train later__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e224cdc1-1bf0-46d7-9a59-a40642bf95b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# with open('coco8.yaml', 'r') as file:\n",
    "with open('coco128.yaml', 'r') as file:\n",
    "\n",
    "    data = file.read()\n",
    "\n",
    "#: here specific for this dataset, we have to update the .yaml file with real I/O locations.\n",
    "os.makedirs(f'{project_location}/data/coco128', exist_ok=True)\n",
    "data = data.replace('path: coco128', f'path: {project_location}data/coco128')\n",
    "\n",
    "\n",
    "# with open('coco8.yaml', 'w') as file:\n",
    "with open('coco128.yaml', 'w') as file:\n",
    "\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "010c5888-7c1c-4bd2-8f2a-7ec5ed1787c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# cat ./coco8.yaml\n",
    "cat ./coco128.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c1c0f6-50c9-47bb-8610-89a13cda17fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# with open('./coco8.yaml', 'r') as file:\n",
    "with open('./coco128.yaml', 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aef5597-e8f6-4bba-b01b-6b63424f114a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import tarfile\n",
    "# import io\n",
    "\n",
    "# response = requests.get(data['download'])\n",
    "# tar = tarfile.open(fileobj=io.BytesIO(response.content), mode='r:gz')\n",
    "# tar.extractall(path=data['path'])\n",
    "# tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ffda763-fe59-4d0c-9422-c42f9a0ffdea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract Zip File from URL and Save to Path"
    }
   },
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "response = requests.get(data['download'])\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "extraction_path = '/'.join(data['path'].split('/')[:-1]) # do this since we dont want to duplicate the \"/coco128/\" part twice in the final path.\n",
    "print(extraction_path)\n",
    "z.extractall(extraction_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d5636b3-8ede-46af-a89f-bd0ac0842ab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7e6010-1b3a-465a-995a-a794541cc5d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "also move coco128.yaml file to there"
    }
   },
   "outputs": [],
   "source": [
    "# %sh\n",
    "# mv ./coco128.yaml /Volumes/yyang/computer_vision/yolo/data/\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "data_yaml_path = f'{extraction_path}/coco128.yaml'\n",
    "print('data_yaml_path is:', data_yaml_path)\n",
    "\n",
    "if os.path.exists(data_yaml_path):\n",
    "    os.remove(data_yaml_path)\n",
    "shutil.move('./coco128.yaml', data_yaml_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d829ce2-835a-4d9c-92fb-742c991e4a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup Mlflow Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da48cc45-99f1-4959-a96a-d1ab35382f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import mlflow\n",
    "import torch.distributed as dist\n",
    "from ultralytics import settings\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.models.signature import ModelSignature\n",
    "\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(\"string\", \"image_source\"),\n",
    "    ]\n",
    ")\n",
    "output_schema = Schema([ColSpec(\"string\",\"class_name\"),\n",
    "                        ColSpec(\"integer\",\"class_num\"),\n",
    "                        ColSpec(\"double\",\"confidence\")]\n",
    "                       )\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, \n",
    "                           outputs=output_schema)\n",
    "\n",
    "# settings.update({\"mlflow\":False}) # Specifically, it disables the integration with MLflow. By setting the mlflow key to False, you are instructing the ultralytics library not to use MLflow for logging or tracking experiments.\n",
    "\n",
    "# ultralytics level setting with MLflow\n",
    "settings.update({\"mlflow\":True}) # if you do want to autolog.\n",
    "# # Config MLflow\n",
    "mlflow.autolog(disable=True)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659998a6-0118-4ca3-9e27-fab85a303cbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "experiment_name = \"/Users/yang.yang@databricks.com/SGC_YOLO_Test/Experiments_YOLO_CoCo\"\n",
    "\n",
    "os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
    "print(f\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING set to {os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING']}\")\n",
    "\n",
    "os.environ['MLFLOW_EXPERIMENT_NAME'] = experiment_name\n",
    "print(f\"MLFLOW_EXPERIMENT_NAME set to {os.environ['MLFLOW_EXPERIMENT_NAME']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7114dd4c-32ed-4f43-912f-a54f597e9879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name)\n",
    "experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "# Reset MLFLOW_RUN_ID, so we dont bump into the wrong one.\n",
    "if 'MLFLOW_RUN_ID' in os.environ:\n",
    "    del os.environ['MLFLOW_RUN_ID']\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id) as parent_run:\n",
    "    active_run_id = mlflow.last_active_run().info.run_id\n",
    "    active_run_name = mlflow.last_active_run().info.run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9cb51d8-ee01-4a8f-acda-920d46ab7023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(experiment_name, experiment_id, active_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17a6ea3-d611-4f79-9546-463b7941191e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test if loadable"
    }
   },
   "outputs": [],
   "source": [
    "YOLO(\"yolo11n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76050ba2-5a74-4804-a5a7-98e607a17586",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "(optional) if you need to manually give the yaml path"
    }
   },
   "outputs": [],
   "source": [
    "# data_yaml_path = \"./coco128.yaml\" # ref: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco128.yaml\n",
    "\n",
    "data_yaml_path = '/Volumes/yyang/computer_vision/yolo/data/coco128.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2ce4204-8ab8-41ba-83f0-6ef73d992ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Start to Train using SGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9ac468-04cd-4bcb-8b2c-525a5a114bed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "helper functions"
    }
   },
   "outputs": [],
   "source": [
    "def setup():\n",
    "    \"\"\"Initialize the distributed training process group\"\"\"\n",
    "    # Check if we're in a distributed environment\n",
    "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        rank = int(os.environ['RANK'])\n",
    "        world_size = int(os.environ['WORLD_SIZE'])\n",
    "        local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "    else:\n",
    "        # Fallback for single GPU\n",
    "        rank = 0\n",
    "        world_size = 1\n",
    "        local_rank = 0\n",
    "\n",
    "    # Initialize process group\n",
    "    if world_size > 1:\n",
    "        if not dist.is_initialized():\n",
    "            dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)\n",
    "\n",
    "    # Set device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(f'cuda:{local_rank}')\n",
    "        torch.cuda.set_device(device)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    return rank, world_size, device\n",
    "  \n",
    "def cleanup():\n",
    "    \"\"\"Clean up the distributed training process group\"\"\"\n",
    "    if dist.is_initialized():\n",
    "        dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d142d022-f643-4279-ac01-37626f5ba276",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "single local gpu test with minimal example"
    }
   },
   "outputs": [],
   "source": [
    "model = YOLO(f\"{project_location}/raw_model/yolo11n.pt\")\n",
    "model.train(\n",
    "    task=\"detect\",\n",
    "    batch=16, # Batch size, with three modes: set as an integer (e.g., batch=16), auto mode for 60% GPU memory utilization (batch=-1), or auto mode with specified utilization fraction (batch=0.70).\n",
    "    device=-1, # need to be LOCAL_RANK, i.e., 0 for this case since we already init_process_group beforehand. RANK wont work. There is no need to specify [0,1] given for example if we have 2 GPUs per node. [0,1] with world_size of 4 or 2 beforehand will both fail. \n",
    "    data=data_yaml_path,\n",
    "    epochs=20,\n",
    "    project=f'{tmp_project_location}', # local VM ephermal location\n",
    "    # project=f'{volume_project_location}', # volume path still wont work\n",
    "    exist_ok=True,\n",
    "    fliplr=1,\n",
    "    flipud=1,\n",
    "    perspective=0.001,\n",
    "    degrees=.45\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f27e1bbb-d077-4dc4-95e4-d8676d97eb97",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "working - v1a with 4 GPUs vs prior 8"
    }
   },
   "outputs": [],
   "source": [
    "settings.update({\"mlflow\":True}) # if you do want to autolog.\n",
    "mlflow.autolog(disable = False)\n",
    "\n",
    "print('data_yaml_path is:', data_yaml_path)\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "@distributed(gpus=4, gpu_type='A10', remote=True)\n",
    "#: -----------worker func: this function is visible to each GPU device.-------------------\n",
    "def train_fn(world_size = None, parent_run_id = None):\n",
    "\n",
    "\n",
    "    # import os\n",
    "    # from ultralytics import YOLO\n",
    "    # import torch\n",
    "    # import mlflow\n",
    "    # import torch.distributed as dist\n",
    "    # from ultralytics import settings\n",
    "    # from mlflow.types.schema import Schema, ColSpec\n",
    "    # from mlflow.models.signature import ModelSignature\n",
    "    from ultralytics.utils import RANK, LOCAL_RANK\n",
    "\n",
    "    # Setup distributed training\n",
    "    rank, world_size, device = setup()\n",
    "\n",
    "    print(f\"Rank: {rank}, World Size: {world_size}, Device: {device}\")\n",
    "    print(f\"Rank: {RANK}, World Size: {world_size}, Device: {LOCAL_RANK}\")\n",
    "\n",
    "\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "\n",
    "    ############################\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\" # use 1 for synchronization operation, debugging model prefers this.\n",
    "    os.environ[\"NCCL_DEBUG\"] = \"INFO\" # \"WARN\" # for more debugging info on the NCCL side.\n",
    "    os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
    "    os.environ['MLFLOW_EXPERIMENT_NAME'] = experiment_name\n",
    "    # We set the experiment details here\n",
    "    experiment = mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    # # #: from repo issue https://github.com/ultralytics/ultralytics/issues/11680\n",
    "    # ## conclusion: doesn't work, has error :\"ValueError: Invalid CUDA 'device=0,1' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\"\n",
    "    # # torch.backends.cudnn.benchmark = False\n",
    "    # # torch.cuda.synchronize()\n",
    "    # print(f\"------Before init_process_group, we have: {RANK=} -- {LOCAL_RANK=}------\")\n",
    "    # dist.init_process_group(\n",
    "    #     backend=\"nccl\",\n",
    "    #     init_method=\"env://\",\n",
    "    #     world_size=world_size,\n",
    "    #     rank=RANK, # this must be from 0 to world_size - 1. LOCAL_RANK wont work.\n",
    "    # )\n",
    "    # print(f\"------After init_process_group, we have: {RANK=} -- {LOCAL_RANK=}------\")\n",
    "\n",
    "    print('data_yaml_path is:', data_yaml_path)\n",
    "    #\n",
    "    # with mlflow.start_run(run_id=parent_run_id):\n",
    "    with mlflow.start_run():\n",
    "        # model = YOLO(f\"yolov11n.pt\") # shared location\n",
    "        model = YOLO(f\"{project_location}/raw_model/yolo11n.pt\")\n",
    "        model.train(\n",
    "            task=\"detect\",\n",
    "            batch=16, # Batch size, with three modes: set as an integer (e.g., batch=16), auto mode for 60% GPU memory utilization (batch=-1), or auto mode with specified utilization fraction (batch=0.70).\n",
    "            device=[LOCAL_RANK], # need to be LOCAL_RANK, i.e., 0 for this case since we already init_process_group beforehand. RANK wont work. There is no need to specify [0,1] given for example if we have 2 GPUs per node. [0,1] with world_size of 4 or 2 beforehand will both fail. \n",
    "            data=data_yaml_path,\n",
    "            epochs=20,\n",
    "            project=f'{tmp_project_location}', # local VM ephermal location\n",
    "            # project=f'{volume_project_location}', # volume path still wont work\n",
    "            exist_ok=True,\n",
    "            fliplr=1,\n",
    "            flipud=1,\n",
    "            perspective=0.001,\n",
    "            degrees=.45\n",
    "        )\n",
    "        success = None\n",
    "        if RANK in (0, -1):\n",
    "            success = model.val()\n",
    "            if success:\n",
    "                model.export() # ref: https://docs.ultralytics.com/modes/export/#introduction\n",
    "        \n",
    "\n",
    "    active_run_id = mlflow.last_active_run().info.run_id\n",
    "    print(\"For YOLO autologging, active_run_id is: \", active_run_id)\n",
    "\n",
    "    # after training is done.\n",
    "    if not dist.is_initialized():\n",
    "      # import torch.distributed as dist\n",
    "      dist.init_process_group(\"nccl\")\n",
    "\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    global_rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    print(f\"------After training, we have: RANK:{global_rank=} -- LOCAL_RANK:{local_rank=} -- world_size: {world_size=}------\")\n",
    "\n",
    "    if global_rank == 0:\n",
    "        with mlflow.start_run(run_id=active_run_id) as run:\n",
    "            mlflow.log_artifact(data_yaml_path, \"input_data_yaml\")\n",
    "            # mlflow.log_dict(data, \"data.yaml\")\n",
    "            mlflow.log_params({\"rank\":global_rank})\n",
    "            mlflow.pytorch.log_model(YOLO(str(model.trainer.best)), \"model\", signature=signature) # this succeeded\n",
    "\n",
    "    # clean up\n",
    "    cleanup()\n",
    "\n",
    "    return \"finished\" # can return any picklable object\n",
    "\n",
    "\n",
    "train_fn.distributed(world_size = None, parent_run_id = None) # now can program can run without specifying manually the parameters of world_size and parent_run_id. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f734b7f-d542-4fff-9983-2fe8400cfcdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Note: I want to test more than 8 GPUs, but currently the dogfood has 8 A10 limitations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01870009-b544-4b31-a179-a2b66b9d3570",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "cleaned version - v2a with 8 GPUs vs prior 8 to test more nodes scalability"
    }
   },
   "outputs": [],
   "source": [
    "settings.update({\"mlflow\":True}) # if you do want to autolog.\n",
    "mlflow.autolog(disable = False)\n",
    "\n",
    "print('data_yaml_path is:', data_yaml_path)\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "@distributed(gpus=8, gpu_type='A10', remote=True)\n",
    "#: -----------worker func: this function is visible to each GPU device.-------------------\n",
    "def train_fn(world_size = None, parent_run_id = None):\n",
    "    try:\n",
    "        from ultralytics.utils import RANK, LOCAL_RANK\n",
    "\n",
    "        # Setup distributed training\n",
    "        rank, world_size, device = setup()\n",
    "\n",
    "        print(f\"Rank: {rank}, World Size: {world_size}, Device: {device}\")\n",
    "        print(f\"Rank: {RANK}, World Size: {world_size}, Device: {LOCAL_RANK}\")\n",
    "\n",
    "\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "            print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "\n",
    "        ############################\n",
    "        os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\" # use 1 for synchronization operation, debugging model prefers this.\n",
    "        os.environ[\"NCCL_DEBUG\"] = \"INFO\" # \"WARN\" # for more debugging info on the NCCL side.\n",
    "        os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
    "        os.environ['MLFLOW_EXPERIMENT_NAME'] = experiment_name\n",
    "        # We set the experiment details here\n",
    "        experiment = mlflow.set_experiment(experiment_name)\n",
    "        print('data_yaml_path is:', data_yaml_path)\n",
    "        \n",
    "        #\n",
    "        # with mlflow.start_run(run_id=parent_run_id):\n",
    "        with mlflow.start_run():\n",
    "            model = YOLO(f\"{project_location}/raw_model/yolo11n.pt\")\n",
    "            model.train(\n",
    "                task=\"detect\",\n",
    "                batch=16, # Batch size, with three modes: set as an integer (e.g., batch=16), auto mode for 60% GPU memory utilization (batch=-1), or auto mode with specified utilization fraction (batch=0.70).\n",
    "                device=[LOCAL_RANK], # need to be LOCAL_RANK, i.e., 0 for this case since we already init_process_group beforehand. RANK wont work. There is no need to specify [0,1] given for example if we have 2 GPUs per node. [0,1] with world_size of 4 or 2 beforehand will both fail. \n",
    "                data=data_yaml_path,\n",
    "                epochs=100,\n",
    "                project=f'{tmp_project_location}', # local VM ephermal location\n",
    "                # project=f'{volume_project_location}', # volume path still wont work\n",
    "                exist_ok=True,\n",
    "                fliplr=1,\n",
    "                flipud=1,\n",
    "                perspective=0.001,\n",
    "                degrees=.45\n",
    "            )\n",
    "            success = None\n",
    "            if RANK in (0, -1):\n",
    "                success = model.val()\n",
    "                if success:\n",
    "                    model.export() # ref: https://docs.ultralytics.com/modes/export/#introduction\n",
    "            \n",
    "\n",
    "        active_run_id = mlflow.last_active_run().info.run_id\n",
    "        print(\"For YOLO autologging, active_run_id is: \", active_run_id)\n",
    "\n",
    "        # after training is done.\n",
    "        if not dist.is_initialized():\n",
    "        # import torch.distributed as dist\n",
    "            dist.init_process_group(\"nccl\")\n",
    "\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        global_rank = int(os.environ[\"RANK\"])\n",
    "        world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "        print(f\"------After training, we have: RANK:{global_rank=} -- LOCAL_RANK:{local_rank=} -- world_size: {world_size=}------\")\n",
    "\n",
    "        if global_rank == 0:\n",
    "            with mlflow.start_run(run_id=active_run_id) as run:\n",
    "                mlflow.log_artifact(data_yaml_path, \"input_data_yaml\")\n",
    "                # mlflow.log_dict(data, \"data.yaml\")\n",
    "                mlflow.log_params({\"rank\":global_rank})\n",
    "                mlflow.pytorch.log_model(YOLO(str(model.trainer.best)), \"model\", signature=signature) # this succeeded\n",
    "                #: TODO: we can log more stuff here\n",
    "        \n",
    "        return \"finished\" # can return any picklable object\n",
    "    \n",
    "    finally:\n",
    "        # clean up\n",
    "        cleanup()\n",
    "\n",
    "\n",
    "train_fn.distributed(world_size = None, parent_run_id = None) # now can program can run without specifying manually the parameters of world_size and parent_run_id. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69cd261d-d018-41fe-aed5-204fcfc1fe2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Supplemental Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9621cf66-f949-4cb7-ad66-4305d4a68746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Tip about if too long waiting and job failed in node launching stage.\n",
    "For GPU resource not ready timeout error, consider to add these settings.\n",
    "\n",
    "error msg: \"torch.distributed.DistStoreError: Timed out after 601 seconds waiting for clients. 7/8 clients joined.\"\n",
    "\n",
    "```\n",
    "os.environ['TORCH_DISTRIBUTED_TIMEOUT'] = '7200'\n",
    "\n",
    "import os\n",
    "os.environ['NCCL_ASYNC_ERROR_HANDLING'] = '1'  # Recommended for better error reporting\n",
    "os.environ['NCCL_BLOCKING_WAIT'] = '1'         # Wait for full timeout\n",
    "os.environ['NCCL_SOCKET_TIMEOUT'] = '600'      # Set a socket timeout in seconds\n",
    "os.environ['NCCL_DEBUG'] = 'INFO'              # Enable debug logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67fc89a3-ec60-45d1-a3ae-421f6d931c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Overall Log Screening and Recommendations\n",
    "\n",
    "# Analysis of YOLO Training Log and Optimization Recommendations\n",
    "\n",
    "Based on the comprehensive analysis of your training log and research into distributed training best practices, here are the key areas for improvement in your cluster and training job configuration.\n",
    "\n",
    "## **Major Issues Identified**\n",
    "\n",
    "### **1. NCCL Network Communication Problems**\n",
    "\n",
    "The most significant issue in your log is the **failed NCCL network initialization**[1][2][3]. The errors show:\n",
    "\n",
    "- `NET/OFI aws-ofi-nccl initialization failed`\n",
    "- `NET/OFI Unable to find a protocol that worked`\n",
    "- `Using network Socket` (fallback to slower TCP networking)\n",
    "\n",
    "This means your distributed training is **falling back to slower Socket-based communication** instead of using optimized network fabrics, significantly reducing performance[4][5].\n",
    "\n",
    "### **2. Suboptimal Batch Size and Worker Configuration**\n",
    "\n",
    "Your current setup shows **8 dataloader workers** with an unspecified batch size. Research indicates this configuration may not be optimal for your 8-GPU setup[6][7][8].\n",
    "\n",
    "### **3. Databricks Serverless GPU Beta Limitations**\n",
    "\n",
    "The warning `serverless_gpu is in Beta. The API is subject to change` indicates you're using experimental infrastructure that may have performance and stability limitations[9][10].\n",
    "\n",
    "## **Cluster-Level Optimizations**\n",
    "\n",
    "### **Network Configuration**\n",
    "\n",
    "**Fix NCCL Network Issues:**\n",
    "- Set `NCCL_SOCKET_IFNAME=eth0` explicitly in your environment variables[3][11]\n",
    "- Add `NCCL_DEBUG=INFO` to get detailed networking information[12][11]\n",
    "- For AWS environments, ensure EFA (Elastic Fabric Adapter) is properly configured if available[4][13]\n",
    "\n",
    "**Recommended Environment Variables:**\n",
    "```bash\n",
    "export NCCL_SOCKET_IFNAME=eth0\n",
    "export NCCL_DEBUG=INFO\n",
    "export NCCL_NET=\"Socket\"  # Explicit fallback if EFA unavailable\n",
    "```\n",
    "\n",
    "### **Multi-GPU Setup Optimization**\n",
    "\n",
    "**Move from Serverless to Dedicated GPU Cluster:**\n",
    "Consider migrating from Databricks Serverless GPU (Beta) to a dedicated multi-GPU cluster for production training[14][15]. **Single-node multi-GPU setups typically outperform multi-node configurations** for YOLO training due to reduced network overhead[14].\n",
    "\n",
    "**Optimal Hardware Configuration:**\n",
    "- **Single node with 8 GPUs** is likely faster than 8 nodes with 1 GPU each[14]\n",
    "- Use **cluster placement groups** to minimize network latency[4]\n",
    "- Ensure all nodes have **identical PyTorch, CUDA, and NCCL versions**[16]\n",
    "\n",
    "## **Training Job Optimizations**\n",
    "\n",
    "### **Batch Size Optimization** \n",
    "\n",
    "**Use Automatic Batch Size Detection:**\n",
    "```python\n",
    "# Use batch=-1 for automatic optimal batch size calculation\n",
    "model.train(data=\"coco128.yaml\", epochs=100, batch=-1, device=[0,1,2,3,4,5,6,7])\n",
    "```\n",
    "\n",
    "This will automatically determine the **maximum batch size your GPUs can handle**[6][7], which is typically more efficient than manual guessing.\n",
    "\n",
    "**Manual Batch Size Guidelines:**\n",
    "- For 8 GPUs: Start with **batch=64** (8 per GPU) and scale up[6]\n",
    "- **Batch sizes of 16, 32, or 64 typically yield best results**[6]\n",
    "- Monitor GPU memory usage and increase until you approach memory limits[7]\n",
    "\n",
    "### **Dataloader Worker Optimization**\n",
    "\n",
    "**Reduce Worker Count:**\n",
    "Your current **8 workers may be excessive** for this setup[6][17]. Try:\n",
    "```python\n",
    "# Start with fewer workers to reduce RAM usage\n",
    "model.train(workers=4)  # or workers=2\n",
    "```\n",
    "\n",
    "**Memory Management:**\n",
    "- Disable image caching if experiencing high RAM usage: `cache=False`[17][18]\n",
    "- Monitor RAM usage during training - high worker counts can exhaust system memory[17]\n",
    "\n",
    "### **Training Parameters**\n",
    "\n",
    "**Enable Mixed Precision Training:**\n",
    "```python\n",
    "model.train(amp=True)  # Automatic Mixed Precision\n",
    "```\n",
    "This can **improve training speed and reduce memory usage** without sacrificing accuracy[7].\n",
    "\n",
    "**Optimize Image Processing:**\n",
    "```python\n",
    "model.train(\n",
    "    data=\"coco128.yaml\",\n",
    "    epochs=100, \n",
    "    batch=-1,           # Auto-detect optimal batch size\n",
    "    workers=4,          # Reduced worker count\n",
    "    device=[0,1,2,3,4,5,6,7],  # All 8 GPUs\n",
    "    amp=True,           # Mixed precision\n",
    "    cache=False         # Disable caching if RAM limited\n",
    ")\n",
    "```\n",
    "\n",
    "## **Monitoring and Debugging**\n",
    "\n",
    "### **Performance Monitoring**\n",
    "\n",
    "**Add NCCL Debugging:**\n",
    "Set `NCCL_DEBUG=INFO` to monitor network communication efficiency[3][11]. Look for:\n",
    "- Successful network initialization messages\n",
    "- Bandwidth utilization statistics\n",
    "- Communication pattern optimization\n",
    "\n",
    "**Track Key Metrics:**\n",
    "- **GPU utilization** (should be >90% during training)\n",
    "- **Network bandwidth utilization**\n",
    "- **Memory usage** (both GPU and system RAM)\n",
    "- **Training iteration time** and consistency[14]\n",
    "\n",
    "### **Troubleshooting Steps**\n",
    "\n",
    "1. **Test NCCL Communication:**\n",
    "   ```bash\n",
    "   # Run NCCL tests to verify network performance\n",
    "   python -c \"import torch; torch.distributed.init_process_group('nccl')\"\n",
    "   ```\n",
    "\n",
    "2. **Verify GPU Topology:**\n",
    "   Check GPU interconnects and ensure optimal placement[19]\n",
    "\n",
    "3. **Monitor Resource Usage:**\n",
    "   Use Databricks cluster metrics to identify bottlenecks[14]\n",
    "\n",
    "## **Long-term Recommendations**\n",
    "\n",
    "### **Infrastructure Migration**\n",
    "\n",
    "**Consider Moving to Production Infrastructure:**\n",
    "- Migrate from **Serverless GPU (Beta)** to stable, dedicated GPU clusters[9]\n",
    "- Use **instance types optimized for ML workloads** (e.g., p3, p4, g4 instances on AWS)[4]\n",
    "- Implement **proper EFA networking** for multi-node scenarios[20][4]\n",
    "\n",
    "### **Training Strategy**\n",
    "\n",
    "**Implement Progressive Training:**\n",
    "- Start with **smaller models and datasets** for parameter tuning\n",
    "- Use **gradient accumulation** if memory constraints limit batch size[7]\n",
    "- Consider **staged training** (train for shorter epochs, then resume) to avoid memory accumulation issues[17]\n",
    "\n",
    "The primary bottleneck in your current setup appears to be the **failed network optimization and suboptimal batch/worker configuration**. Addressing the NCCL networking issues should provide the most significant performance improvement, followed by optimizing batch size and reducing the worker count to prevent memory exhaustion.\n",
    "\n",
    "Sources\n",
    "[1] Slow NCCL gradient synchronization in distributed training https://discuss.pytorch.org/t/slow-nccl-gradient-synchronization-in-distributed-training/89625\n",
    "[2] Model Training with Ultralytics YOLO https://docs.ultralytics.com/modes/train/\n",
    "[3] NCCL Ignores Specified SOCKET_IFNAME Configuration ... - GitHub https://github.com/NVIDIA/nccl/issues/1581\n",
    "[4] Optimizing deep learning on P3 and P3dn with EFA - AWS https://aws.amazon.com/blogs/compute/optimizing-deep-learning-on-p3-and-p3dn-with-efa/\n",
    "[5] NCCL performance for Deep Learning workloads on AWS EFA ... https://github.com/NVIDIA/nccl/issues/235\n",
    "[6] What's an efficient way to fine tune the batch size? #3572 - GitHub https://github.com/ultralytics/ultralytics/issues/3572\n",
    "[7] Machine Learning Best Practices and Tips for Model Training https://docs.ultralytics.com/guides/model-training-tips/\n",
    "[8] I am seeing major improvements in my model and the only change ... https://community.ultralytics.com/t/i-am-seeing-major-improvements-in-my-model-and-the-only-change-has-been-the-machine-it-is-trained-on/1019\n",
    "[9] Serverless GPU compute | Databricks on AWS https://docs.databricks.com/aws/en/compute/serverless/gpu\n",
    "[10] Serverless GPU compute - Azure Databricks - Microsoft Learn https://learn.microsoft.com/en-us/azure/databricks/compute/serverless/gpu\n",
    "[11] How to set NCCL_SOCKET_IFNAME Â· Issue #286 Â· NVIDIA/nccl https://github.com/NVIDIA/nccl/issues/286\n",
    "[12] NCCL - CSCS Documentation https://docs.cscs.ch/software/communication/nccl/\n",
    "[13] Optimizing deep learning on P3 and P3dn with EFA - AWS https://aws.amazon.com/blogs/compute/optimizing-deep-learning-on-p3-and-p3dn-with-efa-part-1/\n",
    "[14] Best practices for deep learning on Databricks https://docs.databricks.com/aws/en/machine-learning/train-model/dl-best-practices\n",
    "[15] Multi-GPU and multi-node distributed training | Databricks on AWS https://docs.databricks.com/aws/en/machine-learning/sgc-examples/gpu-distributed-training\n",
    "[16] Multi node training of YOLOv8 (2 machine with 4GPU each) #7038 https://github.com/ultralytics/ultralytics/issues/7038\n",
    "[17] High RAM utilization during training - PyTorch Forums https://discuss.pytorch.org/t/high-ram-utilization-during-training/159939\n",
    "[18] how to avoid high RAM usage Â· Issue #1467 - GitHub https://github.com/ultralytics/ultralytics/issues/1467\n",
    "[19] Distributed Parallel Training: PyTorch Multi-GPU Setup in Kaggle T4x2 https://learnopencv.com/distributed-parallel-training-pytorch-multi-gpu-setup/\n",
    "[20] Get started with EFA and NCCL for ML workloads on Amazon EC2 https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start-nccl.html\n",
    "[21] DDP: multi node training Â· Issue #6286 - GitHub https://github.com/ultralytics/ultralytics/issues/6286\n",
    "[22] Multi-GPU Training with YOLOv5 - Ultralytics YOLO Docs https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/\n",
    "[23] Neuron Runtime Troubleshooting on Inf1, Inf2 and Trn1 https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-runtime/nrt-troubleshoot.html\n",
    "[24] Enabling Fast Inference and Resilient Training with NCCL 2.27 https://developer.nvidia.com/blog/enabling-fast-inference-and-resilient-training-with-nccl-2-27/\n",
    "[25] How to train yolov8 with multi-gpu? Â· Issue #3308 - GitHub https://github.com/ultralytics/ultralytics/issues/3308\n",
    "[26] [bug] NCCL WARN NET/OFI Only EFA provider is supported #2675 https://github.com/aws/deep-learning-containers/issues/2675\n",
    "[27] Issues when trying to train on a multi-GPU device #5244 - GitHub https://github.com/ultralytics/ultralytics/issues/5244\n",
    "[28] Version Â· Issue #391 Â· aws/aws-ofi-nccl - GitHub https://github.com/aws/aws-ofi-nccl/issues/391\n",
    "[29] Distributed Training: Definition & How it Works - Ultralytics https://www.ultralytics.com/glossary/distributed-training\n",
    "[30] Using EFA on the DLAMI - AWS Deep Learning AMIs https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-efa-using.html\n",
    "[31] On the Performance and Memory Footprint of Distributed Training https://arxiv.org/html/2407.02081v1\n",
    "[32] YOLO v11 training multi-GPU DDP Errors - Stack Overflow https://stackoverflow.com/questions/79372969/yolo-v11-training-multi-gpu-ddp-errors\n",
    "[33] Distributed Parallel Training Example (GPU) https://www.mindspore.cn/tutorials/experts/en/r2.0.0-alpha/parallel/train_gpu.html\n",
    "[34] Configure YOLOv8 for GPU: Accelerate Object Detection https://www.digitalocean.com/community/tutorials/yolov8-for-gpu-accelerate-object-detection\n",
    "[35] Simplifying Training and GenAI Finetuning Using Serverless GPU ... https://www.youtube.com/watch?v=pQMeeQ_jGY0\n",
    "[36] aws-samples/eks-efa-examples - GitHub https://github.com/aws-samples/eks-efa-examples\n",
    "[37] Multi-GPU and multi-node distributed training - Azure Databricks https://learn.microsoft.com/en-us/azure/databricks/machine-learning/sgc-examples/gpu-distributed-training\n",
    "[38] Configuration - Ultralytics YOLO Docs https://docs.ultralytics.com/usage/cfg/\n",
    "[39] Best practices for performance efficiency | Databricks on AWS https://docs.databricks.com/aws/en/lakehouse-architecture/performance-efficiency/best-practices\n",
    "[40] YOLOv5 Study: mAP vs Batch-Size #2452 - GitHub https://github.com/ultralytics/yolov5/discussions/2452\n",
    "[41] High-Performance GPU Memory Transfer on AWS Sagemaker ... https://www.perplexity.ai/hub/blog/high-performance-gpu-memory-transfer-on-aws\n",
    "[42] ML Training Tip Of The Week #1: Optimizing GPU ... - 86677 https://community.databricks.com/t5/technical-blog/ml-training-tip-of-the-week-1-optimizing-gpu-utilization-in/ba-p/86677\n",
    "[43] Tips for Best YOLOv5 Training Results - Ultralytics YOLO Docs https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/\n",
    "[44] NCCL error when using Sagemaker distributed training without ... https://stackoverflow.com/questions/75064559/nccl-error-when-using-sagemaker-distributed-training-without-specifying-a-distri\n",
    "[45] Normal then slow then crashing training - YOLO - Ultralytics https://community.ultralytics.com/t/normal-then-slow-then-crashing-training/1203\n",
    "[46] The usage of video memory fluctuates greatly during YOLO11 training https://github.com/ultralytics/ultralytics/issues/20860\n",
    "[47] Serverless compute plane networking - Azure Databricks https://learn.microsoft.com/en-us/azure/databricks/security/network/serverless-network-security/\n",
    "[48] Unable to see NCCL logs - PyTorch Forums https://discuss.pytorch.org/t/unable-to-see-nccl-logs/176114\n",
    "[49] Optimize GPU utilization while training - YOLO - Ultralytics https://community.ultralytics.com/t/optimize-gpu-utilization-while-training/768\n",
    "[50] https://raw.githubusercontent.com/aws-samples/awso... https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/efa-cheatsheet.md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78dfd331-f622-494b-b38a-8990ad553475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**NCCL logs issues + recommendations** map.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš© Issues Observed in Your Logs\n",
    "\n",
    "1. **Transport Layer**\n",
    "   - NCCL is falling back to **`NET/Socket`** transport.  \n",
    "   - This is functional but **subâ€‘optimal** for multiâ€‘node training if you have InfiniBand, RoCE, or AWS EFA available.  \n",
    "   - GPUDirect RDMA (`GDR`) is disabled (`GDR 0`), so GPU memory copies are going through host memory.\n",
    "\n",
    "2. **CollNet / NVLink**\n",
    "   - Logs show `2 collnet channels` but also earlier warnings about missing `ncclCollNetPlugin_v10`.  \n",
    "   - CollNet is provisioned but not actually active.  \n",
    "   - `MNNVL 0` and `0 nvls channels` confirm no NVLink multiâ€‘node or NVLinkâ€‘SHARP acceleration.\n",
    "\n",
    "3. **Tuner Plugin**\n",
    "   - NCCL tried to load `libnccl-tuner.so` and failed, falling back to the **internal tuner**.  \n",
    "   - This is safe, but you lose the ability to autoâ€‘tune thresholds for your specific network.\n",
    "\n",
    "4. **P2P Support**\n",
    "   - `intraNodeP2pSupport 0 directMode 0` â†’ no GPUâ€‘toâ€‘GPU direct P2P.  \n",
    "   - Expected if you only have one GPU per node, but if you *do* have multiple GPUs per node, this means P2P isnâ€™t configured correctly.\n",
    "\n",
    "5. **Socket Parallelism**\n",
    "   - Using `2 threads` Ã— `8 sockets per thread`.  \n",
    "   - This is decent, but may not saturate highâ€‘bandwidth links if youâ€™re on a 100â€¯Gbps+ fabric.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Recommendations\n",
    "\n",
    "### 1. Optimize Transport\n",
    "- If you have **InfiniBand or RoCE**:\n",
    "  ```bash\n",
    "  export NCCL_NET=IB\n",
    "  ```\n",
    "- If youâ€™re on **AWS with EFA**:\n",
    "  ```bash\n",
    "  export NCCL_NET=OFI\n",
    "  export FI_PROVIDER=efa\n",
    "  ```\n",
    "- If you only have Ethernet, sockets are fine, but you can tune them (see below).\n",
    "\n",
    "### 2. Enable GPUDirect RDMA (if hardware supports it)\n",
    "- Install Mellanox OFED drivers and ensure `nvidia-peermem` is loaded.  \n",
    "- Then NCCL should log `GDR 1` instead of `GDR 0`.\n",
    "\n",
    "### 3. CollNet / Hierarchical Collectives\n",
    "- If you want CollNet:\n",
    "  - Rebuild NCCL with `--with-collnet`.  \n",
    "  - Or install the NCCL package that includes CollNet support.  \n",
    "- If you donâ€™t need it, disable to avoid noise:\n",
    "  ```bash\n",
    "  export NCCL_COLLNET_ENABLE=0\n",
    "  ```\n",
    "\n",
    "### 4. Tuner Plugin\n",
    "- Optional: build or install `libnccl-tuner.so` if you want NCCL to autoâ€‘tune thresholds for your exact network.  \n",
    "- Otherwise, the internal tuner is fine.\n",
    "\n",
    "### 5. Socket Backend Tuning\n",
    "- Increase parallelism if youâ€™re bandwidthâ€‘limited:\n",
    "  ```bash\n",
    "  export NCCL_SOCKET_NTHREADS=4\n",
    "  export NCCL_NSOCKS_PERTHREAD=8\n",
    "  ```\n",
    "- Adjust based on CPU/network load.\n",
    "\n",
    "### 6. P2P (if multiâ€‘GPU per node)\n",
    "- Ensure GPUs are on the same PCIe root complex.  \n",
    "- Check with:\n",
    "  ```bash\n",
    "  nvidia-smi topo -m\n",
    "  ```\n",
    "- If P2P is supported, NCCL should show `intraNodeP2pSupport 1`.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "Right now, your setup is **working but not optimized**:  \n",
    "- Youâ€™re on **Socket transport** with no GPUDirect, no CollNet, and no tuner plugin.  \n",
    "- Thatâ€™s fine for functional correctness, but youâ€™re leaving performance on the table if you have faster interconnects.  \n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to build you a **scenario matrix** (singleâ€‘node vs multiâ€‘node, Ethernet vs IB/EFA, with/without NVLink) that shows the *optimal NCCL env vars* for each case? That way youâ€™d have a readyâ€‘toâ€‘apply playbook depending on where you run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7acaa5bb-248c-4467-bae0-b86858c71f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import mlflow\n",
    "import torch.distributed as dist\n",
    "from ultralytics import settings\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from ultralytics.utils import RANK, LOCAL_RANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a7ddfb2-534a-4960-a1c3-9dd139bc8a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I ran 4-6 tests with different CUDA device settings for the below minimal example to prove it wont work without the right setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b3b98d-bfc0-45b0-b06d-86e4e7cbcdc9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "(Prove minimal wont work)Minimal_example_v"
    }
   },
   "outputs": [],
   "source": [
    "# data_yaml_path = \"coco128.yaml\" # ref: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco128.yaml\n",
    "\n",
    "@distributed(gpus=8, gpu_type='A10', remote=True)\n",
    "def train_fn():\n",
    "  # Start a run to represent the training job\n",
    "  with mlflow.start_run():\n",
    "    model = YOLO(f\"yolo11n\") # shared location\n",
    "    # model = YOLO(\"yolo11n\")\n",
    "    model.train(\n",
    "        task=\"detect\",\n",
    "        batch=16, # Batch size, with three modes: set as an integer (e.g., batch=16), auto mode for 60% GPU memory utilization (batch=-1), or auto mode with specified utilization fraction (batch=0.70).\n",
    "        device=[LOCAL_RANK], # need to be LOCAL_RANK, i.e., 0 for this case since we already init_process_group beforehand. RANK wont work. There is no need to specify [0,1] given for example if we have 2 GPUs per node. [0,1] with world_size of 4 or 2 beforehand will both fail. \n",
    "        data=data_yaml_path,\n",
    "        epochs=100,\n",
    "        project=f'{tmp_project_location}', # local VM ephermal location\n",
    "        # project=f'{volume_project_location}', # volume path still wont work\n",
    "        exist_ok=True,\n",
    "        fliplr=1,\n",
    "        flipud=1,\n",
    "        perspective=0.001,\n",
    "        degrees=.45\n",
    "    )\n",
    "\n",
    "train_fn.distributed()   \n",
    "\n",
    "## : conclusion\n",
    "## after a few iterations (with screenshots stored locally for error msgs from experiment log), we conclude it wont work for @distributed with simple setup.\n",
    "\n",
    "\n",
    "#: ----comment below out cause it was for classical GPU compute.----\n",
    "# distributor = TorchDistributor(num_processes=1, local_mode=True, use_gpu=True)      \n",
    "# distributor.run(train_fn)\n",
    "# # on sgc, error: [CONFIG_NOT_AVAILABLE] Configuration spark.master is not available. SQLSTATE: 42K0I"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6482896598264492,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_MultiNode A10 training on SGC using Ultralytics YOLO CV model with coco128 image dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
