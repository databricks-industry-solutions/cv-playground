{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dca7979-9176-49ed-8ada-dfbb4eb2274f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "An example for distributing the Pytorch training with Hyperparameter tuning optimizations using [Optuna](https://optuna.org/)   \n",
    "**Including checkpointing** --> saved to UC Volumes and logged to MLflow post training after each Optuna trial\n",
    "\n",
    "\n",
    "The cluster used is the same as before -- you are welcome to test other cluster configs. \n",
    "\n",
    "```\n",
    "\"spark_version\": \"16.4.x-scala2.13\",\n",
    "\"node_type_id\": \"g5.12xlarge\",\n",
    "\n",
    "## omit autoscale --> n_workers = 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7189f5-7e13-4941-bcb5-bba786d41083",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "install dependencies"
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow[skinny]>=3 optuna nvidia-ml-py3 --upgrade\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70dc111c-3fa7-48c7-b9d2-55707c5e35b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "for reference"
    }
   },
   "outputs": [],
   "source": [
    "# MLflow version: 3.4.0\n",
    "# Optuna version: 4.5.0\n",
    "# PyTorch version: 2.6.0+cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71fb8064-e02c-463b-89f7-db7ebf3df9c2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "import dependencies"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import shutil\n",
    "import tempfile\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import mlflow\n",
    "import optuna\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "from streaming import StreamingDataset, StreamingDataLoader\n",
    "from streaming.base.util import clean_stale_shared_memory\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35fe90b0-3096-4917-96d6-ce2498a39366",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configs"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATASET CONFIGURATION\n",
    "# ============================================================\n",
    "CATALOG = \"mmt\"\n",
    "SCHEMA = \"pytorch\"\n",
    "VOLUME_NAME = \"torch_data\"\n",
    "\n",
    "# Dataset paths\n",
    "mds_train_dir = 'imagenet_tiny200_mds_train'\n",
    "mds_val_dir = 'imagenet_tiny200_mds_val'\n",
    "data_storage_location = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}\"\n",
    "\n",
    "# Dataset parameters\n",
    "num_classes = 200  # ImageNet Tiny has 200 classes\n",
    "num_workers = 4\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "NUM_EPOCHS = 2 #5\n",
    "N_TRIALS = 3 #10  # Number of Optuna trials\n",
    "\n",
    "# ============================================================\n",
    "# CHECKPOINTING CONFIGURATION\n",
    "# ============================================================\n",
    "ENABLE_CHECKPOINTING = True\n",
    "CHECKPOINT_FREQUENCY = 1  # Save every N epochs\n",
    "RESUME_FROM_CHECKPOINT = False\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT-SPECIFIC PATH CONFIGURATION\n",
    "# ============================================================\n",
    "import os\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "\n",
    "BASE_VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}\"\n",
    "BASE_VOLUME_DBFS = f\"dbfs:/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}\"\n",
    "\n",
    "# Experiment identifier\n",
    "EXPERIMENT_SHORT_NAME = \"imagenet_mobilenetv2_hpt_chkpt\"\n",
    "EXPERIMENT_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# User and MLflow experiment name\n",
    "USER_NAME = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "EXPERIMENT_NAME = f\"/Users/{USER_NAME}/mlflow_experiments/pytorch_{EXPERIMENT_SHORT_NAME}\"\n",
    "\n",
    "# Experiment root directory\n",
    "EXPERIMENT_ROOT = f\"{BASE_VOLUME_PATH}/{EXPERIMENT_SHORT_NAME}\"\n",
    "EXPERIMENT_ROOT_DBFS = f\"{BASE_VOLUME_DBFS}/{EXPERIMENT_SHORT_NAME}\"\n",
    "\n",
    "# Separate paths for checkpoints and MLflow artifacts\n",
    "CHECKPOINT_BASE_DIR = f\"{EXPERIMENT_ROOT}/checkpoints/{EXPERIMENT_TIMESTAMP}\"\n",
    "MLFLOW_ARTIFACT_LOCATION = f\"{EXPERIMENT_ROOT_DBFS}/mlflow_artifacts\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CHECKPOINT_BASE_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{EXPERIMENT_ROOT}/mlflow_artifacts\", exist_ok=True)\n",
    "\n",
    "# MLflow setup\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.enable_system_metrics_logging()\n",
    "\n",
    "# ============================================================\n",
    "# GET OR CREATE EXPERIMENT (REUSE IF EXISTS)\n",
    "# ============================================================\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "if experiment:\n",
    "    experiment_id = experiment.experiment_id\n",
    "    print(f\"✓ Reusing existing MLflow experiment\")\n",
    "    print(f\"  Name: {EXPERIMENT_NAME}\")\n",
    "    print(f\"  Experiment ID: {experiment_id}\")\n",
    "    print(f\"  Artifact Location: {experiment.artifact_location}\")\n",
    "    \n",
    "    # Check if artifact location matches desired location\n",
    "    if experiment.artifact_location != MLFLOW_ARTIFACT_LOCATION:\n",
    "        print(f\"\\nNote: Artifact location differs from desired\")\n",
    "        print(f\"     Desired:  {MLFLOW_ARTIFACT_LOCATION}\")\n",
    "        print(f\"     Actual:   {experiment.artifact_location}\")\n",
    "        print(f\"     This is OK - using existing location for consistency\")\n",
    "        # Use existing artifact location to avoid confusion\n",
    "        MLFLOW_ARTIFACT_LOCATION = experiment.artifact_location\n",
    "else:\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        name=EXPERIMENT_NAME,\n",
    "        artifact_location=MLFLOW_ARTIFACT_LOCATION\n",
    "    )\n",
    "    experiment = mlflow.get_experiment(experiment_id)\n",
    "    print(f\"✓ Created new MLflow experiment\")\n",
    "    print(f\"  Name: {EXPERIMENT_NAME}\")\n",
    "    print(f\"  Experiment ID: {experiment_id}\")\n",
    "    print(f\"  Artifact Location: {MLFLOW_ARTIFACT_LOCATION}\")\n",
    "\n",
    "# Set the experiment as active\n",
    "mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION SUMMARY\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EXPERIMENT CONFIGURATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Experiment: {EXPERIMENT_SHORT_NAME}\")\n",
    "print(f\"Timestamp:  {EXPERIMENT_TIMESTAMP}\")\n",
    "print(f\"\\nDataset Configuration:\")\n",
    "print(f\"  Storage Location: {data_storage_location}\")\n",
    "print(f\"  Training Data:    {data_storage_location}/{mds_train_dir}\")\n",
    "print(f\"  Validation Data:  {data_storage_location}/{mds_val_dir}\")\n",
    "print(f\"  Num Classes:      {num_classes}\")\n",
    "print(f\"\\nDirectory Structure:\")\n",
    "print(f\"  Root: {EXPERIMENT_ROOT}/\")\n",
    "print(f\"  ├── checkpoints/\")\n",
    "print(f\"  │   └── {EXPERIMENT_TIMESTAMP}/\")\n",
    "print(f\"  │       ├── trial_000_*/\")\n",
    "print(f\"  │       ├── trial_001_*/\")\n",
    "print(f\"  │       └── ...\")\n",
    "print(f\"  └── mlflow_artifacts/\")\n",
    "print(f\"      ├── {{parent_run_id}}/artifacts/\")\n",
    "print(f\"      ├── {{trial_0_run_id}}/artifacts/\")\n",
    "print(f\"      └── ...\")\n",
    "print(f\"\\nPaths:\")\n",
    "print(f\"  Experiment Root:     {EXPERIMENT_ROOT}\")\n",
    "print(f\"  Checkpoint Base:     {CHECKPOINT_BASE_DIR}\")\n",
    "print(f\"  MLflow Artifacts:    {MLFLOW_ARTIFACT_LOCATION}\")\n",
    "print(f\"  MLflow Experiment:   {EXPERIMENT_NAME}\")\n",
    "print(f\"  Experiment ID:       {experiment_id}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Num Classes:      {num_classes}\")\n",
    "print(f\"  Num Epochs:       {NUM_EPOCHS}\")\n",
    "print(f\"  Num Workers:      {num_workers}\")\n",
    "print(f\"  Num Trials:       {N_TRIALS}\")\n",
    "print(f\"  Checkpointing:    {ENABLE_CHECKPOINTING}\")\n",
    "print(f\"  Checkpoint Freq:  {CHECKPOINT_FREQUENCY}\")\n",
    "print(f\"  Resume Enabled:   {RESUME_FROM_CHECKPOINT}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# VERIFY PATHS\n",
    "# ============================================================\n",
    "print(\"Verifying paths...\")\n",
    "assert os.path.exists(data_storage_location), f\"Data storage location not found: {data_storage_location}\"\n",
    "assert os.path.exists(f\"{data_storage_location}/{mds_train_dir}\"), f\"Training data not found: {data_storage_location}/{mds_train_dir}\"\n",
    "assert os.path.exists(f\"{data_storage_location}/{mds_val_dir}\"), f\"Validation data not found: {data_storage_location}/{mds_val_dir}\"\n",
    "assert os.path.exists(CHECKPOINT_BASE_DIR), f\"Checkpoint directory not created: {CHECKPOINT_BASE_DIR}\"\n",
    "assert os.path.exists(f\"{EXPERIMENT_ROOT}/mlflow_artifacts\"), f\"MLflow artifacts directory not created\"\n",
    "print(\"✓ All paths verified\")\n",
    "print(f\"  ✓ Training data: {len(os.listdir(f'{data_storage_location}/{mds_train_dir}'))} files\")\n",
    "print(f\"  ✓ Validation data: {len(os.listdir(f'{data_storage_location}/{mds_val_dir}'))} files\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# GLOBAL VARIABLES FOR OBJECTIVE FUNCTION\n",
    "# ============================================================\n",
    "EXPERIMENT_RUN_ID = None  # Will be set when parent run starts\n",
    "\n",
    "# print(f\"{'='*80}\")\n",
    "# print(f\"READY TO START OPTIMIZATION\")\n",
    "# print(f\"{'='*80}\")\n",
    "# print(f\"Configuration complete. Run the optimization cell to begin training.\")\n",
    "# print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51124fa-070b-4b26-9d1d-67d101cb84f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "e.g. structure of checkpoints/artifact paths"
    }
   },
   "outputs": [],
   "source": [
    "# /Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}/\n",
    "# └── imagenet_mobilenetv2_hpt_chkpt/                          # Experiment root\n",
    "#     ├── checkpoints/                          # Operational checkpoints\n",
    "#     │   ├── 20251013_104011/                  # Run timestamp\n",
    "#     │   │   ├── trial_0_AdamW_lr0.001/\n",
    "#     │   │   │   ├── checkpoint_epoch_0.pt\n",
    "#     │   │   │   ├── checkpoint_epoch_1.pt\n",
    "#     │   │   │   └── best_checkpoint.pt\n",
    "#     │   │   ├── trial_1_SGD_lr0.01/\n",
    "#     │   │   └── ...\n",
    "#     │   ├── 20251013_150000/                  # Another run\n",
    "#     │   └── ...\n",
    "#     └── mlflow_artifacts/                     # MLflow tracking\n",
    "#         ├── {parent_run_id}/\n",
    "#         │   └── artifacts/\n",
    "#         │       └── study_results.json\n",
    "#         ├── {trial_0_run_id}/\n",
    "#         │   └── artifacts/\n",
    "#         │       ├── model/                    # if logged model\n",
    "#         │       ├── plots/                    # if logged Training curves\n",
    "#         │       └── best_checkpoint.pt        # Copy for governance\n",
    "#         └── {trial_1_run_id}/\n",
    "#             └── artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333f7b44-2d24-4d58-9f8e-373ed9087703",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dataloader for Multi-Shard Structure"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloader_with_mosaic(remote_path, local_path, batch_size, rank=0):\n",
    "    \"\"\"Fixed dataloader with unique cache directories for train/val\"\"\"\n",
    "    print(f\"Rank {rank}: Getting optimized MDS data from {remote_path}\")\n",
    "    \n",
    "    try:\n",
    "        clean_stale_shared_memory()\n",
    "    except Exception as e:\n",
    "        print(f\"Shared memory cleanup warning: {e}\")\n",
    "    \n",
    "    # Create UNIQUE cache directory using remote path to differentiate train/val ##this helped to speed up data handling\n",
    "    import hashlib\n",
    "    path_hash = hashlib.md5(remote_path.encode()).hexdigest()[:8]\n",
    "    unique_id = f\"{int(time.time())}_{rank}_{path_hash}\"\n",
    "    local_path_unique = f\"/local_disk0/tmp/mds_cache_rank_{rank}_{unique_id}\"\n",
    "    \n",
    "    # Force cleanup of any existing directory\n",
    "    if os.path.exists(local_path_unique):\n",
    "        try:\n",
    "            shutil.rmtree(local_path_unique)\n",
    "            time.sleep(0.1)  # Brief pause to ensure cleanup\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not remove existing cache: {e}\")\n",
    "    \n",
    "    os.makedirs(local_path_unique, mode=0o755, exist_ok=True)\n",
    "    print(f\"Rank {rank}: Created unique cache directory: {local_path_unique}\")\n",
    "    \n",
    "    try:\n",
    "        # MDS configuration for network storage\n",
    "        dataset = StreamingDataset(\n",
    "            remote=remote_path,\n",
    "            local=local_path_unique,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "            num_canonical_nodes=1,\n",
    "            predownload=batch_size * 4,  # Reasonable predownload\n",
    "            keep_zip=False,  # Don't keep zip files to save space\n",
    "            download_retry=2,  # Fewer retries\n",
    "            download_timeout=300,  # 5 minute timeout\n",
    "            validate_hash=False,\n",
    "            epoch_size=None,\n",
    "        )\n",
    "        \n",
    "        print(f\"Rank {rank}: Created optimized StreamingDataset with {len(dataset)} samples\")\n",
    "        \n",
    "        dataloader = StreamingDataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=0,  # No multiprocessing to avoid contention\n",
    "            pin_memory=True,  # Use pinned memory for faster GPU transfer\n",
    "            drop_last=True,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "        \n",
    "        print(f\"Rank {rank}: Created optimized dataloader with {len(dataloader)} batches\")\n",
    "        return dataloader, local_path_unique\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Rank {rank}: Error creating optimized dataloader: {e}\")\n",
    "        # Cleanup on error\n",
    "        if os.path.exists(local_path_unique):\n",
    "            try:\n",
    "                shutil.rmtree(local_path_unique)\n",
    "            except:\n",
    "                pass\n",
    "        raise\n",
    "\n",
    "## e.g. Update configuration - for testing\n",
    "# NUM_EPOCHS = 1  # Test with just 1 epoch first\n",
    "# num_workers = 2  # Reduce to 2 workers to decrease I/O contention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b82505-9204-4a15-9da7-0b01f04b2196",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Loading Functions for Multi-Shard Structure"
    }
   },
   "outputs": [],
   "source": [
    "### your data structure may be different -->  verify_mds_dataset will need to be updated \n",
    "\n",
    "import io\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "from streaming import StreamingDataset\n",
    "\n",
    "def create_comprehensive_label_mapping(remote_path):\n",
    "    \"\"\"Extract class names using DataLoader approach\"\"\"\n",
    "    print(f\"Creating comprehensive label mapping from: {remote_path}\")\n",
    "    \n",
    "    temp_cache = tempfile.mkdtemp(prefix=\"label_mapping_\")\n",
    "    \n",
    "    try:\n",
    "        dataset = StreamingDataset(\n",
    "            remote=remote_path,\n",
    "            local=temp_cache,\n",
    "            shuffle=True,\n",
    "            batch_size=32  # Use a reasonable batch size\n",
    "        )\n",
    "        \n",
    "        # Create DataLoader\n",
    "        from torch.utils.data import DataLoader\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=32,\n",
    "            num_workers=0  # Use 0 to avoid multiprocessing issues during class extraction\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset created with {len(dataset)} total samples\")\n",
    "        \n",
    "        unique_classes = set()\n",
    "        sample_count = 0\n",
    "        max_batches = 50  # Process 50 batches to get class names\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "                \n",
    "            # batch['class_name'] is now a list of class names\n",
    "            class_names = batch['class_name']\n",
    "            unique_classes.update(class_names)\n",
    "            sample_count += len(class_names)\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Processed {batch_idx} batches, {sample_count} samples, found {len(unique_classes)} unique classes\")\n",
    "            \n",
    "            if len(unique_classes) >= 200:\n",
    "                break\n",
    "        \n",
    "        classes = sorted(list(unique_classes))\n",
    "        print(f\"Found {len(classes)} unique classes\")\n",
    "        print(f\"Sample classes: {classes[:5]}\")\n",
    "        \n",
    "        label_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        \n",
    "        print(f\"\\n=== Label Mapping Summary ===\")\n",
    "        print(f\"Total classes: {len(label_to_idx)}\")\n",
    "        \n",
    "        return label_to_idx\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating label mapping: {e}\")\n",
    "        import traceback\n",
    "        print(f\"Full error: {traceback.format_exc()}\")\n",
    "        return create_imagenet_tiny200_mapping()\n",
    "    \n",
    "    finally:\n",
    "        if os.path.exists(temp_cache):\n",
    "            shutil.rmtree(temp_cache, ignore_errors=True)\n",
    "\n",
    "def create_imagenet_tiny200_mapping():\n",
    "    \"\"\"Fallback mapping\"\"\"\n",
    "    print(\"Creating fallback ImageNet Tiny-200 mapping...\")\n",
    "    classes = [f\"class_{i:03d}\" for i in range(200)]\n",
    "    return {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "def verify_mds_dataset(remote_path):\n",
    "    \"\"\"Simple verification for multi-shard MDS dataset\"\"\"\n",
    "    print(f\"Verifying multi-shard MDS dataset at: {remote_path}\")\n",
    "    \n",
    "    if not os.path.exists(remote_path):\n",
    "        print(f\"ERROR: Remote path does not exist: {remote_path}\")\n",
    "        return False\n",
    "    \n",
    "    files_in_dir = os.listdir(remote_path)\n",
    "    print(f\"Items in root directory: {len(files_in_dir)}\")\n",
    "    \n",
    "    # Check for main index.json\n",
    "    has_main_index = 'index.json' in files_in_dir\n",
    "    if has_main_index:\n",
    "        print(\"Found main index.json\")\n",
    "    \n",
    "    # Check for numbered shard directories\n",
    "    shard_dirs = [f for f in files_in_dir if f.isdigit() and os.path.isdir(os.path.join(remote_path, f))]\n",
    "    shard_dirs.sort(key=int)\n",
    "    \n",
    "    print(f\"Found {len(shard_dirs)} shard directories\")\n",
    "    if len(shard_dirs) > 0:\n",
    "        print(f\"  Shard range: {shard_dirs[0]} to {shard_dirs[-1]}\")\n",
    "        print(f\"  Sample shards: {shard_dirs[:5]}{'...' if len(shard_dirs) > 5 else ''}\")\n",
    "        \n",
    "        # Check first shard\n",
    "        if len(shard_dirs) > 0:\n",
    "            shard_path = os.path.join(remote_path, shard_dirs[0])\n",
    "            shard_contents = os.listdir(shard_path)\n",
    "            print(f\"Shard {shard_dirs[0]} contents: {shard_contents}\")\n",
    "            \n",
    "            # Check for shard files\n",
    "            shard_files = [f for f in shard_contents if f.endswith('.mds') or f.endswith('.mds.zstd')]\n",
    "            if shard_files:\n",
    "                shard_file_path = os.path.join(shard_path, shard_files[0])\n",
    "                file_size = os.path.getsize(shard_file_path)\n",
    "                print(f\"Shard file size: {file_size:,} bytes\")\n",
    "        \n",
    "        print(f\"Verified sample shards\")\n",
    "        return True\n",
    "    \n",
    "    print(\"No valid shard directories found\")\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6fd907a-8786-4fbe-af91-8d44ad689f18",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "convert_batch_to_tensors"
    }
   },
   "outputs": [],
   "source": [
    "def convert_batch_to_tensors(batch, device=None, class_to_idx=None, rank=None):\n",
    "    \"\"\"Convert MDS batch to PyTorch tensors - with automatic parameter handling\"\"\"\n",
    "    import io\n",
    "    from PIL import Image\n",
    "    import torchvision.transforms as transforms\n",
    "    import torch\n",
    "    \n",
    "    # Auto-detect device if not provided\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Handle rank parameter - get from distributed training if available\n",
    "    if rank is None:\n",
    "        try:\n",
    "            if torch.distributed.is_initialized():\n",
    "                rank = torch.distributed.get_rank()\n",
    "            else:\n",
    "                rank = 0\n",
    "        except:\n",
    "            rank = 0\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    # Debug info only for rank 0 and only occasionally\n",
    "    debug_output = (rank == 0)\n",
    "    \n",
    "    # Process images\n",
    "    images = []\n",
    "    for i, img_bytes in enumerate(batch['image_data']):\n",
    "        try:\n",
    "            img = Image.open(io.BytesIO(img_bytes)).convert('RGB')\n",
    "            img_tensor = transform(img)\n",
    "            images.append(img_tensor)\n",
    "        except Exception as e:\n",
    "            if debug_output and i < 3:  # Only show first few errors\n",
    "                print(f\"Error processing image {i}: {e}\")\n",
    "            images.append(torch.zeros((3, 224, 224)))\n",
    "    \n",
    "    images_tensor = torch.stack(images).to(device)\n",
    "    \n",
    "    # Process labels \n",
    "    class_names = batch['class_name']\n",
    "    \n",
    "    # Ensure class_to_idx is a dictionary, not an integer\n",
    "    if class_to_idx is None or not isinstance(class_to_idx, dict):\n",
    "        unique_classes = sorted(set(class_names))\n",
    "        class_to_idx = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "        if debug_output:\n",
    "            print(f\"Created temporary class mapping with {len(class_to_idx)} classes\")\n",
    "    \n",
    "    labels = []\n",
    "    for class_name in class_names:\n",
    "        # Use isinstance check instead of 'in' operator with potential int\n",
    "        if isinstance(class_to_idx, dict) and class_name in class_to_idx:\n",
    "            labels.append(class_to_idx[class_name])\n",
    "        else:\n",
    "            labels.append(0)  # Default to class 0 for unknown classes\n",
    "            if debug_output:\n",
    "                print(f\"Warning: Unknown class '{class_name}', using default\")\n",
    "    \n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "    \n",
    "    return images_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e870bd84-a85c-4f84-bb68-83f0edacbaaf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "quick check | Dataset Verification for Multi-Shard Structure"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. FIRST: Define device globally\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Create class mapping\n",
    "print(\"Creating class mapping...\")\n",
    "label_to_idx = create_comprehensive_label_mapping(f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}/{mds_train_dir}\")\n",
    "\n",
    "# 3. Verify datasets\n",
    "print(\"Verifying datasets...\")\n",
    "train_valid = verify_mds_dataset(f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}/{mds_train_dir}\")\n",
    "val_valid = verify_mds_dataset(f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}/{mds_val_dir}\")\n",
    "\n",
    "print(\"Setup complete! Ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315979ff-98c4-46b7-b465-e2388f88441b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Label Mapping"
    }
   },
   "outputs": [],
   "source": [
    "# Label Mapping\n",
    "print(\"=== Creating Comprehensive Label Mapping ===\")\n",
    "train_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}/{mds_train_dir}\"\n",
    "\n",
    "# Global variables that need to be defined\n",
    "label_to_idx = {}\n",
    "\n",
    "try:\n",
    "    label_to_idx = create_comprehensive_label_mapping(train_path)\n",
    "    num_classes = len(label_to_idx)\n",
    "    \n",
    "    # If we didn't get enough classes, use the fallback\n",
    "    if num_classes < 50:\n",
    "        print(\"Insufficient classes found, using fallback mapping...\")\n",
    "        label_to_idx = create_imagenet_tiny200_mapping()\n",
    "        num_classes = len(label_to_idx)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in comprehensive mapping: {e}\")\n",
    "    print(\"Using fallback mapping...\")\n",
    "    label_to_idx = create_imagenet_tiny200_mapping()\n",
    "    num_classes = len(label_to_idx)\n",
    "\n",
    "print(f\"\\nFinal label mapping created with {num_classes} classes\")\n",
    "\n",
    "# Test the mapping with a few samples\n",
    "print(\"\\n=== Testing Label Mapping ===\")\n",
    "test_classes = ['barrel, cask', 'school bus', 'pizza, pizza pie']\n",
    "for test_class in test_classes:\n",
    "    idx = label_to_idx.get(test_class, -1)\n",
    "    print(f\"'{test_class}' -> index {idx}\")\n",
    "\n",
    "# Show some actual mappings that exist\n",
    "print(f\"\\nFirst 10 actual mappings:\")\n",
    "for i, (class_name, idx) in enumerate(list(label_to_idx.items())[:10]):\n",
    "    print(f\"  {idx}: {class_name}\")\n",
    "\n",
    "print(f\"\\nLabel mapping setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43edb6dc-a3bc-497c-9523-56416f28987f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Definition"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(lr=0.001):\n",
    "    \"\"\"Create MobileNetV2 model for ImageNet Tiny-200\"\"\"\n",
    "    from torchvision.models import MobileNet_V2_Weights\n",
    "    \n",
    "    model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Freeze feature extraction layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace classifier for num_classes\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier[1] = torch.nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    # Only train the classifier\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7c0586-f0d5-4cde-859e-528aabd2d3f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simplified Training + Evaluation + Metrics"
    }
   },
   "outputs": [],
   "source": [
    "def distributed_train_and_evaluate(lr=0.001, batch_size=32, optimizer_name='AdamW',\n",
    "                                 weight_decay=1e-4, step_size=7, gamma=0.1,\n",
    "                                 dropout_rate=0.2, label_smoothing=0.1,\n",
    "                                 momentum=0.9, nesterov=False, beta1=0.9, beta2=0.999, eps=1e-8,\n",
    "                                 data_storage_location=None, mds_train_dir=None, \n",
    "                                 mds_val_dir=None, num_epochs=2, num_classes=200,\n",
    "                                 mlflow_run_id=None, mlflow_tracking_uri=None,\n",
    "                                 mlflow_experiment_name=None, trial_number=0,\n",
    "                                 run_name=None, checkpoint_dir=None):\n",
    "    \"\"\"\n",
    "    Distributed training with best checkpoint tracking per trial.\n",
    "    Uses experiment-specific checkpoint directory structure.\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of output classes (added for architecture metadata)\n",
    "        ... (other args same as before)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import time\n",
    "    import shutil\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.distributed as dist\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "    from datetime import timedelta\n",
    "    import numpy as np\n",
    "    \n",
    "    # Initialize core variables\n",
    "    cache_paths = []\n",
    "    training_start_time = time.time()\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "    global_rank = int(os.environ.get(\"RANK\", 0))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    \n",
    "    # Core metrics storage\n",
    "    epoch_metrics = []\n",
    "    essential_model_metrics = {\n",
    "        'parameter_count': 0,\n",
    "        'trainable_parameters': 0,\n",
    "        'model_size_mb': 0.0,\n",
    "        'gradient_norms': [],\n",
    "        'weight_norms': [],\n",
    "        'learning_curves': {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    }\n",
    "    \n",
    "    # Checkpointing variables\n",
    "    start_epoch = 0\n",
    "    best_val_acc = 0.0\n",
    "    best_checkpoint_path = None\n",
    "    best_checkpoint_epoch = -1\n",
    "    checkpoint_paths = []\n",
    "    \n",
    "    # Device info\n",
    "    if global_rank == 0:\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Checkpoint directory: {checkpoint_dir}\")\n",
    "        print(f\"Model configuration: {num_classes} classes, dropout={dropout_rate}\")\n",
    "\n",
    "    try:\n",
    "        if global_rank == 0:\n",
    "            print(f\"Starting training with lr={lr}, batch_size={batch_size}, optimizer={optimizer_name}\")\n",
    "        \n",
    "        # Initialize distributed training\n",
    "        if world_size > 1:\n",
    "            dist.init_process_group(\"nccl\", timeout=timedelta(seconds=1800))\n",
    "        \n",
    "        device = torch.device(f'cuda:{local_rank}' if torch.cuda.is_available() else 'cpu')\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_device(local_rank)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Create model\n",
    "        model = get_model(lr=lr)\n",
    "        \n",
    "        # Add dropout to classifier\n",
    "        if hasattr(model, 'classifier') and isinstance(model.classifier, nn.Sequential):\n",
    "            if len(model.classifier) > 1 and hasattr(model.classifier[1], 'in_features'):\n",
    "                num_features = model.classifier[1].in_features\n",
    "                model.classifier = nn.Sequential(\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(num_features, 512),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(dropout_rate * 0.5),\n",
    "                    nn.Linear(512, num_classes)  # Use num_classes parameter\n",
    "                )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Calculate essential model metrics\n",
    "        if global_rank == 0:\n",
    "            essential_model_metrics['parameter_count'] = sum(p.numel() for p in model.parameters())\n",
    "            essential_model_metrics['trainable_parameters'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            essential_model_metrics['model_size_mb'] = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
    "            print(f\"Model: {essential_model_metrics['parameter_count']:,} total params, \"\n",
    "                  f\"{essential_model_metrics['trainable_parameters']:,} trainable\")\n",
    "        \n",
    "        if world_size > 1:\n",
    "            model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n",
    "        \n",
    "        # Setup training components\n",
    "        criterion = LabelSmoothingCrossEntropy(label_smoothing) if label_smoothing > 0 else nn.CrossEntropyLoss()\n",
    "        \n",
    "        model_params = [p for p in (model.module if world_size > 1 else model).parameters() if p.requires_grad]\n",
    "        \n",
    "        if optimizer_name == 'SGD':\n",
    "            optimizer = torch.optim.SGD(model_params, lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "        elif optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(model_params, lr=lr, weight_decay=weight_decay, betas=(beta1, beta2), eps=eps)\n",
    "        else:  # AdamW\n",
    "            optimizer = torch.optim.AdamW(model_params, lr=lr, weight_decay=weight_decay, betas=(beta1, beta2), eps=eps)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "        \n",
    "        # Check for existing checkpoint to resume from\n",
    "        if RESUME_FROM_CHECKPOINT and global_rank == 0 and checkpoint_dir:\n",
    "            latest_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
    "            if latest_checkpoint:\n",
    "                checkpoint_metadata = load_checkpoint(latest_checkpoint, model, optimizer, scheduler, device)\n",
    "                if checkpoint_metadata:\n",
    "                    start_epoch = checkpoint_metadata['epoch'] + 1\n",
    "                    best_val_acc = checkpoint_metadata['best_val_acc']\n",
    "                    print(f\"Resuming from epoch {start_epoch}, best_val_acc: {best_val_acc:.4f}\")\n",
    "        \n",
    "        # Synchronize start_epoch and best_val_acc across all ranks\n",
    "        if world_size > 1:\n",
    "            start_epoch_tensor = torch.tensor(start_epoch, device=device)\n",
    "            best_val_acc_tensor = torch.tensor(best_val_acc, device=device)\n",
    "            dist.broadcast(start_epoch_tensor, src=0)\n",
    "            dist.broadcast(best_val_acc_tensor, src=0)\n",
    "            start_epoch = int(start_epoch_tensor.item())\n",
    "            best_val_acc = float(best_val_acc_tensor.item())\n",
    "        \n",
    "        # Create data loaders with predictable cache paths\n",
    "        effective_batch_size = max(batch_size // world_size, 8) if world_size > 1 else batch_size\n",
    "        \n",
    "        train_input_remote_path = os.path.join(data_storage_location, mds_train_dir)\n",
    "        val_input_remote_path = os.path.join(data_storage_location, mds_val_dir)\n",
    "        \n",
    "        # Use predictable cache paths based on checkpoint_dir\n",
    "        if checkpoint_dir:\n",
    "            train_cache_base = f\"{checkpoint_dir}/cache/train\"\n",
    "            val_cache_base = f\"{checkpoint_dir}/cache/val\"\n",
    "        else:\n",
    "            train_cache_base = f\"/local_disk0/tmp/train_cache_{run_name or trial_number}\"\n",
    "            val_cache_base = f\"/local_disk0/tmp/val_cache_{run_name or trial_number}\"\n",
    "        \n",
    "        train_cache_path = f\"{train_cache_base}_rank{global_rank}\"\n",
    "        val_cache_path = f\"{val_cache_base}_rank{global_rank}\"\n",
    "        \n",
    "        train_dataloader, _ = get_dataloader_with_mosaic(\n",
    "            train_input_remote_path, train_cache_path, effective_batch_size, global_rank\n",
    "        )\n",
    "        cache_paths.append(train_cache_path)\n",
    "        \n",
    "        val_dataloader, _ = get_dataloader_with_mosaic(\n",
    "            val_input_remote_path, val_cache_path, effective_batch_size, global_rank\n",
    "        )\n",
    "        cache_paths.append(val_cache_path)\n",
    "        \n",
    "        # ============================================================\n",
    "        # DATALOADER METRICS COLLECTION\n",
    "        # ============================================================\n",
    "        if global_rank == 0:\n",
    "            train_batches = len(train_dataloader) if train_dataloader else 0\n",
    "            val_batches = len(val_dataloader) if val_dataloader else 0\n",
    "            \n",
    "            dataloader_metrics = {\n",
    "                'dataloader_train_batches_per_epoch': train_batches,\n",
    "                'dataloader_val_batches_per_epoch': val_batches,\n",
    "                'dataloader_effective_batch_size': effective_batch_size,\n",
    "                'dataloader_world_size': world_size,\n",
    "                'dataloader_samples_per_epoch_train': train_batches * effective_batch_size * world_size,\n",
    "                'dataloader_samples_per_epoch_val': val_batches * effective_batch_size * world_size,\n",
    "            }\n",
    "            \n",
    "            essential_model_metrics.update(dataloader_metrics)\n",
    "            \n",
    "            print(f\"\\nDataLoader Configuration:\")\n",
    "            print(f\"  Train batches/epoch: {train_batches}\")\n",
    "            print(f\"  Val batches/epoch: {val_batches}\")\n",
    "            print(f\"  Effective batch size: {effective_batch_size}\")\n",
    "            print(f\"  World size: {world_size}\")\n",
    "            print(f\"  Est. train samples/epoch: {dataloader_metrics['dataloader_samples_per_epoch_train']}\")\n",
    "            print(f\"  Est. val samples/epoch: {dataloader_metrics['dataloader_samples_per_epoch_val']}\\n\")\n",
    "        \n",
    "        if world_size > 1:\n",
    "            dist.barrier()\n",
    "        \n",
    "        # ============================================================\n",
    "        # TRAINING LOOP WITH ARCHITECTURE METADATA IN CHECKPOINTS\n",
    "        # ============================================================\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            if global_rank == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Training phase\n",
    "            train_results = train_one_epoch(\n",
    "                model, criterion, optimizer, scheduler, train_dataloader, \n",
    "                epoch, device, global_rank, label_to_idx\n",
    "            )\n",
    "            \n",
    "            # Validation phase\n",
    "            val_results = evaluate(\n",
    "                model, criterion, val_dataloader, epoch, device, global_rank, label_to_idx\n",
    "            )\n",
    "            \n",
    "            train_loss, train_acc = train_results['loss'], train_results['accuracy']\n",
    "            val_loss, val_acc = val_results['loss'], val_results['accuracy']\n",
    "            \n",
    "            # Check if this epoch is better than all previous epochs\n",
    "            is_best = val_acc > best_val_acc\n",
    "            \n",
    "            if is_best:\n",
    "                best_val_acc = val_acc\n",
    "                best_checkpoint_epoch = epoch\n",
    "                if global_rank == 0:\n",
    "                    print(f\"*** NEW BEST MODEL *** Epoch {epoch+1}, Val Acc: {best_val_acc:.4f}\")\n",
    "            \n",
    "            # Store metrics\n",
    "            if global_rank == 0:\n",
    "                essential_model_metrics['gradient_norms'].append(train_results.get('gradient_norm', 0.0))\n",
    "                essential_model_metrics['weight_norms'].append(train_results.get('weight_norm', 0.0))\n",
    "                essential_model_metrics['learning_curves']['train_loss'].append(float(train_loss))\n",
    "                essential_model_metrics['learning_curves']['val_loss'].append(float(val_loss))\n",
    "                essential_model_metrics['learning_curves']['train_acc'].append(float(train_acc))\n",
    "                essential_model_metrics['learning_curves']['val_acc'].append(float(val_acc))\n",
    "                \n",
    "                epoch_data = {\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': float(train_loss),\n",
    "                    'train_acc': float(train_acc),\n",
    "                    'val_loss': float(val_loss),\n",
    "                    'val_acc': float(val_acc),\n",
    "                    'learning_rate': float(optimizer.param_groups[0]['lr']),\n",
    "                    'gradient_norm': train_results.get('gradient_norm', 0.0),\n",
    "                    'weight_norm': train_results.get('weight_norm', 0.0),\n",
    "                    'is_best': is_best,\n",
    "                    'best_val_acc_so_far': float(best_val_acc)\n",
    "                }\n",
    "                epoch_metrics.append(epoch_data)\n",
    "                \n",
    "                # Enhanced logging with best indicator\n",
    "                status = \"[BEST]\" if is_best else f\"[Best: {best_val_acc:.4f}]\"\n",
    "                print(f'Epoch {epoch+1}: Train {train_acc:.4f}, Val {val_acc:.4f}, '\n",
    "                      f'Loss {train_loss:.4f}/{val_loss:.4f}, '\n",
    "                      f'LR {optimizer.param_groups[0][\"lr\"]:.2e} {status}')\n",
    "                \n",
    "                # ============================================================\n",
    "                # SAVE CHECKPOINT WITH ARCHITECTURE METADATA\n",
    "                # ============================================================\n",
    "                if ENABLE_CHECKPOINTING and checkpoint_dir and (epoch + 1) % CHECKPOINT_FREQUENCY == 0:\n",
    "                    checkpoint_filename = f\"checkpoint_epoch_{epoch:03d}.pt\"\n",
    "                    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
    "                    \n",
    "                    # Create checkpoint with full architecture metadata\n",
    "                    checkpoint_data = {\n",
    "                        # Training state\n",
    "                        'epoch': epoch,\n",
    "                        'trial_number': trial_number,\n",
    "                        'run_name': run_name,\n",
    "                        'best_val_acc': best_val_acc,\n",
    "                        'val_acc': val_acc,\n",
    "                        'train_loss': train_loss,\n",
    "                        'val_loss': val_loss,\n",
    "                        'is_best': is_best,\n",
    "                        \n",
    "                        # Model state\n",
    "                        'model_state_dict': (model.module if world_size > 1 else model).state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(),\n",
    "                        \n",
    "                        # Architecture metadata (CRITICAL)\n",
    "                        'num_classes': num_classes,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'model_architecture': 'mobilenetv2',\n",
    "                        \n",
    "                        # Hyperparameters\n",
    "                        'hyperparameters': {\n",
    "                            'lr': lr,\n",
    "                            'batch_size': batch_size,\n",
    "                            'optimizer': optimizer_name,\n",
    "                            'weight_decay': weight_decay,\n",
    "                            'dropout_rate': dropout_rate,\n",
    "                            'label_smoothing': label_smoothing,\n",
    "                            'momentum': momentum,\n",
    "                            'nesterov': nesterov,\n",
    "                            'beta1': beta1,\n",
    "                            'beta2': beta2,\n",
    "                            'eps': eps,\n",
    "                            'step_size': step_size,\n",
    "                            'gamma': gamma\n",
    "                        },\n",
    "                        \n",
    "                        # Metadata\n",
    "                        'timestamp': time.time(),\n",
    "                        'world_size': world_size\n",
    "                    }\n",
    "                    \n",
    "                    torch.save(checkpoint_data, checkpoint_path)\n",
    "                    checkpoint_paths.append(checkpoint_path)\n",
    "                    print(f\" Saved checkpoint: {checkpoint_filename}\")\n",
    "                    \n",
    "                    # If this is best, also save as best_checkpoint.pt\n",
    "                    if is_best:\n",
    "                        best_checkpoint_path = os.path.join(checkpoint_dir, \"best_checkpoint.pt\")\n",
    "                        torch.save(checkpoint_data, best_checkpoint_path)\n",
    "                        print(f\" Saved best checkpoint: best_checkpoint.pt\")\n",
    "                    \n",
    "                    # Save metadata JSON\n",
    "                    metadata = {\n",
    "                        'run_name': run_name,\n",
    "                        'trial_number': trial_number,\n",
    "                        'epoch': epoch,\n",
    "                        'best_val_acc': float(best_val_acc),\n",
    "                        'val_acc': float(val_acc),\n",
    "                        'train_loss': float(train_loss),\n",
    "                        'val_loss': float(val_loss),\n",
    "                        'is_best': is_best,\n",
    "                        'checkpoint_path': checkpoint_path,\n",
    "                        'num_classes': num_classes,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'model_architecture': 'mobilenetv2',\n",
    "                        'timestamp': time.time()\n",
    "                    }\n",
    "                    \n",
    "                    metadata_filename = f\"metadata_epoch_{epoch:03d}.json\"\n",
    "                    metadata_path = os.path.join(checkpoint_dir, metadata_filename)\n",
    "                    import json\n",
    "                    with open(metadata_path, 'w') as f:\n",
    "                        json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        training_time = time.time() - training_start_time\n",
    "        \n",
    "        # ============================================================\n",
    "        # SAVE FINAL CHECKPOINT WITH ARCHITECTURE METADATA\n",
    "        # ============================================================\n",
    "        if ENABLE_CHECKPOINTING and global_rank == 0 and checkpoint_dir:\n",
    "            final_checkpoint_filename = f\"checkpoint_epoch_{num_epochs-1:03d}.pt\"\n",
    "            final_checkpoint_path = os.path.join(checkpoint_dir, final_checkpoint_filename)\n",
    "            \n",
    "            # Only save if not already saved\n",
    "            if not os.path.exists(final_checkpoint_path):\n",
    "                final_is_best = (num_epochs - 1 == best_checkpoint_epoch)\n",
    "                \n",
    "                checkpoint_data = {\n",
    "                    'epoch': num_epochs - 1,\n",
    "                    'trial_number': trial_number,\n",
    "                    'run_name': run_name,\n",
    "                    'best_val_acc': best_val_acc,\n",
    "                    'val_acc': val_acc,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'is_best': final_is_best,\n",
    "                    'model_state_dict': (model.module if world_size > 1 else model).state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'num_classes': num_classes,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'model_architecture': 'mobilenetv2',\n",
    "                    'hyperparameters': {\n",
    "                        'lr': lr, 'batch_size': batch_size, 'optimizer': optimizer_name,\n",
    "                        'weight_decay': weight_decay, 'dropout_rate': dropout_rate,\n",
    "                        'label_smoothing': label_smoothing\n",
    "                    },\n",
    "                    'timestamp': time.time(),\n",
    "                    'world_size': world_size\n",
    "                }\n",
    "                \n",
    "                torch.save(checkpoint_data, final_checkpoint_path)\n",
    "                checkpoint_paths.append(final_checkpoint_path)\n",
    "                print(f\"Saved final checkpoint: {final_checkpoint_filename}\")\n",
    "\n",
    "                # Update best checkpoint if final is best\n",
    "                if final_is_best:\n",
    "                    best_checkpoint_path = os.path.join(checkpoint_dir, \"best_checkpoint.pt\")\n",
    "                    torch.save(checkpoint_data, best_checkpoint_path)\n",
    "                    print(f\"Final epoch is best - saved as best_checkpoint.pt\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # CALCULATE DATALOADER EFFICIENCY METRICS\n",
    "        # ============================================================\n",
    "        if global_rank == 0 and training_time > 0:\n",
    "            train_batches = essential_model_metrics.get('dataloader_train_batches_per_epoch', 0)\n",
    "            val_batches = essential_model_metrics.get('dataloader_val_batches_per_epoch', 0)\n",
    "            \n",
    "            if train_batches > 0:\n",
    "                avg_time_per_epoch = training_time / num_epochs\n",
    "                avg_time_per_batch = avg_time_per_epoch / train_batches\n",
    "                samples_per_second = (train_batches * effective_batch_size * world_size) / avg_time_per_epoch\n",
    "                \n",
    "                efficiency_metrics = {\n",
    "                    'dataloader_avg_time_per_epoch': avg_time_per_epoch,\n",
    "                    'dataloader_avg_time_per_batch': avg_time_per_batch,\n",
    "                    'dataloader_samples_per_second': samples_per_second,\n",
    "                    'dataloader_throughput_images_per_sec': samples_per_second,\n",
    "                }\n",
    "                \n",
    "                essential_model_metrics.update(efficiency_metrics)\n",
    "                \n",
    "                print(f\"\\nDataLoader Efficiency:\")\n",
    "                print(f\"  Avg time/epoch: {avg_time_per_epoch:.2f}s\")\n",
    "                print(f\"  Avg time/batch: {avg_time_per_batch:.4f}s\")\n",
    "                print(f\"  Throughput: {samples_per_second:.1f} samples/sec\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # FINAL SUMMARY\n",
    "        # ============================================================\n",
    "        if global_rank == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Training Complete - {run_name or f'Trial {trial_number}'}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Best Validation Accuracy: {best_val_acc:.4f} (Epoch {best_checkpoint_epoch+1})\")\n",
    "            print(f\"Final Validation Accuracy: {val_acc:.4f}\")\n",
    "            print(f\"Training Time: {training_time:.2f}s ({training_time/60:.1f} min)\")\n",
    "            if best_checkpoint_path:\n",
    "                print(f\"Best Checkpoint: {best_checkpoint_path}\")\n",
    "            print(f\"Checkpoint Directory: {checkpoint_dir}\")\n",
    "            print(f\"Total Checkpoints Saved: {len(checkpoint_paths)}\")\n",
    "            print(f\"Model Architecture: {num_classes} classes, dropout={dropout_rate}\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # RETURN RESULTS WITH ARCHITECTURE INFO\n",
    "        # ============================================================\n",
    "        return {\n",
    "            \"val_acc\": float(best_val_acc),\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"train_acc\": float(train_acc),\n",
    "            \"best_val_acc\": float(best_val_acc),\n",
    "            \"best_checkpoint_path\": best_checkpoint_path,\n",
    "            \"best_checkpoint_epoch\": best_checkpoint_epoch,\n",
    "            \"final_val_acc\": float(val_acc),\n",
    "            \"status\": \"completed\",\n",
    "            \"epochs_completed\": num_epochs,\n",
    "            \"training_time\": training_time,\n",
    "            \"epoch_metrics\": epoch_metrics,\n",
    "            \"model_metrics\": essential_model_metrics,\n",
    "            \"checkpoint_paths\": checkpoint_paths,\n",
    "            \"checkpoint_dir\": checkpoint_dir,\n",
    "            \"hyperparameters\": {\n",
    "                \"lr\": lr,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"optimizer\": optimizer_name,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"dropout_rate\": dropout_rate,\n",
    "                \"label_smoothing\": label_smoothing,\n",
    "                \"num_classes\": num_classes  # Include in return\n",
    "            },\n",
    "            \"architecture\": {\n",
    "                \"num_classes\": num_classes,\n",
    "                \"dropout_rate\": dropout_rate,\n",
    "                \"model_name\": \"mobilenetv2\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Rank {global_rank}: Training error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            \"val_acc\": 0.0,\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "            \"checkpoint_paths\": checkpoint_paths,\n",
    "            \"best_checkpoint_path\": best_checkpoint_path,\n",
    "            \"checkpoint_dir\": checkpoint_dir,\n",
    "            \"epoch_metrics\": epoch_metrics,\n",
    "            \"model_metrics\": essential_model_metrics,\n",
    "            \"architecture\": {\n",
    "                \"num_classes\": num_classes,\n",
    "                \"dropout_rate\": dropout_rate,\n",
    "                \"model_name\": \"mobilenetv2\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup cache paths\n",
    "        for cache_path in cache_paths:\n",
    "            if os.path.exists(cache_path):\n",
    "                shutil.rmtree(cache_path, ignore_errors=True)\n",
    "        \n",
    "        # Cleanup distributed process group\n",
    "        if world_size > 1 and dist.is_initialized():\n",
    "            dist.destroy_process_group()\n",
    "\n",
    "\n",
    "\n",
    "##----------------------------------------------------------------            \n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    ## import torch.nn as nn ## moved up in dependencies import\n",
    "\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(1)\n",
    "        log_preds = torch.log_softmax(pred, dim=1)\n",
    "        smooth_target = torch.zeros_like(log_preds).scatter_(1, target.unsqueeze(1), 1)\n",
    "        smooth_target = smooth_target * (1 - self.smoothing) + self.smoothing / n_classes\n",
    "        return (-smooth_target * log_preds).sum(dim=1).mean()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, criterion, optimizer, scheduler, train_dataloader, \n",
    "                             epoch, device, global_rank, label_to_idx):\n",
    "    \n",
    "    \"\"\"Define training loop with essential metrics\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    successful_batches = 0\n",
    "    gradient_norms = []\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        try:\n",
    "            inputs, labels = convert_batch_to_tensors(batch, device, label_to_idx, global_rank)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Calculate gradient norm\n",
    "            if global_rank == 0 and step % 50 == 0:\n",
    "                grad_norm = calculate_gradient_norm(model, 1 if torch.distributed.get_world_size() == 1 else torch.distributed.get_world_size())\n",
    "                gradient_norms.append(grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            successful_batches += 1\n",
    "            \n",
    "            if global_rank == 0 and step % 100 == 0:\n",
    "                current_acc = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "                print(f\"  Step {step}: Loss {loss.item():.4f}, Acc {current_acc:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            if step < 5:\n",
    "                print(f\"Training step {step} error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_loss = running_loss / successful_batches if successful_batches > 0 else 0.0\n",
    "    epoch_acc = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "    avg_gradient_norm = np.mean(gradient_norms) if gradient_norms else 0.0\n",
    "    \n",
    "    return {\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': epoch_acc,\n",
    "        'gradient_norm': avg_gradient_norm,\n",
    "        'weight_norm': calculate_weight_norm(model, 1 if not torch.distributed.is_initialized() else torch.distributed.get_world_size())\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, val_dataloader, epoch, device, global_rank, label_to_idx):\n",
    "    \"\"\"Define evaluation loop\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    successful_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_dataloader):\n",
    "            try:\n",
    "                inputs, labels = convert_batch_to_tensors(batch, device, label_to_idx, global_rank)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                successful_batches += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    val_loss = running_loss / successful_batches if successful_batches > 0 else 0.0\n",
    "    val_acc = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "    \n",
    "    return {'loss': val_loss, 'accuracy': val_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48915cb4-d53c-41d2-8f2f-92a83b159d53",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Checkpointing Utility Functions"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHECKPOINT UTILITIES \n",
    "# ============================================================\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "## this wrapper is defined but not used in the end.\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, trial_number,\n",
    "                     best_val_acc, checkpoint_dir, \n",
    "                     global_rank=0, world_size=1, run_name=None,\n",
    "                     num_classes=200, dropout_rate=0.2, val_acc=0.0, train_loss=0.0, val_loss=0.0, is_best=False):\n",
    "    \"\"\"\n",
    "    Save model checkpoint with architecture metadata.\n",
    "    Checkpoints are saved directly in the trial directory.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        epoch: Current epoch\n",
    "        trial_number: Trial number\n",
    "        best_val_acc: Best validation accuracy so far\n",
    "        checkpoint_dir: Trial-specific checkpoint directory\n",
    "        global_rank: Process rank (only rank 0 saves)\n",
    "        world_size: Total number of processes\n",
    "        run_name: Run name (for metadata only)\n",
    "        num_classes: Number of output classes (for architecture reconstruction)\n",
    "        dropout_rate: Dropout rate used in model (for architecture reconstruction)\n",
    "        val_acc: Current epoch validation accuracy\n",
    "        train_loss: Current epoch training loss\n",
    "        val_loss: Current epoch validation loss\n",
    "        is_best: Whether this is the best checkpoint so far\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to saved checkpoint or None\n",
    "    \"\"\"\n",
    "    if global_rank != 0:  # Only rank 0 saves checkpoints\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Checkpoint directory should already be trial-specific\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Prepare checkpoint data with architecture metadata\n",
    "        checkpoint_data = {\n",
    "            # Training state\n",
    "            'epoch': epoch,\n",
    "            'trial_number': trial_number,\n",
    "            'run_name': run_name,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'val_acc': val_acc,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'is_best': is_best,\n",
    "            \n",
    "            # Model state\n",
    "            'model_state_dict': model.module.state_dict() if world_size > 1 else model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            \n",
    "            # Architecture metadata (CRITICAL for loading)\n",
    "            'num_classes': num_classes,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'model_architecture': 'mobilenetv2',\n",
    "            \n",
    "            # Metadata\n",
    "            'timestamp': time.time(),\n",
    "            'world_size': world_size\n",
    "        }\n",
    "        \n",
    "        # Save epoch checkpoint with zero-padded naming\n",
    "        checkpoint_filename = f\"checkpoint_epoch_{epoch:03d}.pt\"\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
    "        torch.save(checkpoint_data, checkpoint_path)\n",
    "        \n",
    "        # Also save as best_checkpoint.pt if this is the best\n",
    "        if is_best:\n",
    "            best_checkpoint_path = os.path.join(checkpoint_dir, \"best_checkpoint.pt\")\n",
    "            torch.save(checkpoint_data, best_checkpoint_path)\n",
    "            print(f\"★ Saved best checkpoint: best_checkpoint.pt (epoch {epoch})\")\n",
    "        \n",
    "        # Save metadata JSON\n",
    "        metadata = {\n",
    "            'run_name': run_name,\n",
    "            'trial_number': trial_number,\n",
    "            'epoch': epoch,\n",
    "            'best_val_acc': float(best_val_acc),\n",
    "            'val_acc': float(val_acc),\n",
    "            'train_loss': float(train_loss),\n",
    "            'val_loss': float(val_loss),\n",
    "            'is_best': is_best,\n",
    "            'checkpoint_path': checkpoint_path,\n",
    "            'num_classes': num_classes,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'model_architecture': 'mobilenetv2',\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        metadata_filename = f\"metadata_epoch_{epoch:03d}.json\"\n",
    "        metadata_path = os.path.join(checkpoint_dir, metadata_filename)\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"✓ Saved checkpoint: {checkpoint_filename} (val_acc: {val_acc:.4f})\")\n",
    "        return checkpoint_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error saving checkpoint: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer=None, scheduler=None, device=None):\n",
    "    \"\"\"\n",
    "    Load checkpoint and return metadata.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        model: PyTorch model to load state into\n",
    "        optimizer: Optimizer (optional)\n",
    "        scheduler: Scheduler (optional)\n",
    "        device: Device to load tensors to\n",
    "        \n",
    "    Returns:\n",
    "        dict: Checkpoint metadata or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if device is None:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Load model state\n",
    "        if hasattr(model, 'module'):\n",
    "            model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Load optimizer state if provided\n",
    "        if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        # Load scheduler state if provided\n",
    "        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        metadata = {\n",
    "            'epoch': checkpoint.get('epoch', 0),\n",
    "            'trial_number': checkpoint.get('trial_number', 0),\n",
    "            'run_name': checkpoint.get('run_name', None),\n",
    "            'best_val_acc': checkpoint.get('best_val_acc', 0.0),\n",
    "            'timestamp': checkpoint.get('timestamp', 0)\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ Loaded checkpoint: {os.path.basename(checkpoint_path)}\")\n",
    "        print(f\"  Epoch: {metadata['epoch']}, Val Acc: {metadata['best_val_acc']:.4f}\")\n",
    "        \n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error loading checkpoint: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Find the latest checkpoint in a directory.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dir: Directory containing checkpoints\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to latest checkpoint or None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    \n",
    "    # Find all epoch checkpoints\n",
    "    pattern = os.path.join(checkpoint_dir, \"checkpoint_epoch_*.pt\")\n",
    "    checkpoints = glob.glob(pattern)\n",
    "    \n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    \n",
    "    # Sort by epoch number (extracted from filename)\n",
    "    checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    latest = checkpoints[-1]\n",
    "    \n",
    "    print(f\"Found latest checkpoint: {os.path.basename(latest)}\")\n",
    "    return latest\n",
    "\n",
    "\n",
    "def get_checkpoint_info(checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Get information about all checkpoints in a directory.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dir: Directory containing checkpoints\n",
    "        \n",
    "    Returns:\n",
    "        dict: Checkpoint information\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return {\n",
    "            'exists': False,\n",
    "            'checkpoint_count': 0,\n",
    "            'checkpoints': []\n",
    "        }\n",
    "    \n",
    "    checkpoint_files = sorted(glob.glob(os.path.join(checkpoint_dir, \"checkpoint_epoch_*.pt\")))\n",
    "    best_checkpoint = os.path.join(checkpoint_dir, \"best_checkpoint.pt\")\n",
    "    \n",
    "    checkpoint_info = {\n",
    "        'exists': True,\n",
    "        'checkpoint_dir': checkpoint_dir,\n",
    "        'checkpoint_count': len(checkpoint_files),\n",
    "        'has_best': os.path.exists(best_checkpoint),\n",
    "        'checkpoints': []\n",
    "    }\n",
    "    \n",
    "    for cp_path in checkpoint_files:\n",
    "        cp_size = os.path.getsize(cp_path) / (1024 * 1024)  # MB\n",
    "        epoch = int(cp_path.split('_')[-1].split('.')[0])\n",
    "        \n",
    "        checkpoint_info['checkpoints'].append({\n",
    "            'epoch': epoch,\n",
    "            'filename': os.path.basename(cp_path),\n",
    "            'path': cp_path,\n",
    "            'size_mb': round(cp_size, 2)\n",
    "        })\n",
    "    \n",
    "    if checkpoint_info['has_best']:\n",
    "        best_size = os.path.getsize(best_checkpoint) / (1024 * 1024)\n",
    "        checkpoint_info['best_checkpoint'] = {\n",
    "            'filename': 'best_checkpoint.pt',\n",
    "            'path': best_checkpoint,\n",
    "            'size_mb': round(best_size, 2)\n",
    "        }\n",
    "    \n",
    "    return checkpoint_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "748da80e-9adf-4bdd-bdb2-58f3402adc4e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "example usage"
    }
   },
   "outputs": [],
   "source": [
    "# # Example 1: Find and load latest checkpoint\n",
    "# checkpoint_dir = f\"{CHECKPOINT_BASE_DIR}/trial_005_AdamW_lr1.23e-03_bs64\"\n",
    "# latest_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "# if latest_checkpoint:\n",
    "#     metadata = load_checkpoint(latest_checkpoint, model, optimizer, scheduler)\n",
    "#     start_epoch = metadata['epoch'] + 1\n",
    "\n",
    "# # Example 2: Get checkpoint info\n",
    "# info = get_checkpoint_info(checkpoint_dir)\n",
    "# print(f\"Found {info['checkpoint_count']} checkpoints\")\n",
    "# print(f\"Has best checkpoint: {info['has_best']}\")\n",
    "\n",
    "# # Example 3: Load best checkpoint\n",
    "# best_checkpoint_path = os.path.join(checkpoint_dir, \"best_checkpoint.pt\")\n",
    "# if os.path.exists(best_checkpoint_path):\n",
    "#     model_state = torch.load(best_checkpoint_path)\n",
    "#     model.load_state_dict(model_state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24039e51-3eab-4e97-b9b1-1e2492f466df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "functions to calculate gradients"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_gradient_norm(model, world_size):\n",
    "    \"\"\"Calculate L2 norm of gradients\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    total_norm = 0.0\n",
    "    param_count = 0\n",
    "    \n",
    "    if world_size > 1:\n",
    "        parameters = model.module.parameters()\n",
    "    else:\n",
    "        parameters = model.parameters()\n",
    "    \n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "            param_count += 1\n",
    "    \n",
    "    return (total_norm ** 0.5) if param_count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_weight_norm(model, world_size):\n",
    "    \"\"\"Calculate L2 norm of model weights\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    total_norm = 0.0\n",
    "    param_count = 0\n",
    "    \n",
    "    if world_size > 1:\n",
    "        parameters = model.module.parameters()\n",
    "    else:\n",
    "        parameters = model.parameters()\n",
    "    \n",
    "    for p in parameters:\n",
    "        param_norm = p.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "        param_count += 1\n",
    "    \n",
    "    return (total_norm ** 0.5) if param_count > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1398722-90d5-42d9-8123-ec362e1e6831",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "[optuna] objective_function"
    }
   },
   "outputs": [],
   "source": [
    "def objective_function(trial):\n",
    "    \"\"\"\n",
    "    Objective function with experiment-specific checkpoint organization.\n",
    "    Uses predictable paths and avoids temporary file names.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import os\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    import mlflow \n",
    "    import torch\n",
    "    from pyspark.ml.torch.distributor import TorchDistributor\n",
    "    \n",
    "    # ============================================================\n",
    "    # HYPERPARAMETER SAMPLING\n",
    "    # ============================================================\n",
    "    \n",
    "    # Core hyperparameters\n",
    "    lr = trial.suggest_float('lr', 5e-5, 5e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['AdamW', 'SGD', 'Adam'])\n",
    "    \n",
    "    # Optimizer-specific weight decay\n",
    "    if optimizer_name == 'AdamW':\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-1, log=True)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
    "    else:\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "    \n",
    "    # Additional key hyperparameters\n",
    "    step_size = trial.suggest_int('step_size', 5, 15)\n",
    "    gamma = trial.suggest_float('gamma', 0.1, 0.7)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    label_smoothing = trial.suggest_float('label_smoothing', 0.0, 0.2)\n",
    "    \n",
    "    # Optimizer-specific parameters\n",
    "    momentum = trial.suggest_float('momentum', 0.85, 0.99) if optimizer_name == 'SGD' else 0.9\n",
    "    nesterov = trial.suggest_categorical('nesterov', [True, False]) if optimizer_name == 'SGD' else False\n",
    "    beta1 = trial.suggest_float('beta1', 0.85, 0.95) if optimizer_name == 'AdamW' else 0.9\n",
    "    beta2 = trial.suggest_float('beta2', 0.95, 0.999) if optimizer_name == 'AdamW' else 0.999\n",
    "    eps = trial.suggest_float('eps', 1e-9, 1e-7, log=True) if optimizer_name == 'AdamW' else 1e-8\n",
    "    \n",
    "    print(f\"Trial {trial.number}: {optimizer_name}, lr={lr:.2e}, bs={batch_size}, wd={weight_decay:.2e}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # PREDICTABLE PATH SETUP\n",
    "    # ============================================================\n",
    "    \n",
    "    # Create descriptive run name (no random components)\n",
    "    run_name = f\"trial_{trial.number:03d}_{optimizer_name}_lr{lr:.2e}_bs{batch_size}\"\n",
    "    \n",
    "    # Trial-specific checkpoint directory (predictable, organized)\n",
    "    trial_checkpoint_dir = os.path.join(CHECKPOINT_BASE_DIR, run_name)\n",
    "    Path(trial_checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # MLflow artifact subpath (consistent naming)\n",
    "    mlflow_artifact_subpath = \"checkpoints\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRIAL {trial.number} PATHS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Experiment Root:     {EXPERIMENT_ROOT}\")\n",
    "    print(f\"Checkpoint Base:     {CHECKPOINT_BASE_DIR}\")\n",
    "    print(f\"Trial Directory:     {trial_checkpoint_dir}\")\n",
    "    print(f\"Run Name:            {run_name}\")\n",
    "    print(f\"MLflow Artifact Sub: {mlflow_artifact_subpath}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # MLFLOW RUN SETUP\n",
    "    # ============================================================\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name, nested=True) as trial_run:\n",
    "        trial_start_time = time.time()\n",
    "        trial_run_id = trial_run.info.run_id\n",
    "        \n",
    "        print(f\"MLflow Run ID: {trial_run_id}\")\n",
    "        print(f\"Artifact URI:  {trial_run.info.artifact_uri}\\n\")\n",
    "        \n",
    "        # Log essential parameters\n",
    "        trial_params = {\n",
    "            'experiment_run_id': EXPERIMENT_RUN_ID,\n",
    "            'experiment_timestamp': EXPERIMENT_TIMESTAMP,\n",
    "            'experiment_short_name': EXPERIMENT_SHORT_NAME,\n",
    "            'trial_number': trial.number,\n",
    "            'run_name': run_name,\n",
    "            \n",
    "            # Hyperparameters\n",
    "            'lr': lr, \n",
    "            'batch_size': batch_size, \n",
    "            'optimizer': optimizer_name,\n",
    "            'weight_decay': weight_decay, \n",
    "            'step_size': step_size, \n",
    "            'gamma': gamma,\n",
    "            'dropout_rate': dropout_rate, \n",
    "            'label_smoothing': label_smoothing,\n",
    "            'momentum': momentum, \n",
    "            'nesterov': nesterov, \n",
    "            'beta1': beta1, \n",
    "            'beta2': beta2, \n",
    "            'eps': eps,\n",
    "            \n",
    "            # Training config\n",
    "            'num_epochs': NUM_EPOCHS, \n",
    "            'num_workers': num_workers,\n",
    "            'model_architecture': 'mobilenetv2',\n",
    "            \n",
    "            # Dataloader configuration\n",
    "            'dataloader_type': 'streaming_mds',\n",
    "            'dataloader_library': 'mosaic_streaming',\n",
    "            'dataloader_num_workers': 0,\n",
    "            'dataloader_pin_memory': True,\n",
    "            'dataloader_drop_last': True,\n",
    "            'dataloader_persistent_workers': False,\n",
    "            'dataloader_shuffle': True,\n",
    "            'dataloader_predownload_multiplier': 4,\n",
    "            'dataloader_keep_zip': False,\n",
    "            'dataloader_download_retry': 2,\n",
    "            'dataloader_download_timeout': 300,\n",
    "            'dataloader_validate_hash': False,\n",
    "            \n",
    "            # Dataset configuration\n",
    "            'dataset_format': 'mds',\n",
    "            'dataset_storage_type': 'uc_volumes',\n",
    "            'dataset_train_path': f\"{data_storage_location}/{mds_train_dir}\",\n",
    "            'dataset_val_path': f\"{data_storage_location}/{mds_val_dir}\",\n",
    "            'dataset_num_classes': num_classes,\n",
    "            'dataset_cache_location': '/local_disk0/tmp/mds_cache',\n",
    "            \n",
    "            # Checkpointing\n",
    "            'checkpointing_enabled': ENABLE_CHECKPOINTING,\n",
    "            'checkpoint_frequency': CHECKPOINT_FREQUENCY,\n",
    "            'checkpoint_base_dir': CHECKPOINT_BASE_DIR,\n",
    "            'checkpoint_trial_dir': trial_checkpoint_dir,\n",
    "            'mlflow_artifact_subpath': mlflow_artifact_subpath,\n",
    "            'experiment_root': EXPERIMENT_ROOT\n",
    "        }\n",
    "        mlflow.log_params(trial_params)\n",
    "\n",
    "        # ============================================================\n",
    "        # LOG DATASETS AS JSON artifact\n",
    "        # ============================================================                \n",
    "        try:\n",
    "            dataset_metadata = {\n",
    "                \"train\": {\n",
    "                    \"source\": f\"{data_storage_location}/{mds_train_dir}\",\n",
    "                    \"format\": \"mds\",\n",
    "                    \"storage\": \"uc_volumes\",\n",
    "                    \"num_classes\": num_classes,\n",
    "                    \"catalog\": CATALOG,\n",
    "                    \"schema\": SCHEMA,\n",
    "                    \"volume\": VOLUME_NAME,\n",
    "                    \"split\": \"train\"\n",
    "                },\n",
    "                \"validation\": {\n",
    "                    \"source\": f\"{data_storage_location}/{mds_val_dir}\",\n",
    "                    \"format\": \"mds\",\n",
    "                    \"storage\": \"uc_volumes\",\n",
    "                    \"num_classes\": num_classes,\n",
    "                    \"catalog\": CATALOG,\n",
    "                    \"schema\": SCHEMA,\n",
    "                    \"volume\": VOLUME_NAME,\n",
    "                    \"split\": \"validation\"\n",
    "                },\n",
    "                \"experiment_timestamp\": EXPERIMENT_TIMESTAMP,\n",
    "                \"trial_number\": trial.number,\n",
    "                \"logged_at\": time.time()\n",
    "            }\n",
    "            \n",
    "            # Save to trial checkpoint directory\n",
    "            dataset_metadata_path = os.path.join(trial_checkpoint_dir, \"dataset_metadata.json\")\n",
    "            with open(dataset_metadata_path, 'w') as f:\n",
    "                json.dump(dataset_metadata, f, indent=2)\n",
    "            \n",
    "            # Log to MLflow\n",
    "            mlflow.log_artifact(dataset_metadata_path, \"datasets\")\n",
    "            print(\"✓ Logged dataset metadata as artifact\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Dataset metadata logging failed: {e}\")\n",
    "            \n",
    "        try:\n",
    "            # ============================================================\n",
    "            # DISTRIBUTED TRAINING EXECUTION\n",
    "            # ============================================================\n",
    "            \n",
    "            distributor = TorchDistributor(num_processes=num_workers, local_mode=False, use_gpu=True)\n",
    "            result = distributor.run(\n",
    "                distributed_train_and_evaluate,\n",
    "                lr=lr, \n",
    "                batch_size=batch_size, \n",
    "                optimizer_name=optimizer_name,\n",
    "                weight_decay=weight_decay, \n",
    "                step_size=step_size, \n",
    "                gamma=gamma,\n",
    "                dropout_rate=dropout_rate, \n",
    "                label_smoothing=label_smoothing,\n",
    "                momentum=momentum, \n",
    "                nesterov=nesterov, \n",
    "                beta1=beta1, \n",
    "                beta2=beta2, \n",
    "                eps=eps,\n",
    "                data_storage_location=data_storage_location,\n",
    "                mds_train_dir=mds_train_dir, \n",
    "                mds_val_dir=mds_val_dir,\n",
    "                num_epochs=NUM_EPOCHS,\n",
    "                num_classes=num_classes,  \n",
    "                mlflow_run_id=trial_run_id,\n",
    "                mlflow_tracking_uri=mlflow.get_tracking_uri(),\n",
    "                trial_number=trial.number,\n",
    "                run_name=run_name,\n",
    "                checkpoint_dir=trial_checkpoint_dir\n",
    "            )\n",
    "            \n",
    "            trial_end_time = time.time()\n",
    "            trial_duration = trial_end_time - trial_start_time\n",
    "            \n",
    "            # ============================================================\n",
    "            # RESULT VALIDATION\n",
    "            # ============================================================\n",
    "            \n",
    "            if not isinstance(result, dict):\n",
    "                print(f\"WARNING: Result is not a dict, got {type(result)}\")\n",
    "                mlflow.log_param(\"result_type_error\", f\"expected_dict_got_{type(result).__name__}\")\n",
    "                mlflow.log_metric(\"final_validation_accuracy\", 0.0)\n",
    "                mlflow.set_tag(\"trial_status\", \"failed_invalid_result\")\n",
    "                return 0.0\n",
    "            \n",
    "            # Extract key metrics\n",
    "            trial_metric = result.get('val_acc', 0.0)\n",
    "            best_val_acc = result.get('best_val_acc', trial_metric)\n",
    "            final_val_acc = result.get('final_val_acc', trial_metric)\n",
    "            best_checkpoint_path = result.get('best_checkpoint_path')\n",
    "            best_checkpoint_epoch = result.get('best_checkpoint_epoch', -1)\n",
    "            checkpoint_paths = result.get('checkpoint_paths', [])\n",
    "            returned_checkpoint_dir = result.get('checkpoint_dir', '')\n",
    "            \n",
    "            # Verify checkpoint directory\n",
    "            if returned_checkpoint_dir != trial_checkpoint_dir:\n",
    "                print(f\"⚠ WARNING: Checkpoint directory mismatch!\")\n",
    "                print(f\"  Expected: {trial_checkpoint_dir}\")\n",
    "                print(f\"  Returned: {returned_checkpoint_dir}\")\n",
    "                mlflow.log_param(\"checkpoint_dir_mismatch\", \"yes\")\n",
    "            else:\n",
    "                mlflow.log_param(\"checkpoint_dir_verified\", \"yes\")\n",
    "            \n",
    "            # ============================================================\n",
    "            # CHECKPOINT LOGGING TO MLFLOW\n",
    "            # ============================================================\n",
    "\n",
    "            if checkpoint_paths:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"LOGGING CHECKPOINTS - Trial {trial.number}\")\n",
    "                print(f\"{'='*60}\")\n",
    "                print(f\"Total checkpoints: {len(checkpoint_paths)}\")\n",
    "                print(f\"Physical location: {trial_checkpoint_dir}\")\n",
    "                print(f\"MLflow artifact:   {mlflow_artifact_subpath}\")\n",
    "                \n",
    "                logged_count = 0\n",
    "                failed_count = 0\n",
    "                total_size_mb = 0.0\n",
    "                \n",
    "                checkpoint_manifest = {\n",
    "                    \"trial_number\": trial.number,\n",
    "                    \"run_name\": run_name,\n",
    "                    \"physical_directory\": trial_checkpoint_dir,\n",
    "                    \"mlflow_artifact_uri\": f\"{trial_run.info.artifact_uri}/{mlflow_artifact_subpath}\",\n",
    "                    \"total_checkpoints\": len(checkpoint_paths),\n",
    "                    \"checkpoint_files\": []\n",
    "                }\n",
    "                \n",
    "                # Sort checkpoints for consistent ordering\n",
    "                checkpoint_paths_sorted = sorted(checkpoint_paths)\n",
    "                \n",
    "                # ============================================================\n",
    "                # Log epoch checkpoints (INSIDE LOOP)\n",
    "                # ============================================================\n",
    "                for checkpoint_path in checkpoint_paths_sorted:\n",
    "                    if os.path.exists(checkpoint_path):\n",
    "                        try:\n",
    "                            checkpoint_filename = os.path.basename(checkpoint_path)\n",
    "                            checkpoint_size_mb = os.path.getsize(checkpoint_path) / (1024 * 1024)\n",
    "                            total_size_mb += checkpoint_size_mb\n",
    "                            \n",
    "                            # Load checkpoint to check if it was the best epoch\n",
    "                            checkpoint_data = torch.load(checkpoint_path, map_location='cpu')\n",
    "                            is_best_epoch = checkpoint_data.get('is_best', False)\n",
    "                            checkpoint_epoch = checkpoint_data.get('epoch', -1)\n",
    "                            \n",
    "                            # Log checkpoint file\n",
    "                            mlflow.log_artifact(checkpoint_path, mlflow_artifact_subpath)\n",
    "                            logged_count += 1\n",
    "                            \n",
    "                            # Determine if this is the best checkpoint\n",
    "                            is_best = (checkpoint_epoch == best_checkpoint_epoch)\n",
    "                            \n",
    "                            marker = \"★\" if is_best else \"✓\"\n",
    "                            print(f\"  {marker} {checkpoint_filename} ({checkpoint_size_mb:.2f} MB)\"\n",
    "                                f\"{' [BEST EPOCH]' if is_best else ''}\")\n",
    "                            \n",
    "                            # Add to manifest\n",
    "                            checkpoint_manifest[\"checkpoint_files\"].append({\n",
    "                                \"filename\": checkpoint_filename,\n",
    "                                \"epoch\": checkpoint_epoch,\n",
    "                                \"size_mb\": round(checkpoint_size_mb, 2),\n",
    "                                \"physical_path\": checkpoint_path,\n",
    "                                \"is_best\": is_best,\n",
    "                                \"was_best_when_saved\": is_best_epoch\n",
    "                            })\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            failed_count += 1\n",
    "                            print(f\"  ⚠ Failed to log {checkpoint_filename}: {e}\")\n",
    "                    else:\n",
    "                        failed_count += 1\n",
    "                        print(f\"  ⚠ Not found: {checkpoint_path}\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # Log best_checkpoint.pt ONCE (OUTSIDE LOOP) - FIXED\n",
    "                # ============================================================\n",
    "                best_checkpoint_file = os.path.join(trial_checkpoint_dir, \"best_checkpoint.pt\")\n",
    "                if os.path.exists(best_checkpoint_file):\n",
    "                    try:\n",
    "                        best_size_mb = os.path.getsize(best_checkpoint_file) / (1024 * 1024)\n",
    "                        total_size_mb += best_size_mb\n",
    "                        \n",
    "                        mlflow.log_artifact(best_checkpoint_file, mlflow_artifact_subpath)\n",
    "                        logged_count += 1\n",
    "                        \n",
    "                        print(f\"  ★ best_checkpoint.pt ({best_size_mb:.2f} MB) [BEST MODEL]\")\n",
    "                        \n",
    "                        checkpoint_manifest[\"checkpoint_files\"].append({\n",
    "                            \"filename\": \"best_checkpoint.pt\",\n",
    "                            \"epoch\": best_checkpoint_epoch,\n",
    "                            \"size_mb\": round(best_size_mb, 2),\n",
    "                            \"physical_path\": best_checkpoint_file,\n",
    "                            \"is_best\": True,\n",
    "                            \"was_best_when_saved\": True,\n",
    "                            \"note\": \"Copy of best epoch checkpoint\"\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"  ⚠ Failed to log best_checkpoint.pt: {e}\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # Finalize manifest (OUTSIDE LOOP) - FIXED\n",
    "                # ============================================================\n",
    "                checkpoint_manifest.update({\n",
    "                    \"logged_count\": logged_count,\n",
    "                    \"failed_count\": failed_count,\n",
    "                    \"total_size_mb\": round(total_size_mb, 2),\n",
    "                    \"best_checkpoint\": {\n",
    "                        \"filename\": os.path.basename(best_checkpoint_path) if best_checkpoint_path else None,\n",
    "                        \"epoch\": best_checkpoint_epoch,\n",
    "                        \"validation_accuracy\": best_val_acc\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                print(f\"\\nSummary: {logged_count}/{len(checkpoint_paths) + 1} logged, {total_size_mb:.2f} MB\")\n",
    "                print(f\"{'='*60}\\n\")\n",
    "                \n",
    "                # Log checkpoint summary metrics\n",
    "                mlflow.log_metrics({\n",
    "                    \"total_checkpoints_saved\": len(checkpoint_paths),\n",
    "                    \"checkpoints_logged_to_mlflow\": logged_count,\n",
    "                    \"checkpoints_failed_to_log\": failed_count,\n",
    "                    \"total_checkpoints_size_mb\": total_size_mb\n",
    "                })\n",
    "                \n",
    "                # Save and log manifest with predictable name\n",
    "                manifest_filename = f\"checkpoint_manifest_trial_{trial.number:03d}.json\"\n",
    "                manifest_path = os.path.join(trial_checkpoint_dir, manifest_filename)\n",
    "                \n",
    "                with open(manifest_path, 'w') as f:\n",
    "                    json.dump(checkpoint_manifest, f, indent=2)\n",
    "                \n",
    "                mlflow.log_artifact(manifest_path, mlflow_artifact_subpath)\n",
    "                print(f\"✓ Logged manifest: {manifest_filename}\\n\")\n",
    "            \n",
    "            # ============================================================\n",
    "            # MODEL METRICS LOGGING\n",
    "            # ============================================================\n",
    "            \n",
    "            model_metrics = result.get('model_metrics', {})\n",
    "            if model_metrics:\n",
    "                mlflow.log_metrics({\n",
    "                    \"model_parameter_count\": model_metrics.get('parameter_count', 0),\n",
    "                    \"model_trainable_parameters\": model_metrics.get('trainable_parameters', 0),\n",
    "                    \"model_size_mb\": model_metrics.get('model_size_mb', 0.0)\n",
    "                })\n",
    "            \n",
    "                gradient_norms = model_metrics.get('gradient_norms', [])\n",
    "                weight_norms = model_metrics.get('weight_norms', [])\n",
    "                \n",
    "                if gradient_norms:\n",
    "                    mlflow.log_metrics({\n",
    "                        \"final_gradient_norm\": gradient_norms[-1],\n",
    "                        \"mean_gradient_norm\": np.mean(gradient_norms),\n",
    "                        \"max_gradient_norm\": np.max(gradient_norms),\n",
    "                        \"gradient_stability\": 1.0 / (np.var(gradient_norms) + 1e-8)\n",
    "                    })\n",
    "                \n",
    "                if weight_norms:\n",
    "                    mlflow.log_metrics({\n",
    "                        \"final_weight_norm\": weight_norms[-1],\n",
    "                        \"mean_weight_norm\": np.mean(weight_norms),\n",
    "                        \"max_weight_norm\": np.max(weight_norms)\n",
    "                    })\n",
    "            \n",
    "            # ============================================================\n",
    "            # EPOCH-BY-EPOCH METRICS LOGGING\n",
    "            # ============================================================\n",
    "            \n",
    "            epoch_metrics = result.get('epoch_metrics', [])\n",
    "            if epoch_metrics:\n",
    "                for epoch_data in epoch_metrics:\n",
    "                    epoch = epoch_data['epoch']\n",
    "                    mlflow.log_metrics({\n",
    "                        \"train_loss_epoch\": epoch_data['train_loss'],\n",
    "                        \"train_accuracy_epoch\": epoch_data['train_acc'],\n",
    "                        \"val_loss_epoch\": epoch_data['val_loss'],\n",
    "                        \"val_accuracy_epoch\": epoch_data['val_acc'],\n",
    "                        \"learning_rate_epoch\": epoch_data['learning_rate'],\n",
    "                        \"gradient_norm_epoch\": epoch_data['gradient_norm']\n",
    "                    }, step=epoch)\n",
    "                    \n",
    "                    if epoch_data.get('is_best', False):\n",
    "                        mlflow.log_metric(\"is_best_epoch\", 1.0, step=epoch)\n",
    "                \n",
    "                # Save epoch metrics to file with predictable name\n",
    "                epoch_metrics_filename = f\"epoch_metrics_trial_{trial.number:03d}.json\"\n",
    "                epoch_metrics_path = os.path.join(trial_checkpoint_dir, epoch_metrics_filename)\n",
    "                \n",
    "                with open(epoch_metrics_path, 'w') as f:\n",
    "                    json.dump(epoch_metrics, f, indent=2)\n",
    "                \n",
    "                mlflow.log_artifact(epoch_metrics_path, \"metrics\")\n",
    "                print(f\"✓ Logged epoch metrics: {epoch_metrics_filename}\")\n",
    "            \n",
    "            # ============================================================\n",
    "            # FINAL TRAINING METRICS\n",
    "            # ============================================================\n",
    "            \n",
    "            training_metrics = {\n",
    "                'best_validation_accuracy': best_val_acc,\n",
    "                'final_validation_accuracy': final_val_acc,\n",
    "                'final_train_loss': result.get('train_loss', 0.0),\n",
    "                'final_val_loss': result.get('val_loss', 0.0),\n",
    "                'final_train_accuracy': result.get('train_acc', 0.0),\n",
    "                'epochs_completed': result.get('epochs_completed', NUM_EPOCHS),\n",
    "                'training_time_seconds': result.get('training_time', 0.0),\n",
    "                'trial_duration_seconds': trial_duration\n",
    "            }\n",
    "            mlflow.log_metrics(training_metrics)\n",
    "            \n",
    "            # ============================================================\n",
    "            # EFFICIENCY METRICS\n",
    "            # ============================================================\n",
    "            \n",
    "            if trial_duration > 0:\n",
    "                total_samples = 100000 * NUM_EPOCHS  # Adjust based on dataset\n",
    "                mlflow.log_metrics({\n",
    "                    'samples_per_second': total_samples / trial_duration,\n",
    "                    'time_per_epoch_seconds': trial_duration / NUM_EPOCHS,\n",
    "                    'throughput_images_per_second': total_samples / trial_duration\n",
    "                })\n",
    "                \n",
    "                if best_checkpoint_epoch >= 0:\n",
    "                    time_to_best = (best_checkpoint_epoch + 1) * (trial_duration / NUM_EPOCHS)\n",
    "                    mlflow.log_metrics({\n",
    "                        'time_to_best_checkpoint_seconds': time_to_best,\n",
    "                        'efficiency_score': best_val_acc / (time_to_best / 60)\n",
    "                    })\n",
    "            \n",
    "            # ============================================================\n",
    "            # CONVERGENCE METRICS\n",
    "            # ============================================================\n",
    "            \n",
    "            if epoch_metrics and len(epoch_metrics) > 1:\n",
    "                val_accs = [ep['val_acc'] for ep in epoch_metrics]\n",
    "                train_losses = [ep['train_loss'] for ep in epoch_metrics]\n",
    "                val_losses = [ep['val_loss'] for ep in epoch_metrics]\n",
    "                \n",
    "                final_acc = val_accs[-1]\n",
    "                initial_acc = val_accs[0]\n",
    "                improvement = final_acc - initial_acc\n",
    "                \n",
    "                mlflow.log_metrics({\n",
    "                    \"total_accuracy_improvement\": improvement,\n",
    "                    \"accuracy_improvement_rate\": improvement / len(val_accs)\n",
    "                })\n",
    "                \n",
    "                # Training stability\n",
    "                if len(val_accs) >= 3:\n",
    "                    recent_stability = 1.0 / (np.var(val_accs[-3:]) + 1e-8)\n",
    "                    overall_stability = 1.0 / (np.var(val_accs) + 1e-8)\n",
    "                    mlflow.log_metrics({\n",
    "                        \"validation_stability\": recent_stability,\n",
    "                        \"overall_validation_stability\": overall_stability\n",
    "                    })\n",
    "                \n",
    "                # Loss convergence\n",
    "                if len(train_losses) >= 2:\n",
    "                    train_loss_improvement = train_losses[0] - train_losses[-1]\n",
    "                    val_loss_improvement = val_losses[0] - val_losses[-1]\n",
    "                    mlflow.log_metrics({\n",
    "                        \"train_loss_improvement\": train_loss_improvement,\n",
    "                        \"val_loss_improvement\": val_loss_improvement\n",
    "                    })\n",
    "                \n",
    "                # Overfitting detection\n",
    "                if best_checkpoint_epoch >= 0 and best_checkpoint_epoch < len(val_accs) - 1:\n",
    "                    overfit_gap = best_val_acc - final_val_acc\n",
    "                    epochs_since_best = len(val_accs) - 1 - best_checkpoint_epoch\n",
    "                    \n",
    "                    mlflow.log_metrics({\n",
    "                        \"overfitting_gap\": overfit_gap,\n",
    "                        \"epochs_since_best\": epochs_since_best\n",
    "                    })\n",
    "                    \n",
    "                    if overfit_gap > 0.01:\n",
    "                        mlflow.log_param(\"overfitting_detected\", \"yes\")\n",
    "                        print(f\"⚠ Overfitting detected: best={best_val_acc:.4f} vs final={final_val_acc:.4f}\")\n",
    "                        print(f\"  Gap: {overfit_gap:.4f} ({epochs_since_best} epochs since best)\")\n",
    "                    else:\n",
    "                        mlflow.log_param(\"overfitting_detected\", \"no\")\n",
    "                \n",
    "                # Learning curve analysis\n",
    "                if len(val_accs) >= 5:\n",
    "                    recent_trend = val_accs[-1] - val_accs[-3]\n",
    "                    mlflow.log_metric(\"recent_validation_trend\", recent_trend)\n",
    "                    \n",
    "                    if recent_trend > 0:\n",
    "                        mlflow.log_param(\"convergence_status\", \"still_improving\")\n",
    "                    elif abs(recent_trend) < 0.001:\n",
    "                        mlflow.log_param(\"convergence_status\", \"converged\")\n",
    "                    else:\n",
    "                        mlflow.log_param(\"convergence_status\", \"degrading\")\n",
    "            \n",
    "            # ============================================================\n",
    "            # TAGS\n",
    "            # ============================================================\n",
    "            \n",
    "            performance_tier = (\n",
    "                \"excellent\" if best_val_acc > 0.7 else \n",
    "                \"good\" if best_val_acc > 0.5 else \n",
    "                \"fair\" if best_val_acc > 0.3 else \n",
    "                \"poor\"\n",
    "            )\n",
    "            \n",
    "            convergence_status = \"unknown\"\n",
    "            if epoch_metrics and len(epoch_metrics) > 1:\n",
    "                if best_checkpoint_epoch == len(epoch_metrics) - 1:\n",
    "                    convergence_status = \"still_improving\"\n",
    "                elif best_checkpoint_epoch >= 0 and best_checkpoint_epoch < len(epoch_metrics) - 3:\n",
    "                    convergence_status = \"early_stopped\"\n",
    "                else:\n",
    "                    convergence_status = \"converged\"\n",
    "            \n",
    "            mlflow.set_tags({\n",
    "                \"experiment_run_id\": EXPERIMENT_RUN_ID,\n",
    "                \"experiment_timestamp\": EXPERIMENT_TIMESTAMP,\n",
    "                \"experiment_short_name\": EXPERIMENT_SHORT_NAME,\n",
    "                \"trial_number\": f\"{trial.number:03d}\",\n",
    "                \"run_name\": run_name,\n",
    "                \"optimizer\": optimizer_name,\n",
    "                \"performance_tier\": performance_tier,\n",
    "                \"trial_status\": \"completed\",\n",
    "                \"checkpointing_enabled\": str(ENABLE_CHECKPOINTING),\n",
    "                \"checkpoint_storage\": \"uc_volumes_organized\",\n",
    "                \"checkpoint_base_dir\": CHECKPOINT_BASE_DIR,\n",
    "                \"checkpoint_trial_dir\": trial_checkpoint_dir,\n",
    "                \"total_checkpoints\": str(len(checkpoint_paths)) if checkpoint_paths else \"0\",\n",
    "                \"has_best_checkpoint\": \"yes\" if best_checkpoint_path else \"no\",\n",
    "                \"best_checkpoint_epoch\": f\"{best_checkpoint_epoch + 1:03d}\" if best_checkpoint_epoch >= 0 else \"none\",\n",
    "                \"overfitting\": \"yes\" if (best_checkpoint_epoch >= 0 and best_checkpoint_epoch < NUM_EPOCHS - 2) else \"no\",\n",
    "                \"convergence_status\": convergence_status,\n",
    "                \"batch_size\": str(batch_size),\n",
    "                \"learning_rate\": f\"{lr:.2e}\"\n",
    "            })\n",
    "            \n",
    "            # ============================================================\n",
    "            # SUMMARY OUTPUT\n",
    "            # ============================================================\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"TRIAL {trial.number:03d} COMPLETED SUCCESSFULLY\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Run Name: {run_name}\")\n",
    "            print(f\"\\nPerformance:\")\n",
    "            print(f\"  Best Accuracy:  {best_val_acc:.4f} (Epoch {best_checkpoint_epoch + 1})\")\n",
    "            print(f\"  Final Accuracy: {final_val_acc:.4f}\")\n",
    "            print(f\"  Performance Tier: {performance_tier}\")\n",
    "            print(f\"\\nTiming:\")\n",
    "            print(f\"  Duration: {trial_duration:.1f}s ({trial_duration/60:.1f} min)\")\n",
    "            print(f\"  Time per Epoch: {trial_duration/NUM_EPOCHS:.1f}s\")\n",
    "            if best_checkpoint_epoch >= 0:\n",
    "                time_to_best = (best_checkpoint_epoch + 1) * (trial_duration / NUM_EPOCHS)\n",
    "                print(f\"  Time to Best: {time_to_best:.1f}s ({time_to_best/60:.1f} min)\")\n",
    "            print(f\"\\nStorage:\")\n",
    "            print(f\"  Experiment Root:   {EXPERIMENT_ROOT}\")\n",
    "            print(f\"  Checkpoint Dir:    {trial_checkpoint_dir}\")\n",
    "            print(f\"  MLflow Artifacts:  {trial_run.info.artifact_uri}/{mlflow_artifact_subpath}/\")\n",
    "            if checkpoint_paths:\n",
    "                print(f\"\\n  Checkpoints:\")\n",
    "                print(f\"    Total Saved:      {len(checkpoint_paths)}\")\n",
    "                print(f\"    Logged to MLflow: {logged_count}\")\n",
    "                print(f\"    Total Storage:    {total_size_mb:.2f} MB\")\n",
    "                if best_checkpoint_path:\n",
    "                    print(f\"    Best Checkpoint:  {os.path.basename(best_checkpoint_path)}\")\n",
    "            print(f\"\\nMLflow:\")\n",
    "            print(f\"  Run ID:   {trial_run_id}\")\n",
    "            print(f\"  Run Name: {run_name}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            \n",
    "            return best_val_acc\n",
    "            \n",
    "        except Exception as e:\n",
    "            # ============================================================\n",
    "            # EXCEPTION HANDLING\n",
    "            # ============================================================\n",
    "            \n",
    "            trial_end_time = time.time()\n",
    "            trial_duration = trial_end_time - trial_start_time\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"TRIAL {trial.number:03d} FAILED\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Run Name: {run_name}\")\n",
    "            print(f\"Error Type: {type(e).__name__}\")\n",
    "            print(f\"Error Message: {e}\")\n",
    "            print(f\"Duration before failure: {trial_duration:.1f}s\")\n",
    "            print(f\"Checkpoint Directory: {trial_checkpoint_dir}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            \n",
    "            # Log failure information\n",
    "            mlflow.log_params({\n",
    "                \"trial_status\": \"failed\",\n",
    "                \"error_message\": str(e)[:500],\n",
    "                \"error_type\": type(e).__name__\n",
    "            })\n",
    "            \n",
    "            mlflow.log_metrics({\n",
    "                \"final_validation_accuracy\": 0.0,\n",
    "                \"trial_duration_seconds\": trial_duration\n",
    "            })\n",
    "            \n",
    "            # Log error details to file with predictable name\n",
    "            error_filename = f\"error_log_trial_{trial.number:03d}.txt\"\n",
    "            error_path = os.path.join(trial_checkpoint_dir, error_filename)\n",
    "            \n",
    "            try:\n",
    "                with open(error_path, 'w') as f:\n",
    "                    f.write(f\"Trial {trial.number:03d} Error Report\\n\")\n",
    "                    f.write(f\"{'='*80}\\n\\n\")\n",
    "                    f.write(f\"Run Name: {run_name}\\n\")\n",
    "                    f.write(f\"Error Type: {type(e).__name__}\\n\")\n",
    "                    f.write(f\"Error Message: {str(e)}\\n\\n\")\n",
    "                    f.write(f\"Storage Configuration:\\n\")\n",
    "                    f.write(f\"  Experiment Root: {EXPERIMENT_ROOT}\\n\")\n",
    "                    f.write(f\"  Checkpoint Base: {CHECKPOINT_BASE_DIR}\\n\")\n",
    "                    f.write(f\"  Trial Directory: {trial_checkpoint_dir}\\n\\n\")\n",
    "                    f.write(f\"Hyperparameters:\\n\")\n",
    "                    f.write(f\"  Optimizer: {optimizer_name}\\n\")\n",
    "                    f.write(f\"  Learning Rate: {lr:.2e}\\n\")\n",
    "                    f.write(f\"  Batch Size: {batch_size}\\n\")\n",
    "                    f.write(f\"  Weight Decay: {weight_decay:.2e}\\n\\n\")\n",
    "                    f.write(f\"Full Traceback:\\n\")\n",
    "                    f.write(traceback.format_exc())\n",
    "                \n",
    "                mlflow.log_artifact(error_path, \"error_logs\")\n",
    "                print(f\"✓ Error log saved: {error_filename}\")\n",
    "            except Exception as log_error:\n",
    "                print(f\"⚠ Could not save error log: {log_error}\")\n",
    "            \n",
    "            mlflow.set_tags({\n",
    "                \"experiment_run_id\": EXPERIMENT_RUN_ID,\n",
    "                \"experiment_timestamp\": EXPERIMENT_TIMESTAMP,\n",
    "                \"experiment_short_name\": EXPERIMENT_SHORT_NAME,\n",
    "                \"trial_number\": f\"{trial.number:03d}\",\n",
    "                \"run_name\": run_name,\n",
    "                \"trial_status\": \"failed\",\n",
    "                \"optimizer\": optimizer_name,\n",
    "                \"checkpointing_enabled\": str(ENABLE_CHECKPOINTING),\n",
    "                \"checkpoint_storage\": \"uc_volumes_organized\",\n",
    "                \"checkpoint_base_dir\": CHECKPOINT_BASE_DIR,\n",
    "                \"checkpoint_trial_dir\": trial_checkpoint_dir,\n",
    "                \"has_best_checkpoint\": \"no\",\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"batch_size\": str(batch_size),\n",
    "                \"learning_rate\": f\"{lr:.2e}\"\n",
    "            })\n",
    "            \n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff83ecb-13b3-45a5-a4e2-34c5b3e493cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "checkpoint helper funcs"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHECKPOINT HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Find the latest checkpoint in a directory.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dir: Directory containing checkpoints\n",
    "        \n",
    "    Returns:\n",
    "        Path to latest checkpoint or None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    \n",
    "    checkpoint_files = [\n",
    "        f for f in os.listdir(checkpoint_dir) \n",
    "        if f.startswith('checkpoint_epoch_') and f.endswith('.pt')\n",
    "    ]\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    # Sort by epoch number\n",
    "    checkpoint_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[-1])\n",
    "    \n",
    "    return latest_checkpoint\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler, device):\n",
    "    \"\"\"\n",
    "    Load checkpoint and return metadata.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        model: Model to load state into\n",
    "        optimizer: Optimizer to load state into\n",
    "        scheduler: Scheduler to load state into\n",
    "        device: Device to load tensors to\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with checkpoint metadata or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Load model state\n",
    "        if hasattr(model, 'module'):\n",
    "            model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Load optimizer state\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        # Load scheduler state\n",
    "        if 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        metadata = {\n",
    "            'epoch': checkpoint.get('epoch', 0),\n",
    "            'best_val_acc': checkpoint.get('best_val_acc', 0.0),\n",
    "            'val_acc': checkpoint.get('val_acc', 0.0),\n",
    "            'train_loss': checkpoint.get('train_loss', 0.0),\n",
    "            'val_loss': checkpoint.get('val_loss', 0.0)\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ Loaded checkpoint: {os.path.basename(checkpoint_path)}\")\n",
    "        print(f\"  Epoch: {metadata['epoch']}, Val Acc: {metadata['val_acc']:.4f}\")\n",
    "        \n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Failed to load checkpoint {checkpoint_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_checkpoint_summary(checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Get summary of checkpoints in a directory.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dir: Directory containing checkpoints\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with checkpoint summary\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return {\n",
    "            'exists': False,\n",
    "            'total_checkpoints': 0,\n",
    "            'total_size_mb': 0.0,\n",
    "            'checkpoints': []\n",
    "        }\n",
    "    \n",
    "    checkpoint_files = [\n",
    "        f for f in os.listdir(checkpoint_dir)\n",
    "        if f.endswith('.pt')\n",
    "    ]\n",
    "    \n",
    "    total_size = 0\n",
    "    checkpoint_info = []\n",
    "    \n",
    "    for filename in sorted(checkpoint_files):\n",
    "        filepath = os.path.join(checkpoint_dir, filename)\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        \n",
    "        checkpoint_info.append({\n",
    "            'filename': filename,\n",
    "            'size_mb': round(size_mb, 2),\n",
    "            'path': filepath\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'exists': True,\n",
    "        'total_checkpoints': len(checkpoint_files),\n",
    "        'total_size_mb': round(total_size, 2),\n",
    "        'checkpoints': checkpoint_info\n",
    "    }\n",
    "\n",
    "def cleanup_old_checkpoints(checkpoint_base_dir, keep_recent=3):\n",
    "    \"\"\"\n",
    "    Clean up old trial checkpoints, keeping only recent ones.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_base_dir: Base directory containing trial subdirectories\n",
    "        keep_recent: Number of recent trials to keep\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_base_dir):\n",
    "        return\n",
    "    \n",
    "    trial_dirs = [\n",
    "        d for d in os.listdir(checkpoint_base_dir)\n",
    "        if os.path.isdir(os.path.join(checkpoint_base_dir, d)) and d.startswith('trial_')\n",
    "    ]\n",
    "    \n",
    "    if len(trial_dirs) <= keep_recent:\n",
    "        print(f\"Only {len(trial_dirs)} trial directories found, no cleanup needed\")\n",
    "        return\n",
    "    \n",
    "    # Sort by modification time\n",
    "    trial_dirs_with_time = [\n",
    "        (d, os.path.getmtime(os.path.join(checkpoint_base_dir, d)))\n",
    "        for d in trial_dirs\n",
    "    ]\n",
    "    trial_dirs_sorted = sorted(trial_dirs_with_time, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Keep recent, remove old\n",
    "    dirs_to_keep = [d[0] for d in trial_dirs_sorted[:keep_recent]]\n",
    "    dirs_to_remove = [d[0] for d in trial_dirs_sorted[keep_recent:]]\n",
    "    \n",
    "    print(f\"\\nCleaning up old checkpoints:\")\n",
    "    print(f\"  Keeping {len(dirs_to_keep)} recent trials\")\n",
    "    print(f\"  Removing {len(dirs_to_remove)} old trials\")\n",
    "    \n",
    "    import shutil\n",
    "    for trial_dir in dirs_to_remove:\n",
    "        trial_path = os.path.join(checkpoint_base_dir, trial_dir)\n",
    "        try:\n",
    "            shutil.rmtree(trial_path)\n",
    "            print(f\"  ✓ Removed: {trial_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ Failed to remove {trial_dir}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08523cc7-f120-4b09-b789-3b136afe1b55",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configs for testing"
    }
   },
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "# num_classes = 200\n",
    "NUM_EPOCHS = 3 #5  #Test with just 1 or 2 epoch first if needed\n",
    "num_workers = 4  #Reduce to 2 workers to decrease I/O contention\n",
    "\n",
    "## Optuna Configuration related \n",
    "# parameterized N_TRIALS \n",
    "N_TRIALS = 3 #5 ## can be updated/increased \n",
    "# batch_sizes = [8, 16] -- update parameter-ranges/etc\n",
    "# optimizers = ['AdamW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "015bcd4e-d4a3-4770-ba61-bf965a9d3962",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optimization Run with Checkpoint Management"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN OPTUNA OPTIMIZATION WITH ORGANIZED CHECKPOINTING\n",
    "# ============================================================\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Verify configuration is set up\n",
    "if not all([EXPERIMENT_ROOT, CHECKPOINT_BASE_DIR, MLFLOW_ARTIFACT_LOCATION, EXPERIMENT_NAME]):\n",
    "    raise ValueError(\"Configuration not properly initialized. Run the configuration cell first.\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STARTING OPTUNA OPTIMIZATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Experiment: {EXPERIMENT_SHORT_NAME}\")\n",
    "print(f\"Timestamp: {EXPERIMENT_TIMESTAMP}\")\n",
    "print(f\"Trials: {N_TRIALS}\")\n",
    "print(f\"Epochs per Trial: {NUM_EPOCHS}\")\n",
    "print(f\"Workers: {num_workers}\")\n",
    "print(f\"Checkpointing: {'Enabled' if ENABLE_CHECKPOINTING else 'Disabled'}\")\n",
    "print(f\"\\nPaths:\")\n",
    "print(f\"  Experiment Root:  {EXPERIMENT_ROOT}\")\n",
    "print(f\"  Checkpoint Base:  {CHECKPOINT_BASE_DIR}\")\n",
    "print(f\"  MLflow Artifacts: {MLFLOW_ARTIFACT_LOCATION}\")\n",
    "print(f\"  MLflow Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(\n",
    "    study_name=f\"pytorch_{EXPERIMENT_SHORT_NAME}_{EXPERIMENT_TIMESTAMP}\",\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Start parent MLflow run\n",
    "with mlflow.start_run(run_name=f\"optuna_study_{EXPERIMENT_TIMESTAMP}\") as parent_run:\n",
    "    EXPERIMENT_RUN_ID = parent_run.info.run_id\n",
    "    parent_run_id = parent_run.info.run_id\n",
    "    \n",
    "    print(f\"Parent MLflow Run ID: {parent_run_id}\")\n",
    "    print(f\"Starting optimization...\\n\")        \n",
    "    \n",
    "    # Log optimization configuration\n",
    "    mlflow.log_params({\n",
    "        \"experiment_run_id\": EXPERIMENT_RUN_ID,\n",
    "        \"experiment_timestamp\": EXPERIMENT_TIMESTAMP,\n",
    "        \"experiment_short_name\": EXPERIMENT_SHORT_NAME,\n",
    "        \"study_name\": study.study_name,\n",
    "        \"n_trials\": N_TRIALS,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"model_architecture\": \"mobilenetv2\",\n",
    "        \"optimization_type\": \"optuna_hpt_organized\",\n",
    "        \n",
    "        # ADD GLOBAL DATALOADER CONFIGURATION\n",
    "        \"global_dataloader_type\": \"streaming_mds\",\n",
    "        \"global_dataloader_library\": \"mosaic_streaming\",\n",
    "        \"global_dataset_format\": \"mds\",\n",
    "        \"global_dataset_storage\": \"uc_volumes\",\n",
    "        \"global_dataset_train_path\": f\"{data_storage_location}/{mds_train_dir}\",\n",
    "        \"global_dataset_val_path\": f\"{data_storage_location}/{mds_val_dir}\",\n",
    "        \"global_dataset_num_classes\": num_classes,\n",
    "        \"global_dataset_catalog\": CATALOG,\n",
    "        \"global_dataset_schema\": SCHEMA,\n",
    "        \"global_dataset_volume\": VOLUME_NAME,\n",
    "        \n",
    "        # Checkpointing\n",
    "        \"checkpointing_enabled\": ENABLE_CHECKPOINTING,\n",
    "        \"checkpoint_frequency\": CHECKPOINT_FREQUENCY,\n",
    "        \"checkpoint_base_dir\": CHECKPOINT_BASE_DIR,\n",
    "        \"experiment_root\": EXPERIMENT_ROOT,\n",
    "        \"mlflow_artifact_location\": MLFLOW_ARTIFACT_LOCATION,\n",
    "        \"resume_from_checkpoint\": RESUME_FROM_CHECKPOINT\n",
    "    })\n",
    "    \n",
    "    # Run optimization (sequential for simplicity)\n",
    "    study.optimize(objective_function, n_trials=N_TRIALS, n_jobs=1)\n",
    "    \n",
    "    # Calculate duration\n",
    "    end_time = time.time()\n",
    "    optimization_duration = end_time - start_time\n",
    "    \n",
    "    # Get results\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value\n",
    "    best_trial_number = study.best_trial.number if study.best_trial else None\n",
    "    \n",
    "    completed_trials = [t for t in study.trials if t.state.name == \"COMPLETE\" and t.value is not None]\n",
    "    failed_trials = [t for t in study.trials if t.state.name != \"COMPLETE\" or t.value is None]\n",
    "    trial_values = [t.value for t in completed_trials]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"OPTIMIZATION COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Duration: {optimization_duration:.1f}s ({optimization_duration/60:.1f} min)\")\n",
    "    print(f\"Completed Trials: {len(completed_trials)}/{len(study.trials)}\")\n",
    "    print(f\"Failed Trials: {len(failed_trials)}\")\n",
    "    print(f\"\\nBest Trial: {best_trial_number}\")\n",
    "    print(f\"Best Validation Accuracy: {best_value:.4f}\")\n",
    "    print(f\"Best Parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # CHECKPOINT SUMMARY\n",
    "    # ============================================================\n",
    "    \n",
    "    checkpoint_summary = {\n",
    "        \"experiment_timestamp\": EXPERIMENT_TIMESTAMP,\n",
    "        \"checkpoint_base_dir\": CHECKPOINT_BASE_DIR,\n",
    "        \"trial_checkpoints\": {},\n",
    "        \"total_checkpoints\": 0,\n",
    "        \"total_size_mb\": 0.0,\n",
    "        \"best_trial_number\": best_trial_number,\n",
    "        \"best_trial_checkpoint_dir\": None\n",
    "    }\n",
    "    \n",
    "    if ENABLE_CHECKPOINTING and os.path.exists(CHECKPOINT_BASE_DIR):\n",
    "        print(\"Collecting checkpoint summary...\")\n",
    "        \n",
    "        # Get all trial directories\n",
    "        trial_dirs = [\n",
    "            d for d in os.listdir(CHECKPOINT_BASE_DIR)\n",
    "            if os.path.isdir(os.path.join(CHECKPOINT_BASE_DIR, d)) and d.startswith('trial_')\n",
    "        ]\n",
    "        \n",
    "        total_checkpoints = 0\n",
    "        total_size_bytes = 0\n",
    "        \n",
    "        for trial_dir in sorted(trial_dirs):\n",
    "            trial_path = os.path.join(CHECKPOINT_BASE_DIR, trial_dir)\n",
    "            \n",
    "            # Count checkpoint files\n",
    "            checkpoint_files = [\n",
    "                f for f in os.listdir(trial_path)\n",
    "                if f.endswith('.pt')\n",
    "            ]\n",
    "            \n",
    "            if checkpoint_files:\n",
    "                # Calculate size\n",
    "                trial_size = sum(\n",
    "                    os.path.getsize(os.path.join(trial_path, f))\n",
    "                    for f in checkpoint_files\n",
    "                )\n",
    "                trial_size_mb = trial_size / (1024 * 1024)\n",
    "                \n",
    "                # Check for best checkpoint\n",
    "                has_best = os.path.exists(os.path.join(trial_path, \"best_checkpoint.pt\"))\n",
    "                \n",
    "                checkpoint_summary[\"trial_checkpoints\"][trial_dir] = {\n",
    "                    \"count\": len(checkpoint_files),\n",
    "                    \"size_mb\": round(trial_size_mb, 2),\n",
    "                    \"has_best\": has_best,\n",
    "                    \"path\": trial_path\n",
    "                }\n",
    "                \n",
    "                total_checkpoints += len(checkpoint_files)\n",
    "                total_size_bytes += trial_size\n",
    "        \n",
    "        checkpoint_summary[\"total_checkpoints\"] = total_checkpoints\n",
    "        checkpoint_summary[\"total_size_mb\"] = round(total_size_bytes / (1024 * 1024), 2)\n",
    "        \n",
    "        # Find best trial checkpoint directory\n",
    "        if best_trial_number is not None:\n",
    "            best_trial_dirs = [\n",
    "                d for d in trial_dirs\n",
    "                if d.startswith(f\"trial_{best_trial_number:03d}_\")\n",
    "            ]\n",
    "            if best_trial_dirs:\n",
    "                best_trial_checkpoint_dir = os.path.join(CHECKPOINT_BASE_DIR, best_trial_dirs[0])\n",
    "                checkpoint_summary[\"best_trial_checkpoint_dir\"] = best_trial_checkpoint_dir\n",
    "        \n",
    "        print(f\"✓ Found {total_checkpoints} checkpoints across {len(trial_dirs)} trials\")\n",
    "        print(f\"  Total Size: {checkpoint_summary['total_size_mb']:.2f} MB\")\n",
    "        if checkpoint_summary[\"best_trial_checkpoint_dir\"]:\n",
    "            print(f\"  Best Trial Checkpoints: {checkpoint_summary['best_trial_checkpoint_dir']}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # LOG OPTIMIZATION RESULTS\n",
    "    # ============================================================\n",
    "    \n",
    "    # Essential metrics\n",
    "    optimization_results = {\n",
    "        \"best_validation_accuracy\": best_value,\n",
    "        \"total_trials\": len(study.trials),\n",
    "        \"completed_trials\": len(completed_trials),\n",
    "        \"failed_trials\": len(failed_trials),\n",
    "        \"optimization_duration_seconds\": optimization_duration,\n",
    "        \"optimization_duration_minutes\": optimization_duration / 60.0,\n",
    "        \"success_rate\": len(completed_trials) / len(study.trials) if len(study.trials) > 0 else 0.0,\n",
    "        \"total_checkpoints_saved\": checkpoint_summary[\"total_checkpoints\"],\n",
    "        \"total_checkpoints_size_mb\": checkpoint_summary[\"total_size_mb\"]\n",
    "    }\n",
    "    \n",
    "    mlflow.log_metrics(optimization_results)\n",
    "    \n",
    "    # Log best hyperparameters\n",
    "    for param_name, param_value in best_params.items():\n",
    "        mlflow.log_param(f\"best_{param_name}\", param_value)\n",
    "    \n",
    "    # Log trial statistics\n",
    "    if trial_values:\n",
    "        trial_stats = {\n",
    "            \"mean_trial_accuracy\": np.mean(trial_values),\n",
    "            \"std_trial_accuracy\": np.std(trial_values),\n",
    "            \"min_trial_accuracy\": np.min(trial_values),\n",
    "            \"max_trial_accuracy\": np.max(trial_values),\n",
    "            \"median_trial_accuracy\": np.median(trial_values)\n",
    "        }\n",
    "        mlflow.log_metrics(trial_stats)\n",
    "        \n",
    "        # Calculate improvement metrics\n",
    "        if len(trial_values) > 1:\n",
    "            improvement = best_value - np.min(trial_values)\n",
    "            mlflow.log_metric(\"accuracy_improvement_range\", improvement)\n",
    "    \n",
    "    # Log per-trial checkpoint counts\n",
    "    for trial_dir, info in checkpoint_summary[\"trial_checkpoints\"].items():\n",
    "        trial_num = trial_dir.split('_')[1]\n",
    "        mlflow.log_metric(f\"trial_{trial_num}_checkpoint_count\", info[\"count\"])\n",
    "        mlflow.log_metric(f\"trial_{trial_num}_checkpoint_size_mb\", info[\"size_mb\"])\n",
    "    \n",
    "    # ============================================================\n",
    "    # PARAMETER IMPORTANCE\n",
    "    # ============================================================\n",
    "    \n",
    "    try:\n",
    "        if len(completed_trials) > 1:\n",
    "            importance = optuna.importance.get_param_importances(study)\n",
    "            \n",
    "            print(\"\\nParameter Importance:\")\n",
    "            for param_name, importance_value in sorted(importance.items(), key=lambda x: x[1], reverse=True):\n",
    "                mlflow.log_metric(f\"param_importance_{param_name}\", importance_value)\n",
    "                print(f\"  {param_name}: {importance_value:.4f}\")\n",
    "            \n",
    "            most_important = max(importance.items(), key=lambda x: x[1])\n",
    "            mlflow.log_param(\"most_important_param\", most_important[0])\n",
    "            mlflow.log_metric(\"most_important_param_score\", most_important[1])\n",
    "            print(f\"\\nMost Important: {most_important[0]} ({most_important[1]:.4f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate parameter importance: {e}\")\n",
    "        mlflow.log_param(\"importance_error\", str(e)[:200])\n",
    "    \n",
    "    # ============================================================\n",
    "    # SAVE AND LOG CHECKPOINT SUMMARY\n",
    "    # ============================================================\n",
    "    \n",
    "    # Save checkpoint summary to file\n",
    "    summary_filename = f\"optimization_summary_{EXPERIMENT_TIMESTAMP}.json\"\n",
    "    summary_path = os.path.join(CHECKPOINT_BASE_DIR, summary_filename)\n",
    "    \n",
    "    full_summary = {\n",
    "        \"optimization\": {\n",
    "            \"experiment_timestamp\": EXPERIMENT_TIMESTAMP,\n",
    "            \"experiment_short_name\": EXPERIMENT_SHORT_NAME,\n",
    "            \"study_name\": study.study_name,\n",
    "            \"duration_seconds\": optimization_duration,\n",
    "            \"duration_minutes\": optimization_duration / 60.0,\n",
    "            \"n_trials\": N_TRIALS,\n",
    "            \"completed_trials\": len(completed_trials),\n",
    "            \"failed_trials\": len(failed_trials)\n",
    "        },\n",
    "        \"best_trial\": {\n",
    "            \"trial_number\": best_trial_number,\n",
    "            \"validation_accuracy\": best_value,\n",
    "            \"parameters\": best_params,\n",
    "            \"checkpoint_dir\": checkpoint_summary[\"best_trial_checkpoint_dir\"]\n",
    "        },\n",
    "        \"checkpoints\": checkpoint_summary,\n",
    "        \"paths\": {\n",
    "            \"experiment_root\": EXPERIMENT_ROOT,\n",
    "            \"checkpoint_base_dir\": CHECKPOINT_BASE_DIR,\n",
    "            \"mlflow_artifact_location\": MLFLOW_ARTIFACT_LOCATION\n",
    "        },\n",
    "        \"mlflow\": {\n",
    "            \"parent_run_id\": parent_run_id,\n",
    "            \"experiment_name\": EXPERIMENT_NAME\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(full_summary, f, indent=2)\n",
    "    \n",
    "    mlflow.log_artifact(summary_path, \"optimization_summary\")\n",
    "    print(f\"\\n✓ Saved optimization summary: {summary_filename}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # SAVE STUDY OBJECT\n",
    "    # ============================================================\n",
    "    \n",
    "    # Save Optuna study for later analysis\n",
    "    study_filename = f\"optuna_study_{EXPERIMENT_TIMESTAMP}.pkl\"\n",
    "    study_path = os.path.join(CHECKPOINT_BASE_DIR, study_filename)\n",
    "    \n",
    "    import pickle\n",
    "    with open(study_path, 'wb') as f:\n",
    "        pickle.dump(study, f)\n",
    "    \n",
    "    mlflow.log_artifact(study_path, \"optimization_summary\")\n",
    "    print(f\"✓ Saved Optuna study: {study_filename}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # SET TAGS\n",
    "    # ============================================================\n",
    "    \n",
    "    mlflow.set_tags({\n",
    "        \"optimization_type\": \"optuna_hpt_organized\",\n",
    "        \"experiment_short_name\": EXPERIMENT_SHORT_NAME,\n",
    "        \"experiment_timestamp\": EXPERIMENT_TIMESTAMP,\n",
    "        \"model_architecture\": \"mobilenetv2\",\n",
    "        \"best_optimizer\": best_params.get('optimizer', 'unknown'),\n",
    "        \"optimization_status\": \"completed\",\n",
    "        \"total_trials\": str(len(study.trials)),\n",
    "        \"completed_trials\": str(len(completed_trials)),\n",
    "        \"best_trial_number\": str(best_trial_number) if best_trial_number is not None else \"none\",\n",
    "        \"best_accuracy\": f\"{best_value:.4f}\",\n",
    "        \"checkpointing_enabled\": str(ENABLE_CHECKPOINTING),\n",
    "        \"total_checkpoints\": str(checkpoint_summary[\"total_checkpoints\"]),\n",
    "        \"checkpoint_base_dir\": CHECKPOINT_BASE_DIR\n",
    "    })\n",
    "\n",
    "# ============================================================\n",
    "# FINAL SUMMARY DISPLAY\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"OPTIMIZATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Parent Run ID: {parent_run_id}\")\n",
    "print(f\"Duration: {optimization_duration:.1f}s ({optimization_duration/60:.1f} min)\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Best Trial: {best_trial_number}\")\n",
    "print(f\"  Best Accuracy: {best_value:.4f}\")\n",
    "print(f\"  Completed: {len(completed_trials)}/{len(study.trials)}\")\n",
    "print(f\"  Success Rate: {len(completed_trials)/len(study.trials)*100:.1f}%\")\n",
    "print(f\"\\nCheckpoints:\")\n",
    "print(f\"  Total Saved: {checkpoint_summary['total_checkpoints']}\")\n",
    "print(f\"  Total Size: {checkpoint_summary['total_size_mb']:.2f} MB\")\n",
    "print(f\"  Location: {CHECKPOINT_BASE_DIR}\")\n",
    "if checkpoint_summary[\"best_trial_checkpoint_dir\"]:\n",
    "    print(f\"  Best Trial: {checkpoint_summary['best_trial_checkpoint_dir']}\")\n",
    "print(f\"\\nMLflow:\")\n",
    "print(f\"  Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"  Parent Run: {parent_run_id}\")\n",
    "print(f\"  Artifacts: {MLFLOW_ARTIFACT_LOCATION}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# DISPLAY DETAILED CHECKPOINT SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "if checkpoint_summary[\"trial_checkpoints\"]:\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"CHECKPOINT DETAILS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for trial_dir in sorted(checkpoint_summary[\"trial_checkpoints\"].keys()):\n",
    "        info = checkpoint_summary[\"trial_checkpoints\"][trial_dir]\n",
    "        best_marker = \"★\" if info[\"has_best\"] else \" \"\n",
    "        print(f\"{best_marker} {trial_dir}:\")\n",
    "        print(f\"    Checkpoints: {info['count']}\")\n",
    "        print(f\"    Size: {info['size_mb']:.2f} MB\")\n",
    "        print(f\"    Path: {info['path']}\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE PRETTY-PRINTED SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"Checkpoint Summary (JSON):\")\n",
    "print(json.dumps(checkpoint_summary, indent=2))\n",
    "\n",
    "print(f\"\\n✓ All results saved to: {CHECKPOINT_BASE_DIR}\")\n",
    "print(f\"✓ MLflow experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"✓ Parent run ID: {parent_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76178464-ab4e-4ee0-8f3d-6621bce08845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42bf31c3-c360-4877-9dce-49dbc1d6fd7d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "retrieve parent_run_id"
    }
   },
   "outputs": [],
   "source": [
    "parent_run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c931ea-0ac5-4fc1-8a37-3d156c37c71b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "load parent_run_id best opt model checkpoint + make inference"
    }
   },
   "outputs": [],
   "source": [
    "def load_best_model(parent_run_id):\n",
    "    \"\"\"Load best model from HPO run.\"\"\"\n",
    "    import mlflow\n",
    "    import torch\n",
    "    from torchvision import models\n",
    "    import json\n",
    "    import glob\n",
    "    \n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    \n",
    "    # Get optimization summary\n",
    "    summary_path = client.download_artifacts(parent_run_id, \"optimization_summary\")\n",
    "    summary_file = glob.glob(f\"{summary_path}/*.json\")[0]\n",
    "    \n",
    "    with open(summary_file, 'r') as f:\n",
    "        summary = json.load(f)\n",
    "    \n",
    "    # Get best checkpoint\n",
    "    best_trial = summary['best_trial']\n",
    "    checkpoint_dir = best_trial['checkpoint_dir']\n",
    "    checkpoint_path = f\"{checkpoint_dir}/best_checkpoint.pt\"\n",
    "    \n",
    "    # Load checkpoint\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    \n",
    "    # Build model to match checkpoint\n",
    "    model = models.mobilenet_v2(weights=None)\n",
    "    \n",
    "    if 'classifier.4.weight' in state_dict:\n",
    "        hidden_dim = state_dict['classifier.1.weight'].shape[0]\n",
    "        num_classes = state_dict['classifier.4.weight'].shape[0]\n",
    "        \n",
    "        model.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(model.last_channel, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        num_classes = state_dict['classifier.1.weight'].shape[0]\n",
    "        model.classifier[1] = torch.nn.Linear(model.last_channel, num_classes)\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Loaded model with {num_classes} classes\")\n",
    "    return model, device\n",
    "\n",
    "\n",
    "def calculate_metrics(all_predictions, all_labels, all_probs, num_classes):\n",
    "    \"\"\"Calculate accuracy, mAP, and per-class metrics.\"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import average_precision_score, confusion_matrix\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = (all_predictions == all_labels).sum() / len(all_labels)\n",
    "    \n",
    "    # mAP (mean Average Precision)\n",
    "    labels_onehot = np.zeros((len(all_labels), num_classes))\n",
    "    labels_onehot[np.arange(len(all_labels)), all_labels] = 1\n",
    "    \n",
    "    # Calculate AP for each class\n",
    "    aps = []\n",
    "    for i in range(num_classes):\n",
    "        if labels_onehot[:, i].sum() > 0:\n",
    "            ap = average_precision_score(labels_onehot[:, i], all_probs[:, i])\n",
    "            aps.append(ap)\n",
    "    \n",
    "    mAP = np.mean(aps) if aps else 0.0\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    \n",
    "    # Top-5 accuracy\n",
    "    top5_preds = np.argsort(all_probs, axis=1)[:, -5:]\n",
    "    top5_correct = np.any(top5_preds == all_labels[:, None], axis=1).sum()\n",
    "    top5_accuracy = top5_correct / len(all_labels)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'mAP': mAP,\n",
    "        'top5_accuracy': top5_accuracy,\n",
    "        'per_class_accuracy': per_class_acc,\n",
    "        'mean_class_accuracy': np.mean(per_class_acc)\n",
    "    }\n",
    "\n",
    "\n",
    "def run_inference(parent_run_id, val_path, batch_size=64):\n",
    "    \"\"\"Run inference with comprehensive metrics.\"\"\"\n",
    "    import torch\n",
    "    import shutil\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load model\n",
    "    model, device = load_best_model(parent_run_id)\n",
    "    \n",
    "    # Create class mapping\n",
    "    print(\"Creating class mapping...\")\n",
    "    label_to_idx = create_comprehensive_label_mapping(val_path)\n",
    "    num_classes = len(label_to_idx)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader, cache_dir = get_dataloader_with_mosaic(\n",
    "        remote_path=val_path,\n",
    "        local_path=\"/local_disk0/tmp/mds_inference\",\n",
    "        batch_size=batch_size,\n",
    "        rank=0\n",
    "    )\n",
    "    \n",
    "    # Collect predictions and labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images, labels = convert_batch_to_tensors(\n",
    "                    batch, \n",
    "                    device=device, \n",
    "                    class_to_idx=label_to_idx,\n",
    "                    rank=0\n",
    "                )\n",
    "                \n",
    "                outputs = model(images)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                predictions = outputs.argmax(dim=1)\n",
    "                \n",
    "                all_predictions.append(predictions.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "    \n",
    "    finally:\n",
    "        if os.path.exists(cache_dir):\n",
    "            shutil.rmtree(cache_dir, ignore_errors=True)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_predictions, all_labels, all_probs, num_classes)\n",
    "    \n",
    "    # Add sample count to metrics for easy access\n",
    "    metrics['sample_count'] = len(all_labels)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"INFERENCE METRICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Samples:              {len(all_labels):,}\")\n",
    "    print(f\"  Accuracy:             {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
    "    print(f\"  Top-5 Accuracy:       {metrics['top5_accuracy']:.4f} ({metrics['top5_accuracy']*100:.2f}%)\")\n",
    "    print(f\"  mAP:                  {metrics['mAP']:.4f}\")\n",
    "    print(f\"  Mean Class Accuracy:  {metrics['mean_class_accuracy']:.4f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Usage\n",
    "val_path = \"/Volumes/mmt/pytorch/torch_data/imagenet_tiny200_mds_val\"\n",
    "accuracy = run_inference(parent_run_id, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aa9bb7b-524f-4934-a830-c8f24efda10e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "accuracy.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6efa2a3b-0e36-44ba-84a8-aa3f42c70151",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Log inference metrics to MLflow"
    }
   },
   "outputs": [],
   "source": [
    "# Log inference metrics to the existing parent MLflow run\n",
    "# This maintains the logical connection between optimization and inference\n",
    "\n",
    "# Get the metrics programmatically from the previous inference run\n",
    "# The 'accuracy' variable from cell 23 actually contains all metrics\n",
    "if 'accuracy' in locals() and isinstance(accuracy, dict):\n",
    "    # Extract sample count from the metrics dictionary itself\n",
    "    if 'sample_count' in accuracy:\n",
    "        sample_count = accuracy['sample_count']\n",
    "    else:\n",
    "        raise ValueError(\"Sample count not found in metrics! Please re-run the inference cell (cell 23) with the updated version.\")\n",
    "    \n",
    "    # Use the metrics returned from run_inference function\n",
    "    metrics_dict = {\n",
    "        \"inference_accuracy\": accuracy['accuracy'],\n",
    "        \"inference_top5_accuracy\": accuracy['top5_accuracy'], \n",
    "        \"inference_map\": accuracy['mAP'],\n",
    "        \"inference_mean_class_accuracy\": accuracy['mean_class_accuracy'],\n",
    "        \"inference_samples\": sample_count\n",
    "    }\n",
    "    print(f\"Extracted metrics programmatically from inference results\")\n",
    "    print(f\"Sample count: {sample_count:,} samples\")\n",
    "else:\n",
    "    # Error out if metrics are not available - no fallback\n",
    "    raise ValueError(\"Inference metrics not found! Please run the inference cell (cell 23) first to generate the 'accuracy' variable with metrics.\")\n",
    "\n",
    "# Log to the existing parent run from optimization\n",
    "with mlflow.start_run(run_id=parent_run_id):\n",
    "    # Log inference metrics (metrics can be overwritten)\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "    # Try to log parameters, but skip if they already exist\n",
    "    params_to_log = {\n",
    "        \"inference_batch_size\": 64,\n",
    "        \"inference_dataset_path\": val_path,\n",
    "        \"inference_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    for param_name, param_value in params_to_log.items():\n",
    "        try:\n",
    "            mlflow.log_param(param_name, param_value)\n",
    "        except Exception as e:\n",
    "            if \"already logged\" in str(e):\n",
    "                print(f\"Parameter '{param_name}' already exists, skipping...\")\n",
    "            else:\n",
    "                print(f\"Warning: Could not log parameter '{param_name}': {e}\")\n",
    "    \n",
    "    # Add inference tags (tags can be overwritten)\n",
    "    mlflow.set_tag(\"inference_completed\", \"true\")\n",
    "    mlflow.set_tag(\"inference_phase\", \"post_optimization\")\n",
    "    mlflow.set_tag(\"inference_timestamp\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "    print(f\"\\nInference metrics logged to existing MLflow run: {parent_run_id}\")\n",
    "    print(f\"Final Results Summary:\")\n",
    "    print(f\"   • Accuracy: {metrics_dict['inference_accuracy']:.4f} ({metrics_dict['inference_accuracy']*100:.2f}%)\")\n",
    "    print(f\"   • Top-5 Accuracy: {metrics_dict['inference_top5_accuracy']:.4f} ({metrics_dict['inference_top5_accuracy']*100:.2f}%)\")\n",
    "    print(f\"   • Mean Average Precision: {metrics_dict['inference_map']:.4f}\")\n",
    "    print(f\"   • Mean Class Accuracy: {metrics_dict['inference_mean_class_accuracy']:.4f}\")\n",
    "    print(f\"   • Total validation samples: {metrics_dict['inference_samples']:,}\")\n",
    "    print(f\"\\nView complete experiment: {EXPERIMENT_NAME}\")\n",
    "    print(f\"Run ID: {parent_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e6ca2e-43a2-4e9e-8c37-f415951e6c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_65bc13ea-276c-4905-a728-9fe2fb1780e2",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "imgnet_mbnetv2_TorchDistr_Optuna_mlflow_v0.2.3.1 (UCVols_chkpt)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
