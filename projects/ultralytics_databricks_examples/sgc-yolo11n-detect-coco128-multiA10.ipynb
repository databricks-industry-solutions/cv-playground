{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da57367-ba6b-4f58-9304-ecf5ba3ec2bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Documentation"
    }
   },
   "source": [
    "<!--Author: May Merkle-Tan-->\n",
    "<!--Author: @may-merkletan_data --> \n",
    "\n",
    "# YOLO11n Object Detection on Databricks Serverless GPU (Distributed)\n",
    "\n",
    "This notebook covers a workflow for training [YOLO11n](https://docs.ultralytics.com/models/yolo11/) on [COCO128 dataset](https://www.kaggle.com/datasets/ultralytics/coco128) using [Databricks Serverless GPU](https://docs.databricks.com/aws/en/compute/serverless/dependencies) with **distributed multi-node training** across multiple A10 GPUs, [MLflow tracking](https://docs.databricks.com/en/mlflow/index.html), and [Model Serving](https://docs.databricks.com/en/machine-learning/model-serving/index.html) deployment.\n",
    "\n",
    "---\n",
    "\n",
    "> #### ⚠️ Dataset Size & Overfitting\n",
    ">\n",
    "> **COCO128 is for demonstration only.** With only 128 images (~80 train, ~24 val, ~24 test), this dataset is too small for production. The model will severely overfit, with validation loss likely increasing after initial epochs.\n",
    ">\n",
    "> **For production:** Use larger datasets (e.g. with 100K+ images, or 1K+ domain-specific images). This workflow is production-ready and can be applied to larger datasets by updating data paths. See [NuInsSeg](https://github.com/databricks-industry-solutions/cv-playground/tree/main/projects/NuInsSeg) for a real-world example using YOLO instance segmentation on celltypes nuclei data.\n",
    "\n",
    "---\n",
    "\n",
    "### Workflow Overview\n",
    "1. **Environment Setup**: Install packages, import libraries, define helper functions\n",
    "2. **Unity Catalog Configuration**: Create schema, volume, and configure paths\n",
    "3. **Data Preparation**: Download COCO128, create train/val/test splits\n",
    "4. **MLflow Configuration**: Configure experiment and infer model signature\n",
    "5. **Model Training**: Train YOLO11n with **distributed multi-GPU training**, MLflow tracking, and register to Unity Catalog\n",
    "6. **Model Evaluation**: Test on validation/test sets, validate serving format\n",
    "7. **Model Deployment**: Deploy to serving endpoint with [AI Gateway inference tables](https://docs.databricks.com/en/ai-gateway/inference-tables/) automatically enabled for request/response logging\n",
    "\n",
    "### Key Features\n",
    "* **Distributed multi-node training** - Automatic scaling across multiple A10 GPUs using `@distributed` decorator\n",
    "* **[Base64 input format](https://en.wikipedia.org/wiki/Base64)** - Universal, works across network boundaries\n",
    "* **Bounding box output** - Complete detection results (class, confidence, coordinates)\n",
    "* **[Custom MLflow PyFunc wrapper](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models)** - Clean integration with MLflow\n",
    "* **[Unity Catalog Volumes](https://docs.databricks.com/en/connect/unity-catalog/volumes.html)** - Organized artifacts storage\n",
    "* **[WorkspaceClient SDK](https://docs.databricks.com/en/dev-tools/sdk-python.html)** - Modern SDK methods for endpoint management\n",
    "* **Production-ready** - Tested locally before deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de07131a-dafd-400d-82e5-19f1839ceb30",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Connect to serverless GPU compute and install dependencies\n",
    "\n",
    "Connect your notebook to serverless A10 GPU:\n",
    "\n",
    "1. Click the **Connect** dropdown at the top.\n",
    "1. Select **Serverless GPU**.\n",
    "1. Open the **Environment** side panel on the right side of the notebook.\n",
    "1. Set **Accelerator** to A10 for this demo. You do not need to install any dependencies in the environment panel to run this notebook.\n",
    "1. Under **Environment** select **`AI v4 (Beta)`** Base environment \n",
    "1. Select **Apply** and click **Confirm** to apply this environment to your notebook.\n",
    "\n",
    "---\n",
    "\n",
    "> **⚠️ IMPORTANT NOTE:**  \n",
    "> This notebook requires **`AI v4` Environment** for Serverless GPU and uses the **`@distributed`** decorator for multi-node training, which automatically provisions multiple A10 GPU nodes as needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d2c77b-61a6-4ca7-a884-14206b44762f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Environment Setup"
    }
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install required packages and configure Python environment for YOLO training on Serverless GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8b9120-1b2e-4f47-85b8-26f4ee90bda4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Required Packages"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PACKAGE INSTALLATION FOR SERVERLESS GPU\n",
    "# ============================================================\n",
    "\n",
    "# %pip install databricks-serverless-gpu         # Serverless GPU for multi-node distributed training (not needed - pre-installed in runtime)\n",
    "%pip install -U mlflow>=3.0                    # MLflow for experiment tracking and model registry\n",
    "%pip install ultralytics==8.3.204              # YOLO11n object detection framework\n",
    "%pip install nvidia-ml-py==13.580.82           # NVIDIA GPU monitoring\n",
    "%pip install threadpoolctl==3.1.0              # Controls CPU thread usage\n",
    "%pip install pyrsmi==0.2.0                     # AMD GPU monitoring (if using AMD GPUs)\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "# Set Ultralytics config directory before importing (avoids permission errors)\n",
    "import os\n",
    "import uuid\n",
    "config_dir = f'/tmp/yolo_config_{uuid.uuid4().hex[:8]}'\n",
    "os.environ['YOLO_CONFIG_DIR'] = config_dir\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "\n",
    "print(\"[OK] Packages installed and Python restarted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c51eea-58a6-4661-919b-4febc508f1e1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify Package Installation"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PACKAGE VERIFICATION\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import importlib.metadata\n",
    "\n",
    "print(\"Checking required packages...\\n\")\n",
    "\n",
    "missing_packages = []\n",
    "installed_packages = {}\n",
    "\n",
    "# Check each required package using importlib.metadata\n",
    "packages_to_check = [\n",
    "    ('mlflow', 'mlflow>=3.0'),\n",
    "    ('ultralytics', 'ultralytics==8.3.204'),\n",
    "    ('opencv-python', 'opencv-python (provides cv2)'),\n",
    "    ('nvidia-ml-py', 'nvidia-ml-py==13.580.82'),\n",
    "    ('threadpoolctl', 'threadpoolctl==3.1.0'),\n",
    "    ('pyrsmi', 'pyrsmi==0.2.0')\n",
    "]\n",
    "\n",
    "for package_name, package_spec in packages_to_check:\n",
    "    try:\n",
    "        version = importlib.metadata.version(package_name)\n",
    "        installed_packages[package_name] = version\n",
    "        print(f\"✓ {package_name}: {version}\")\n",
    "    except importlib.metadata.PackageNotFoundError:\n",
    "        missing_packages.append(package_spec)\n",
    "        print(f\"✗ {package_name}: NOT INSTALLED\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if missing_packages:\n",
    "    print(\"[ACTION REQUIRED] Missing packages detected!\")\n",
    "    print(\"\\nInstall missing packages and restart kernel.\")\n",
    "    for pkg in missing_packages:\n",
    "        print(f\"   - {pkg}\")\n",
    "else:\n",
    "    print(\"[OK] All required packages are installed!\")\n",
    "    print(f\"   Python version: {sys.version.split()[0]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40d94b4-4cc2-4549-be9e-e967ffc00c1e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper Functions Overview"
    }
   },
   "source": [
    "## Helper Functions\n",
    "\n",
    "Utility functions for the complete YOLO training and deployment workflow:\n",
    "\n",
    "**Data Management:**\n",
    "* `download_file()` - Download models and configs to UC Volume\n",
    "* `download_and_extract_dataset()` - Download and extract COCO128\n",
    "* `split_dataset()` - Create reproducible train/val/test splits\n",
    "\n",
    "**MLflow Integration:**\n",
    "* `infer_model_signature()` - Automatically infer model signature from predictions\n",
    "* `setup_mlflow_experiment()` - Configure MLflow with system metrics\n",
    "* `register_yolo_model()` - Register model to Unity Catalog with custom wrapper\n",
    "\n",
    "**Model Evaluation:**\n",
    "* `evaluate_model_on_split()` - Evaluate and visualize predictions on data splits\n",
    "\n",
    "**Custom Wrapper:**\n",
    "* `YOLOWrapper` - [MLflow PyFunc wrapper](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models) for YOLO models\n",
    "  * **Input:** [Base64-encoded](https://en.wikipedia.org/wiki/Base64) images (universal format, works across network boundaries)\n",
    "  * **Output:** DataFrame with class, confidence, bounding boxes (11 columns)\n",
    "  * **Purpose:** Enables deployment to [Model Serving endpoints](https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html)\n",
    "  * **Production-ready:** Tested locally before deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "907ab588-7bc9-45d6-a1f7-017fba3eec0c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper Functions"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import random\n",
    "import yaml\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import importlib.metadata\n",
    "\n",
    "\n",
    "def download_file(url, destination, description=\"file\"):\n",
    "    \"\"\"Download a file from URL to destination path.\"\"\"\n",
    "    if os.path.exists(destination):\n",
    "        print(f\"[INFO] {description} already exists at: {destination}\")\n",
    "        print(f\"   Skipping download\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"Downloading {description}...\")\n",
    "    print(f\"   From: {url}\")\n",
    "    print(f\"   To: {destination}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "            with open(destination, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"[OK] Downloaded successfully\")\n",
    "            if destination.endswith('.pt'):\n",
    "                print(f\"   Size: {os.path.getsize(destination) / (1024*1024):.2f} MB\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"[ERROR] Download failed with status code: {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Download failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_and_extract_dataset(download_url, extraction_path):\n",
    "    \"\"\"Download and extract a zip dataset.\"\"\"\n",
    "    print(\"Downloading dataset...\")\n",
    "    response = requests.get(download_url)\n",
    "    \n",
    "    print(\"Extracting dataset...\")\n",
    "    z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "    z.extractall(extraction_path)\n",
    "    \n",
    "    print(f\"[OK] Dataset downloaded and extracted to: {extraction_path}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def split_dataset(source_images_dir, source_labels_dir, base_images_dir, base_labels_dir,\n",
    "                  train_ratio=0.625, val_ratio=0.1875, random_seed=42):\n",
    "    \"\"\"Split dataset into train/val/test sets with reproducible random seed.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET SPLITTING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    random.seed(random_seed)\n",
    "    print(f\"\\nRandom seed: {random_seed}\")\n",
    "    \n",
    "    test_ratio = 1.0 - train_ratio - val_ratio\n",
    "    print(f\"Split ratios: Train={train_ratio:.1%}, Val={val_ratio:.1%}, Test={test_ratio:.1%}\\n\")\n",
    "    \n",
    "    # Get all images\n",
    "    all_images = sorted([f for f in os.listdir(source_images_dir) if f.endswith('.jpg')])\n",
    "    print(f\"Total images: {len(all_images)}\")\n",
    "    \n",
    "    # Shuffle and split\n",
    "    random.shuffle(all_images)\n",
    "    train_size = int(len(all_images) * train_ratio)\n",
    "    val_size = int(len(all_images) * val_ratio)\n",
    "    \n",
    "    train_images = all_images[:train_size]\n",
    "    val_images = all_images[train_size:train_size + val_size]\n",
    "    test_images = all_images[train_size + val_size:]\n",
    "    \n",
    "    print(f\"Split sizes: Train={len(train_images)}, Val={len(val_images)}, Test={len(test_images)}\\n\")\n",
    "    \n",
    "    # Create directories\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        os.makedirs(f\"{base_images_dir}/{split_name}\", exist_ok=True)\n",
    "        os.makedirs(f\"{base_labels_dir}/{split_name}\", exist_ok=True)\n",
    "    \n",
    "    # Copy files\n",
    "    print(\"Copying files to splits...\")\n",
    "    for split_name, image_list in [('train', train_images), ('val', val_images), ('test', test_images)]:\n",
    "        print(f\"  Processing {split_name} split ({len(image_list)} images)...\")\n",
    "        for img_name in image_list:\n",
    "            # Copy image\n",
    "            src_img = os.path.join(source_images_dir, img_name)\n",
    "            dst_img = os.path.join(base_images_dir, split_name, img_name)\n",
    "            shutil.copy2(src_img, dst_img)\n",
    "            \n",
    "            # Copy label if exists\n",
    "            label_name = img_name.replace('.jpg', '.txt')\n",
    "            src_label = os.path.join(source_labels_dir, label_name)\n",
    "            dst_label = os.path.join(base_labels_dir, split_name, label_name)\n",
    "            if os.path.exists(src_label):\n",
    "                shutil.copy2(src_label, dst_label)\n",
    "        print(f\"    [OK] {split_name}: {len(image_list)} images copied\")\n",
    "    \n",
    "    print(f\"\\n[OK] Dataset split complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    return len(train_images), len(val_images), len(test_images)\n",
    "\n",
    "\n",
    "def infer_model_signature(model_path, sample_image_path):\n",
    "    \"\"\"Infer MLflow model signature using actual model predictions.\"\"\"\n",
    "    import base64\n",
    "    \n",
    "    print(\"[INFO] Inferring model signature...\\n\")\n",
    "    \n",
    "    # Load YOLO model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Read and encode image as base64\n",
    "    with open(sample_image_path, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    \n",
    "    # Create input example\n",
    "    input_example = pd.DataFrame({\"image_base64\": [image_base64]})\n",
    "    \n",
    "    # Create YOLOWrapper instance and get predictions to infer output schema\n",
    "    wrapper = YOLOWrapper()\n",
    "    \n",
    "    # Simulate load_context\n",
    "    class MockContext:\n",
    "        def __init__(self, model_path):\n",
    "            self.artifacts = {\"yolo_model\": model_path}\n",
    "    \n",
    "    wrapper.load_context(MockContext(model_path))\n",
    "    \n",
    "    # Get output example by running prediction\n",
    "    output_example = wrapper.predict(None, input_example)\n",
    "    \n",
    "    # Use MLflow's infer_signature to automatically create signature\n",
    "    signature = mlflow.models.infer_signature(input_example, output_example)\n",
    "    \n",
    "    print(f\"[OK] Model signature inferred successfully!\")\n",
    "    print(f\"   Input: DataFrame with 'image_base64' column (base64 string)\")\n",
    "    print(f\"   Output: DataFrame with {len(output_example.columns)} columns\")\n",
    "    print(f\"   Columns: {', '.join(output_example.columns.tolist())}\")\n",
    "    \n",
    "    # Optional: Show how to use manual schema (commented out)\n",
    "    # from mlflow.types.schema import Schema, ColSpec\n",
    "    # from mlflow.models.signature import ModelSignature\n",
    "    # input_schema = Schema([ColSpec(\"string\", \"image_base64\")])\n",
    "    # output_schema = Schema([\n",
    "    #     ColSpec(\"string\", \"class_name\"),\n",
    "    #     ColSpec(\"long\", \"class_num\"),\n",
    "    #     ColSpec(\"double\", \"confidence\"),\n",
    "    #     ColSpec(\"double\", \"bbox_x1\"),\n",
    "    #     ColSpec(\"double\", \"bbox_y1\"),\n",
    "    #     ColSpec(\"double\", \"bbox_x2\"),\n",
    "    #     ColSpec(\"double\", \"bbox_y2\"),\n",
    "    #     ColSpec(\"double\", \"bbox_center_x\"),\n",
    "    #     ColSpec(\"double\", \"bbox_center_y\"),\n",
    "    #     ColSpec(\"double\", \"bbox_width\"),\n",
    "    #     ColSpec(\"double\", \"bbox_height\")\n",
    "    # ])\n",
    "    # signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "    \n",
    "    return signature, input_example\n",
    "\n",
    "\n",
    "def setup_mlflow_experiment(notebook_path):\n",
    "    \"\"\"Setup MLflow experiment with system metrics enabled.\"\"\"\n",
    "    notebook_dir = '/'.join(notebook_path.split('/')[:-1])\n",
    "    experiment_name = f\"{notebook_dir}/Experiments_YOLO_CoCo\"\n",
    "    \n",
    "    os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
    "    os.environ['MLFLOW_EXPERIMENT_NAME'] = experiment_name\n",
    "    \n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "    \n",
    "    if 'MLFLOW_RUN_ID' in os.environ:\n",
    "        del os.environ['MLFLOW_RUN_ID']\n",
    "    \n",
    "    print(f\"[OK] MLflow experiment initialized: {experiment_name}\")\n",
    "    print(f\"   Experiment ID: {experiment_id}\")\n",
    "    print(f\"   System metrics: ENABLED\")\n",
    "    return experiment_name, experiment_id\n",
    "\n",
    "\n",
    "class YOLOWrapper(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Custom MLflow wrapper for YOLO models using base64-encoded images.\"\"\"\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load YOLO model from artifacts.\"\"\"\n",
    "        from ultralytics import YOLO\n",
    "        model_path = context.artifacts[\"yolo_model\"]\n",
    "        self.model = YOLO(model_path, task='detect')\n",
    "    \n",
    "    def _format_predictions(self, predictions):\n",
    "        \"\"\"Format YOLO prediction results with bounding boxes.\n",
    "        \n",
    "        Args:\n",
    "            predictions: YOLO prediction results from model.predict()\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame with class, confidence, and bounding box coordinates\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        \n",
    "        all_results = []\n",
    "        for prediction in predictions:\n",
    "            if prediction.boxes is not None:\n",
    "                boxes = prediction.boxes\n",
    "                for i in range(len(boxes)):\n",
    "                    # Get bounding box coordinates in both formats\n",
    "                    box_xyxy = boxes.xyxy[i].cpu().numpy()\n",
    "                    box_xywh = boxes.xywh[i].cpu().numpy()\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        \"class_name\": prediction.names[int(boxes.cls[i])],\n",
    "                        \"class_num\": int(boxes.cls[i]),\n",
    "                        \"confidence\": float(boxes.conf[i]),\n",
    "                        \"bbox_x1\": float(box_xyxy[0]),\n",
    "                        \"bbox_y1\": float(box_xyxy[1]),\n",
    "                        \"bbox_x2\": float(box_xyxy[2]),\n",
    "                        \"bbox_y2\": float(box_xyxy[3]),\n",
    "                        \"bbox_center_x\": float(box_xywh[0]),\n",
    "                        \"bbox_center_y\": float(box_xywh[1]),\n",
    "                        \"bbox_width\": float(box_xywh[2]),\n",
    "                        \"bbox_height\": float(box_xywh[3])\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(all_results)\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"Run YOLO prediction on base64-encoded images.\n",
    "        \n",
    "        Args:\n",
    "            context: MLflow context\n",
    "            model_input: DataFrame with 'image_base64' column (base64-encoded images)\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame with detection results including bounding boxes\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        import base64\n",
    "        from PIL import Image\n",
    "        import io\n",
    "        import numpy as np\n",
    "        \n",
    "        if not isinstance(model_input, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a DataFrame with 'image_base64' column\")\n",
    "        \n",
    "        if 'image_base64' not in model_input.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'image_base64' column with base64-encoded images\")\n",
    "        \n",
    "        # Process base64-encoded images\n",
    "        all_predictions = []\n",
    "        for image_base64 in model_input['image_base64'].tolist():\n",
    "            # Decode base64 to image\n",
    "            image_bytes = base64.b64decode(image_base64)\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image_array = np.array(image)\n",
    "            \n",
    "            # Run prediction\n",
    "            predictions = self.model.predict(image_array, verbose=False)\n",
    "            all_predictions.extend(predictions)\n",
    "        \n",
    "        return self._format_predictions(all_predictions)\n",
    "\n",
    "\n",
    "def register_yolo_model(run_id, model_path, catalog_name, schema_name, model_name,\n",
    "                       signature=None, input_example=None, data_yaml_path=None):\n",
    "    \"\"\"Register YOLO model to Unity Catalog with custom wrapper.\"\"\"\n",
    "    registered_model_name = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "    ultralytics_version = importlib.metadata.version('ultralytics')\n",
    "    cloudpickle_version = importlib.metadata.version('cloudpickle')\n",
    "    \n",
    "    print(f\"\\n[INFO] Registering model to Unity Catalog...\")\n",
    "    print(f\"   Model name: {registered_model_name}\")\n",
    "    print(f\"   Using custom YOLO wrapper (base64 input, bbox output)\")\n",
    "    print(f\"   Pinning CloudPickle version: {cloudpickle_version}\")\n",
    "    \n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        if data_yaml_path:\n",
    "            mlflow.log_artifact(data_yaml_path, \"input_data\")\n",
    "        \n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",\n",
    "            python_model=YOLOWrapper(),\n",
    "            artifacts={\"yolo_model\": model_path},\n",
    "            signature=signature,\n",
    "            input_example=input_example,\n",
    "            registered_model_name=registered_model_name,\n",
    "            pip_requirements=[\n",
    "                f\"ultralytics=={ultralytics_version}\",\n",
    "                f\"cloudpickle=={cloudpickle_version}\",\n",
    "                \"torch\", \n",
    "                \"torchvision\", \n",
    "                \"pillow\", \n",
    "                \"numpy\"\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    print(f\"   [OK] Model registered: {registered_model_name}\")\n",
    "    return registered_model_name\n",
    "\n",
    "\n",
    "def evaluate_model_on_split(model, image_dir, split_name, output_dir, run_id, \n",
    "                           registered_model_name, organized_run_name, num_samples=3):\n",
    "    \"\"\"Evaluate model on a dataset split and save results.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{split_name.upper()} SET EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    images = glob.glob(f\"{image_dir}/*.jpg\")\n",
    "    \n",
    "    if not images:\n",
    "        print(f\"[WARNING] No {split_name} images found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{split_name.capitalize()} set: {len(images)} images\\n\")\n",
    "    \n",
    "    # Visualize sample predictions\n",
    "    sample_images = images[:num_samples]\n",
    "    fig, axes = plt.subplots(1, len(sample_images), figsize=(15, 5))\n",
    "    if len(sample_images) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    results = []\n",
    "    for i, img_path in enumerate(sample_images):\n",
    "        print(f\"Sample {i+1}/{len(sample_images)}: {img_path.split('/')[-1]}\")\n",
    "        predictions = model.predict(img_path, verbose=False)\n",
    "        \n",
    "        if len(predictions) > 0:\n",
    "            result = predictions[0]\n",
    "            annotated_img = result.plot()\n",
    "            axes[i].imshow(annotated_img)\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "            if result.boxes is not None:\n",
    "                num_detections = len(result.boxes)\n",
    "                axes[i].set_title(f\"{img_path.split('/')[-1]}\\n{num_detections} objects\", fontsize=10)\n",
    "                print(f\"   [OK] Detections: {num_detections} objects\")\n",
    "                \n",
    "                img_results = {\n",
    "                    \"image\": img_path.split('/')[-1],\n",
    "                    \"num_detections\": num_detections,\n",
    "                    \"detections\": []\n",
    "                }\n",
    "                \n",
    "                for j in range(min(num_detections, 3)):\n",
    "                    class_name = result.names[int(result.boxes.cls[j])]\n",
    "                    confidence = float(result.boxes.conf[j])\n",
    "                    print(f\"      - {class_name}: {confidence:.3f}\")\n",
    "                    img_results[\"detections\"].append({\n",
    "                        \"class_name\": class_name,\n",
    "                        \"confidence\": confidence\n",
    "                    })\n",
    "                results.append(img_results)\n",
    "        print()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"{split_name.capitalize()} Set Predictions - Run {run_id[:8]}\", fontsize=14, y=1.02)\n",
    "    \n",
    "    plot_path = os.path.join(output_dir, f\"{split_name}_predictions.png\")\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"[OK] Plot saved to: {plot_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Save results JSON\n",
    "    json_path = os.path.join(output_dir, f\"{split_name}_results.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump({\n",
    "            \"run_id\": run_id,\n",
    "            \"registered_model\": registered_model_name,\n",
    "            \"timestamp\": organized_run_name.split('_run_')[0],\n",
    "            \"num_images\": len(images),\n",
    "            \"sample_results\": results\n",
    "        }, f, indent=2)\n",
    "    print(f\"[OK] Results saved to: {json_path}\")\n",
    "    \n",
    "    # Log to MLflow\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        mlflow.log_artifact(plot_path, split_name)\n",
    "        mlflow.log_artifact(json_path, split_name)\n",
    "    \n",
    "    print(f\"\\n[OK] {split_name.upper()} SET EVALUATION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "print(\"[OK] Helper functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c531b751-2f78-4dc6-96dc-309b003cb196",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup I/O Locations"
    }
   },
   "source": [
    "## Setup Unity Catalog Storage\n",
    "\n",
    "Configure catalog, schema, and volume for persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464bf70e-6939-48b1-8a5b-e4af3c306e11",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clear existing widgets"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d14e30-c744-4232-bf5f-8ad34d9f26e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Unity Catalog Schema and Volume"
    }
   },
   "outputs": [],
   "source": [
    "# Define widgets for catalog, schema, volume, model name, and deployment approval\n",
    "dbutils.widgets.text(\"catalog_name\", \"main\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"sgc-nightly\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"volume_name\", \"yolo_sgc_distributed\", \"Volume Name\")\n",
    "dbutils.widgets.text(\"model_name\", \"yolo11n_coco128_sgc_distributed\", \"Model Name\")\n",
    "dbutils.widgets.dropdown(\"proceed_with_deployment\", \"false\", [\"false\", \"true\"], \"Proceed with Deployment\")\n",
    "\n",
    "# Get widget values\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "volume_name = dbutils.widgets.get(\"volume_name\")\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "proceed_with_deployment_str = dbutils.widgets.get(\"proceed_with_deployment\")\n",
    "\n",
    "print(f\"[Configuration]\")\n",
    "print(f\"   Catalog: {catalog_name}\")\n",
    "print(f\"   Schema: {schema_name}\")\n",
    "print(f\"   Volume: {volume_name}\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Proceed with Deployment: {proceed_with_deployment_str}\")\n",
    "\n",
    "print(f\"\\nUsing catalog: {catalog_name} (already exists)\")\n",
    "\n",
    "# Create schema if it doesn't exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog_name}`.`{schema_name}`\")\n",
    "print(f\"[OK] Schema: {catalog_name}.{schema_name}\")\n",
    "\n",
    "# Create volume for persistent storage\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS `{catalog_name}`.`{schema_name}`.`{volume_name}`\")\n",
    "print(f\"[OK] Volume: {catalog_name}.{schema_name}.{volume_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c934f1e5-273f-4448-ad06-984749ce9d20",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configure Project Paths"
    }
   },
   "outputs": [],
   "source": [
    "# Get Unity Catalog parameters\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "volume_name = dbutils.widgets.get(\"volume_name\")\n",
    "\n",
    "# Construct volume path from parameters\n",
    "project_location = f'/Volumes/{catalog_name}/{schema_name}/{volume_name}/'\n",
    "print(f\"Using Unity Catalog Volume: {catalog_name}.{schema_name}.{volume_name}\")\n",
    "print(f\"Volume path: {project_location}\")\n",
    "\n",
    "# Create subdirectories in the volume\n",
    "os.makedirs(f'{project_location}runs/', exist_ok=True)       # Training runs (organized by task/model/dataset)\n",
    "os.makedirs(f'{project_location}data/', exist_ok=True)       # Dataset storage\n",
    "os.makedirs(f'{project_location}raw_model/', exist_ok=True)  # Pretrained models\n",
    "\n",
    "# Ephemeral /tmp/ location for faster I/O during training\n",
    "tmp_project_location = \"/tmp/training_results/\"\n",
    "os.makedirs(tmp_project_location, exist_ok=True)\n",
    "\n",
    "print(f\"\\n[OK] Project directories created:\")\n",
    "print(f\"   Runs: {project_location}runs/\")\n",
    "print(f\"   Data: {project_location}data/\")\n",
    "print(f\"   Raw models: {project_location}raw_model/\")\n",
    "print(f\"   Temp (training): {tmp_project_location}  # Ephemeral, fast I/O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e06f4769-62df-40ba-97e5-28d352ed59e0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Project Folder Structure"
    }
   },
   "source": [
    "## Project Folder Structure\n",
    "\n",
    "Unity Catalog Volume organization:\n",
    "\n",
    "```\n",
    "/Volumes/{catalog}/{schema}/{volume}/\n",
    "├── data/\n",
    "│   ├── coco128.yaml                    # Dataset configuration\n",
    "│   └── coco128/\n",
    "│       ├── images/\n",
    "│       │   ├── train2017/              # Original 128 images (from zip)\n",
    "│       │   ├── train/  val/  test/     # Custom splits (80/24/24)\n",
    "│       └── labels/\n",
    "│           ├── train2017/              # Original labels\n",
    "│           └── train/  val/  test/     # Split labels\n",
    "│\n",
    "├── raw_model/\n",
    "│   └── yolo11n.pt                      # Pretrained YOLO11n weights\n",
    "│\n",
    "└── runs/\n",
    "    └── {task}_{model}_{dataset}_{timestamp}_run_{mlflow_run_id}/\n",
    "        ├── train/                      # MLflow training outputs\n",
    "        │   ├── weights/ (best.pt, last.pt)\n",
    "        │   └── results.csv, confusion_matrix.png\n",
    "        ├── validation_metrics/         # YOLO validation outputs\n",
    "        ├── validation_samples/         # Custom evaluation samples\n",
    "        └── test_samples/               # Test evaluation samples\n",
    "```\n",
    "\n",
    "**Run Naming:** `detection_yolo11n_coco128_20260120_143052_run_{mlflow_run_id}`\n",
    "* Includes task, model, dataset, timestamp, and MLflow run ID for easy identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f064fee9-26ef-4575-abef-7b482bdf6433",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download Pretrained Model"
    }
   },
   "source": [
    "## Download Pretrained YOLO Model\n",
    "\n",
    "Download YOLO11n pretrained weights to Unity Catalog Volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a4872b7-4071-4881-b1f8-359d791e4f44",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download YOLO11n to Unity Catalog Volume"
    }
   },
   "outputs": [],
   "source": [
    "# Download pretrained YOLO11n model to Unity Catalog Volume\n",
    "model_path = f\"{project_location}raw_model/yolo11n.pt\"\n",
    "model_url = \"https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt\"\n",
    "\n",
    "download_file(model_url, model_path, \"YOLO11n model\")\n",
    "print(f\"\\n[OK] Pretrained model ready at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb57faa-8cd4-4efe-bc23-88aebc793fd0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dataset Preparation"
    }
   },
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Download and configure COCO128 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d1e829-1767-401e-8fcb-6a405b2afb75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download and Prepare Dataset"
    }
   },
   "outputs": [],
   "source": [
    "# Download COCO128 dataset configuration to UC Volume\n",
    "import yaml\n",
    "\n",
    "# Create data directory in UC Volume\n",
    "os.makedirs(f'{project_location}data/coco128', exist_ok=True)\n",
    "\n",
    "# Download config directly to UC Volume\n",
    "config_url = \"https://github.com/ultralytics/ultralytics/raw/main/ultralytics/cfg/datasets/coco128.yaml\"\n",
    "config_path = f\"{project_location}data/coco128.yaml\"  # UC Volume path\n",
    "\n",
    "download_file(config_url, config_path, \"COCO128 config\")\n",
    "\n",
    "# Load and update configuration\n",
    "with open(config_path, 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "print(f\"\\n[Dataset Configuration]\")\n",
    "print(f\"   Dataset: {data.get('path', 'coco128')}\")\n",
    "print(f\"   Classes: {data.get('nc', 'unknown')}\")\n",
    "print(f\"   Download URL: {data.get('download', 'N/A')}\")\n",
    "\n",
    "# Update paths for Unity Catalog Volume\n",
    "data['path'] = f\"{project_location}data/coco128\"\n",
    "\n",
    "# Check if dataset already exists\n",
    "dataset_images_dir = f\"{project_location}data/coco128/images/train2017\"\n",
    "if os.path.exists(dataset_images_dir) and len(os.listdir(dataset_images_dir)) > 0:\n",
    "    print(f\"\\n[INFO] Dataset already exists at: {dataset_images_dir}\")\n",
    "    print(f\"   Found {len(os.listdir(dataset_images_dir))} images\")\n",
    "    print(f\"   Skipping download\")\n",
    "else:\n",
    "    # Download and extract dataset\n",
    "    extraction_path = f\"{project_location}data\"\n",
    "    download_and_extract_dataset(data['download'], extraction_path)\n",
    "\n",
    "# Save updated configuration to UC Volume\n",
    "data_yaml_path = f\"{project_location}data/coco128.yaml\"\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(data, f, default_flow_style=False)\n",
    "\n",
    "print(f\"\\n[OK] Dataset configuration saved to UC Volume: {data_yaml_path}\")\n",
    "print(f\"   All dataset files in: {project_location}data/coco128/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "359206f2-de4f-4cc5-acb5-aae5bc8cfb90",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dataset Splitting"
    }
   },
   "source": [
    "## Dataset Splits\n",
    "\n",
    "Split COCO128 into train (62.5%), val (18.75%), and test (18.75%) sets with reproducible random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e30669dc-2736-4e9f-8087-919d524e9d15",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Train/Val/Test Splits"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into train/val/test with reproducible random seed\n",
    "source_images_dir = f\"{project_location}data/coco128/images/train2017\"\n",
    "source_labels_dir = f\"{project_location}data/coco128/labels/train2017\"\n",
    "base_images_dir = f\"{project_location}data/coco128/images\"\n",
    "base_labels_dir = f\"{project_location}data/coco128/labels\"\n",
    "\n",
    "train_size, val_size, test_size = split_dataset(\n",
    "    source_images_dir=source_images_dir,\n",
    "    source_labels_dir=source_labels_dir,\n",
    "    base_images_dir=base_images_dir,\n",
    "    base_labels_dir=base_labels_dir,\n",
    "    train_ratio=0.625,  # 62.5%\n",
    "    val_ratio=0.1875,   # 18.75%\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit summary:\")\n",
    "print(f\"  - Train: {train_size} images (62.5%)\")\n",
    "print(f\"  - Val: {val_size} images (18.75%)\")\n",
    "print(f\"  - Test: {test_size} images (18.75%)\")\n",
    "print(f\"  - Random seed: 42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33e17eba-8a74-45bf-af1f-c9d606ce81fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update data.yaml for Splits"
    }
   },
   "outputs": [],
   "source": [
    "# Update data.yaml to use train/val/test splits\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    yaml_content = yaml.safe_load(f)\n",
    "\n",
    "# Update paths\n",
    "yaml_content['train'] = f\"{project_location}data/coco128/images/train\"\n",
    "yaml_content['val'] = f\"{project_location}data/coco128/images/val\"\n",
    "yaml_content['test'] = f\"{project_location}data/coco128/images/test\"\n",
    "\n",
    "# Save updated configuration\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "\n",
    "print(f\"[OK] data.yaml updated with train/val/test splits\")\n",
    "print(f\"   Train: {yaml_content['train']}\")\n",
    "print(f\"   Val: {yaml_content['val']}\")\n",
    "print(f\"   Test: {yaml_content['test']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d829ce2-835a-4d9c-92fb-742c991e4a6e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "MLflow Setup"
    }
   },
   "source": [
    "## MLflow Configuration\n",
    "\n",
    "Infer model signature and configure experiment tracking with system metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da48cc45-99f1-4959-a96a-d1ab35382f5c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Infer MLflow Model Signature"
    }
   },
   "outputs": [],
   "source": [
    "# Infer model signature from sample prediction\n",
    "# This defines the input/output schema for the serving endpoint\n",
    "# Input: base64-encoded images\n",
    "# Output: class, confidence, bounding boxes (11 columns)\n",
    "\n",
    "model_path = f\"{project_location}raw_model/yolo11n.pt\"\n",
    "\n",
    "# Find a sample image from training set\n",
    "sample_images = glob.glob(f\"{project_location}data/coco128/images/train/*.jpg\")\n",
    "\n",
    "if sample_images:\n",
    "    signature, input_example = infer_model_signature(model_path, sample_images[0])\n",
    "    print(f\"\\n[OK] Signature and input example ready for model registration\")\n",
    "else:\n",
    "    print(\"[WARNING] No sample images found. Run dataset preparation first.\")\n",
    "    signature = None\n",
    "    input_example = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08198146-adcf-4162-88d3-f8f6661ea3d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check signature and input format"
    }
   },
   "outputs": [],
   "source": [
    "# signature, input_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92654133-4928-427e-9d01-46290886f392",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configure for MLflow Integration and Autologging"
    }
   },
   "outputs": [],
   "source": [
    "# Configure YOLO to use MLflow\n",
    "from ultralytics import settings\n",
    "settings.update({\"mlflow\": True})\n",
    "\n",
    "# Enable MLflow autologging for system metrics\n",
    "mlflow.autolog(disable=False)\n",
    "\n",
    "print(f\"\\n[MLflow Configuration]\")\n",
    "print(f\"   YOLO MLflow integration: Enabled\")\n",
    "print(f\"   MLflow autologging: Enabled\")\n",
    "print(f\"   System metrics: Enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659998a6-0118-4ca3-9e27-fab85a303cbb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup MLflow Experiment"
    }
   },
   "outputs": [],
   "source": [
    "# Setup MLflow experiment with system metrics\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "\n",
    "# Extract username from notebook path to create experiment in workspace\n",
    "# This avoids the error when notebook is in a Git folder\n",
    "path_parts = notebook_path.split('/')\n",
    "if 'Users' in path_parts:\n",
    "    user_idx = path_parts.index('Users')\n",
    "    username = path_parts[user_idx + 1]\n",
    "    # Create experiment in user's workspace, not in Git folder\n",
    "    experiment_name = f\"/Users/{username}/Experiments_YOLO_CoCo\"\n",
    "else:\n",
    "    # Fallback: use a generic workspace location\n",
    "    experiment_name = \"/Shared/Experiments_YOLO_CoCo\"\n",
    "\n",
    "# Enable system metrics\n",
    "os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
    "os.environ['MLFLOW_EXPERIMENT_NAME'] = experiment_name\n",
    "\n",
    "# Set or create experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "# Clear any existing run ID\n",
    "if 'MLFLOW_RUN_ID' in os.environ:\n",
    "    del os.environ['MLFLOW_RUN_ID']\n",
    "\n",
    "print(f\"[OK] MLflow experiment initialized: {experiment_name}\")\n",
    "print(f\"   Experiment ID: {experiment_id}\")\n",
    "print(f\"   System metrics: ENABLED\")\n",
    "print(f\"   Note: Experiment created in workspace (not in Git folder)\")\n",
    "\n",
    "print(f\"\\n[Ready for Training]\")\n",
    "print(f\"   Experiment: {experiment_name}\")\n",
    "print(f\"   Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ce4204-8ab8-41ba-83f0-6ef73d992ed0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Training"
    }
   },
   "source": [
    "## Model Training\n",
    "\n",
    "Train YOLO11n with MLflow tracking and register to Unity Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b29a09-eaf7-45ec-b661-ed72bb984493",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Distributed Training Overview"
    }
   },
   "source": [
    "### Distributed Multi-Node GPU Training with Serverless GPU\n",
    "\n",
    "This notebook is configured for **distributed training across multiple A10 GPU nodes** using Databricks Serverless GPU.\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "* **Serverless GPU API**: Uses `@distributed` decorator for automatic multi-node orchestration    \n",
    "* **Multi-node training**: Automatically scales across multiple A10 GPU nodes        \n",
    "* **Data parallelism**: Each GPU processes different batches in parallel      \n",
    "* **Gradient synchronization**: Efficient all-reduce operations across nodes     \n",
    "* **No cluster management**: Serverless handles provisioning and scaling     \n",
    "* **Cost optimization**: Automatic scale-to-zero when not training     \n",
    "\n",
    "#### Key Differences from Single-GPU Training:\n",
    "\n",
    "**Traditional Approach (Single GPU):**\n",
    "```python\n",
    "model = YOLO('yolo11n.pt')\n",
    "model.train(device=0, batch=16, epochs=100)\n",
    "```\n",
    "\n",
    "**Distributed Approach (8 GPUs):**\n",
    "```python\n",
    "@distributed(gpus=8, gpu_type='A10', remote=True)\n",
    "def run_distributed_training():\n",
    "    model = YOLO('yolo11n.pt')\n",
    "    model.train(device=local_rank, batch=16, epochs=100)  # 16 per GPU = 128 total\n",
    "\n",
    "run_distributed_training.distributed()\n",
    "```\n",
    "\n",
    "#### How It Works:\n",
    "\n",
    "1. Wrap training function with `@distributed(gpus=8, gpu_type='A10', remote=True)`\n",
    "2. Databricks provisions multiple A10 GPU nodes automatically (e.g., 8 single-GPU nodes or 2 quad-GPU nodes)\n",
    "3. PyTorch Distributed Data Parallel (DDP) is set up automatically\n",
    "4. Training data is distributed across all GPUs\n",
    "5. Each GPU computes gradients independently\n",
    "6. Gradients are synchronized using all-reduce (NCCL)\n",
    "7. Model weights are updated consistently across all nodes\n",
    "8. Resources are cleaned up automatically when done\n",
    "\n",
    "#### Queuing and Resource Provisioning:\n",
    "\n",
    "Before training begins, Serverless GPU goes through a provisioning phase:\n",
    "\n",
    "1. **Job Submission**: When you call `.distributed()`, the job is submitted to the Serverless GPU queue\n",
    "2. **Resource Allocation**: Databricks allocates 8 A10 GPUs from the serverless pool\n",
    "   * May provision 8 single-GPU nodes, or 2 quad-GPU nodes, or other combinations\n",
    "   * Resources are dynamically allocated based on availability\n",
    "3. **Queue Wait Time**: Job waits in queue if resources are not immediately available\n",
    "   * Typical wait: seconds to a few minutes depending on cluster load\n",
    "   * You'll see \"Launching distributed training...\" status during this phase\n",
    "4. **Environment Setup**: Once resources are allocated:\n",
    "   * Docker containers are initialized on each node\n",
    "   * Python environment and dependencies are loaded\n",
    "   * Network connections between nodes are established\n",
    "5. **Training Starts**: After setup completes, your training function begins executing\n",
    "\n",
    "**Note**: The entire provisioning process is automatic and transparent. You don't need to manage clusters, instances, or networking.\n",
    "\n",
    "#### What Happens During Training:\n",
    "\n",
    "1. **Process Group**: PyTorch DDP process group is initialized automatically\n",
    "2. **Data Distribution**: Training data is split across all 8 GPUs\n",
    "3. **Forward Pass**: Each GPU processes its batch independently (16 images per GPU)\n",
    "4. **Backward Pass**: Gradients are computed locally on each GPU\n",
    "5. **All-Reduce**: Gradients are synchronized across all GPUs using NCCL\n",
    "6. **Weight Update**: Model weights are updated consistently on all GPUs\n",
    "7. **Repeat**: Process continues for all epochs\n",
    "\n",
    "#### Performance Benefits:\n",
    "\n",
    "* **\\~8x faster training** with 8 GPUs (near-linear scaling)\n",
    "* **8x larger effective batch size** (16 per GPU × 8 = 128 total)\n",
    "* **Better convergence** with larger batch sizes\n",
    "* **No cluster configuration** needed\n",
    "* **Automatic resource provisioning** and cleanup\n",
    "\n",
    "#### Customization:\n",
    "\n",
    "Change the number of GPUs by modifying the decorator in the training cell:\n",
    "```python\n",
    "@distributed(gpus=4, gpu_type='A10', remote=True)   # 4 GPUs\n",
    "@distributed(gpus=8, gpu_type='A10', remote=True)   # 8 GPUs (default)\n",
    "@distributed(gpus=16, gpu_type='A10', remote=True)  # 16 GPUs\n",
    "```\n",
    "\n",
    "**No other configuration needed!** Just run the training cell and Serverless GPU handles everything.\n",
    "\n",
    "#### Important Notes:\n",
    "\n",
    "* **Data loading must be inside the decorated function** to avoid pickle errors\n",
    "* **Only rank 0 saves artifacts** to avoid conflicts\n",
    "* **MLflow tracking works automatically** from rank 0\n",
    "* **YOLO detects DDP environment** via RANK, WORLD_SIZE, LOCAL_RANK env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f89db9d-59c7-4940-9ec7-cb55df07bb54",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Distributed Training Setup for YOLO Model on A10 GPUs"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import uuid\n",
    "import shutil\n",
    "from serverless_gpu import distributed\n",
    "\n",
    "# Close any active MLflow runs\n",
    "mlflow.end_run()\n",
    "\n",
    "# Create unique timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Model configuration\n",
    "model_task = \"detection\"     # Task type\n",
    "model_arch = \"yolo11n\"       # Model architecture\n",
    "dataset_name = \"coco128\"     # Dataset name\n",
    "\n",
    "# Distributed training configuration\n",
    "num_gpus = 8                 # Number of A10 GPUs to use\n",
    "gpu_type = 'A10'             # GPU type\n",
    "batch_size_per_gpu = 16      # Batch size per GPU\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DISTRIBUTED TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n[Serverless GPU Configuration]\")\n",
    "print(f\"   GPUs requested: {num_gpus} {gpu_type} GPUs\")\n",
    "print(f\"   Training mode: Multi-node distributed (remote=True)\")\n",
    "print(f\"   Orchestration: Databricks Serverless GPU\")\n",
    "print(f\"   Backend: PyTorch DDP (automatic)\")\n",
    "\n",
    "print(f\"\\n[Training Configuration]\")\n",
    "print(f\"   Task: {model_task}\")\n",
    "print(f\"   Model: {model_arch}\")\n",
    "print(f\"   Dataset: {dataset_name}\")\n",
    "print(f\"   Epochs: 100\")\n",
    "print(f\"   Batch size per GPU: {batch_size_per_gpu}\")\n",
    "print(f\"   Effective batch size: {batch_size_per_gpu * num_gpus} ({batch_size_per_gpu} × {num_gpus} GPUs)\")\n",
    "print()\n",
    "\n",
    "# Get notebook context for MLflow source tracking\n",
    "notebook_context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "notebook_path = notebook_context.notebookPath().get()\n",
    "notebook_id = notebook_context.notebookId().get()\n",
    "\n",
    "# Define distributed training function\n",
    "@distributed(gpus=num_gpus, gpu_type=gpu_type, remote=True)\n",
    "def run_distributed_training():\n",
    "    \"\"\"\n",
    "    Distributed training function that runs across multiple A10 GPU nodes.\n",
    "    \n",
    "    The @distributed decorator:\n",
    "    - Provisions multiple A10 GPUs across multiple nodes\n",
    "    - Sets up environment variables (RANK, WORLD_SIZE, LOCAL_RANK)\n",
    "    - We manually initialize PyTorch DDP process group\n",
    "    - Synchronizes gradients across all GPUs\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    import mlflow\n",
    "    from ultralytics import YOLO\n",
    "    from ultralytics.utils import RANK, LOCAL_RANK\n",
    "    \n",
    "    # Configure MLflow to use Unity Catalog\n",
    "    mlflow.set_registry_uri('databricks-uc')\n",
    "    \n",
    "    # Get distributed training info\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    \n",
    "    # Initialize PyTorch distributed process group (required for YOLO DDP)\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    \n",
    "    try:\n",
    "        # Set MLflow environment variables\n",
    "        os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
    "        os.environ['MLFLOW_EXPERIMENT_NAME'] = experiment_name\n",
    "        \n",
    "        # Only print from rank 0 to avoid duplicate logs\n",
    "        if rank == 0:\n",
    "            print(f\"\\n[Distributed Training Started]\")\n",
    "            print(f\"   World size: {world_size} GPUs\")\n",
    "            print(f\"   Effective batch size: {batch_size_per_gpu * world_size}\")\n",
    "            print(f\"   RANK: {RANK}, LOCAL_RANK: {LOCAL_RANK}\")\n",
    "            print()\n",
    "        \n",
    "        # Create unique temp directory for this training run\n",
    "        tmp_project_location_unique = f\"/tmp/training_results_{uuid.uuid4().hex[:8]}/\"\n",
    "        os.makedirs(tmp_project_location_unique, exist_ok=True)\n",
    "        \n",
    "        # Model path (from Unity Catalog Volume)\n",
    "        model_path = f\"{project_location}raw_model/yolo11n.pt\"\n",
    "        \n",
    "        # Create descriptive run name (without run_id since it doesn't exist yet)\n",
    "        run_name = f\"{model_task}_{model_arch}_{dataset_name}_{timestamp}\"\n",
    "        \n",
    "        # Initialize model\n",
    "        if rank == 0:\n",
    "            print(f\"Loading YOLO model on rank 0...\")\n",
    "            print(f\"Data config: {data_yaml_path}\")\n",
    "            print(f\"MLflow run name: {run_name}\\n\")\n",
    "        \n",
    "        model = YOLO(model_path)\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(\"Starting distributed training...\\n\")\n",
    "            print(\"[INFO] YOLO will automatically:\")\n",
    "            print(\"   1. Detect DDP environment (RANK, WORLD_SIZE, LOCAL_RANK)\")\n",
    "            print(\"   2. Use existing PyTorch DDP process group\")\n",
    "            print(\"   3. Distribute data across all GPUs\")\n",
    "            print(\"   4. Synchronize gradients using all-reduce\")\n",
    "            print(\"   5. Update model weights consistently\\n\")\n",
    "        \n",
    "        # Train with DDP inside MLflow run context\n",
    "        with mlflow.start_run(run_name=run_name) as run:\n",
    "            # Log source notebook information (rank 0 only)\n",
    "            if rank == 0:\n",
    "                mlflow.set_tag(\"mlflow.source.name\", notebook_path)\n",
    "                mlflow.set_tag(\"mlflow.source.type\", \"NOTEBOOK\")\n",
    "                mlflow.set_tag(\"mlflow.databricks.notebookPath\", notebook_path)\n",
    "                mlflow.set_tag(\"mlflow.databricks.notebookID\", notebook_id)\n",
    "            \n",
    "            results = model.train(\n",
    "                task=\"detect\",\n",
    "                batch=batch_size_per_gpu,    # Batch size per GPU\n",
    "                device=[LOCAL_RANK],         # Use LOCAL_RANK as list (required for DDP)\n",
    "                data=data_yaml_path,\n",
    "                epochs=100,\n",
    "                project=tmp_project_location_unique,\n",
    "                name=run_name,               # Folder name for training outputs\n",
    "                exist_ok=True,\n",
    "                fliplr=1,\n",
    "                flipud=1,\n",
    "                perspective=0.001,\n",
    "                degrees=0.45,\n",
    "                amp=True,                    # Automatic Mixed Precision\n",
    "                patience=50,\n",
    "                dropout=0.2,\n",
    "                weight_decay=0.0005,\n",
    "                save=True,\n",
    "                save_period=10,\n",
    "                workers=8,                   # Data loading workers per GPU\n",
    "                close_mosaic=10\n",
    "            )\n",
    "            \n",
    "            # Validate on rank 0\n",
    "            if RANK in (0, -1):\n",
    "                success = model.val(\n",
    "                    project=tmp_project_location_unique,\n",
    "                    name=\"validation_metrics\"\n",
    "                )\n",
    "            \n",
    "            # Get run_id while still inside the context (before it closes)\n",
    "            active_run_id = run.info.run_id\n",
    "        \n",
    "        # Only rank 0 handles model registration and artifact copying\n",
    "        if rank == 0:\n",
    "            print(f\"\\n[OK] Distributed training complete! MLflow Run ID: {active_run_id}\")\n",
    "            \n",
    "            # Create full organized name with run_id\n",
    "            organized_run_name = f\"{model_task}_{model_arch}_{dataset_name}_{timestamp}_run_{active_run_id}\"\n",
    "            \n",
    "            # Add organized name as MLflow tag for easy reference\n",
    "            with mlflow.start_run(run_id=active_run_id):\n",
    "                mlflow.set_tag(\"organized_run_name\", organized_run_name)\n",
    "                mlflow.set_tag(\"training_mode\", \"distributed\")\n",
    "                mlflow.set_tag(\"num_gpus\", world_size)\n",
    "                mlflow.set_tag(\"gpu_type\", gpu_type)\n",
    "                mlflow.log_artifact(data_yaml_path, \"input_data_yaml\")\n",
    "            \n",
    "            print(f\"\\n[Distributed Training Summary]\")\n",
    "            print(f\"   GPUs used: {world_size}\")\n",
    "            print(f\"   Effective batch size: {batch_size_per_gpu * world_size}\")\n",
    "            print(f\"   Training mode: DDP (Distributed Data Parallel)\")\n",
    "            print(f\"   MLflow run name: {run_name}\")\n",
    "            print(f\"   Organized run name: {organized_run_name}\")\n",
    "            \n",
    "            # Copy training results to Unity Catalog Volume\n",
    "            print(f\"\\n[INFO] Copying training results to Unity Catalog Volume...\")\n",
    "            training_run_dir = os.path.join(tmp_project_location_unique, run_name)\n",
    "            \n",
    "            # Use organized name for volume folder\n",
    "            volume_run_dir = os.path.join(project_location, \"runs\", organized_run_name)\n",
    "            volume_train_dir = os.path.join(volume_run_dir, \"train\")\n",
    "            \n",
    "            if os.path.exists(training_run_dir):\n",
    "                shutil.copytree(training_run_dir, volume_train_dir, dirs_exist_ok=True)\n",
    "                print(f\"   [OK] Training outputs copied to: {volume_train_dir}\")\n",
    "            \n",
    "            # Copy validation metrics\n",
    "            val_metrics_dir = os.path.join(tmp_project_location_unique, \"validation_metrics\")\n",
    "            if os.path.exists(val_metrics_dir):\n",
    "                volume_val_metrics_dir = os.path.join(volume_run_dir, \"validation_metrics\")\n",
    "                shutil.copytree(val_metrics_dir, volume_val_metrics_dir, dirs_exist_ok=True)\n",
    "                print(f\"   [OK] Validation metrics copied to: {volume_val_metrics_dir}\")\n",
    "            \n",
    "            # Save best model\n",
    "            print(\"\\n[INFO] Saving best model...\")\n",
    "            best_model = YOLO(str(model.trainer.best))\n",
    "            best_model_path = f\"/tmp/best_yolo_model_{timestamp}.pt\"\n",
    "            best_model.save(best_model_path)\n",
    "            print(f\"   Saved to: {best_model_path}\")\n",
    "            \n",
    "            # Register model to Unity Catalog using model_name widget\n",
    "            registered_model_name = register_yolo_model(\n",
    "                run_id=active_run_id,\n",
    "                model_path=best_model_path,\n",
    "                catalog_name=catalog_name,\n",
    "                schema_name=schema_name,\n",
    "                model_name=model_name,  # Use widget parameter\n",
    "                signature=signature,\n",
    "                input_example=input_example,\n",
    "                data_yaml_path=data_yaml_path\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n\" + \"=\" * 60)\n",
    "            print(\"[OK] DISTRIBUTED TRAINING COMPLETE\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"\\n[Model Details]\")\n",
    "            print(f\"   - Name: {registered_model_name}\")\n",
    "            print(f\"   - Run ID: {active_run_id}\")\n",
    "            print(f\"   - Location: Unity Catalog Model Registry\")\n",
    "            print(f\"   - Format: Custom YOLO wrapper (base64 input, bbox output)\")\n",
    "            print(f\"   - Training: Distributed across {world_size} {gpu_type} GPUs\")\n",
    "            print(f\"\\n[Training Artifacts]\")\n",
    "            print(f\"   - Volume location: {volume_run_dir}\")\n",
    "            print(f\"   - Run name: {organized_run_name}\")\n",
    "            print(f\"   - Structure: train/, validation_metrics/\")\n",
    "            print(f\"\\n[View Results]\")\n",
    "            print(f\"   {mlflow.get_tracking_uri()}/#/experiments/{experiment_id}/runs/{active_run_id}\")\n",
    "            \n",
    "            # Return values for notebook access\n",
    "            return active_run_id, registered_model_name, organized_run_name, volume_run_dir\n",
    "        \n",
    "        return None, None, None, None\n",
    "    \n",
    "    finally:\n",
    "        # Clean up process group\n",
    "        if dist.is_initialized():\n",
    "            dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d079bf6-b53d-47da-9c0e-71745950d019",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initiating Serverless GPU Distributed Training Process"
    }
   },
   "outputs": [],
   "source": [
    "# Launch distributed training\n",
    "print(\"[INFO] Launching distributed training on Serverless GPU...\")\n",
    "print(f\"   Databricks will provision {num_gpus} {gpu_type} GPUs automatically\")\n",
    "print(\"   This may take a few minutes to start...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d142d022-f643-4279-ac01-37626f5ba276",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Launch & Extract Training Artifacts from Distributed Training Ru ..."
    }
   },
   "outputs": [],
   "source": [
    "result = run_distributed_training.distributed()\n",
    "\n",
    "# Extract results from rank 0 (first element of the list)\n",
    "if result and result[0]:\n",
    "    run_id, registered_model_name, organized_run_name, volume_run_dir = result[0]\n",
    "    print(f\"\\n[OK] Training artifacts available in notebook scope\")\n",
    "    print(f\"   Variables: run_id, registered_model_name, organized_run_name, volume_run_dir\")\n",
    "else:\n",
    "    print(f\"\\n[INFO] Training complete. Check MLflow for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec50836-ecdb-4031-81d3-2305ebb46677",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check result"
    }
   },
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b6f0ea-f73e-4c7e-b5fb-9d1473625329",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Testing Approaches Explained"
    }
   },
   "source": [
    "## Model Evaluations\n",
    "\n",
    "**Split Evaluation (Native YOLO):** Assess model accuracy using file paths from UC Volume. Validates model quality on validation/test sets.\n",
    "\n",
    "**Local Serving Test (MLflow PyFunc):** Validate production serving format using base64-encoded images. Ensures endpoint compatibility before deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b3d861-04cc-4af8-9b22-ca471e39d9fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Evaluation"
    }
   },
   "source": [
    "### Split Evaluation\n",
    "\n",
    "Evaluate model performance on validation and test sets before deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "284285d3-49a1-4ba8-8a0d-90152e8f8607",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validation Set Evaluation"
    }
   },
   "outputs": [],
   "source": [
    "# Load model from MLflow and evaluate on validation set\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model_path = mlflow.artifacts.download_artifacts(model_uri)\n",
    "\n",
    "# Find the .pt file\n",
    "import glob as glob_module\n",
    "pt_files = glob_module.glob(f\"{model_path}/**/*.pt\", recursive=True)\n",
    "if pt_files:\n",
    "    loaded_model = YOLO(pt_files[0], task='detect')\n",
    "    print(f\"[OK] Model loaded from MLflow\\n\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_image_dir = f\"{project_location}data/coco128/images/val\"\n",
    "    val_output_dir = os.path.join(volume_run_dir, \"validation_samples\")\n",
    "    \n",
    "    evaluate_model_on_split(\n",
    "        model=loaded_model,\n",
    "        image_dir=val_image_dir,\n",
    "        split_name=\"validation\",\n",
    "        output_dir=val_output_dir,\n",
    "        run_id=run_id,\n",
    "        registered_model_name=registered_model_name,\n",
    "        organized_run_name=organized_run_name,\n",
    "        num_samples=3\n",
    "    )\n",
    "else:\n",
    "    print(\"[ERROR] Model file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0b8c04-a2c0-4c31-b958-53f73cbcf5ce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test Set Evaluation"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate model on test set (uses loaded_model from validation cell)\n",
    "test_image_dir = f\"{project_location}data/coco128/images/test\"\n",
    "test_output_dir = os.path.join(volume_run_dir, \"test_samples\")\n",
    "\n",
    "evaluate_model_on_split(\n",
    "    model=loaded_model,\n",
    "    image_dir=test_image_dir,\n",
    "    split_name=\"test\",\n",
    "    output_dir=test_output_dir,\n",
    "    run_id=run_id,\n",
    "    registered_model_name=registered_model_name,\n",
    "    organized_run_name=organized_run_name,\n",
    "    num_samples=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f9babf3-e088-4503-be8d-6a59ed592e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### (Registered Model) Local Serving Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b672ff52-f7bb-4874-a732-2ce1ba237aa0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test Registered Model (Local)"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# Test registered model locally with base64 input (serving format)\n",
    "print(\"=\" * 60)\n",
    "print(\"LOCAL MODEL TEST - BASE64 INPUT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Ensure we have the registered model name\n",
    "if 'registered_model_name' not in dir():\n",
    "    registered_model_name = f\"{catalog_name}.{schema_name}.yolo11n_coco128_sgc_distributed\"\n",
    "\n",
    "print(f\"\\nTesting model: {registered_model_name}\\n\")\n",
    "\n",
    "try:\n",
    "    # Get latest model version\n",
    "    model_versions = client.search_model_versions(f\"name='{registered_model_name}'\")\n",
    "    \n",
    "    if model_versions:\n",
    "        latest_version = model_versions[0].version\n",
    "        print(f\"[OK] Found model version: {latest_version}\")\n",
    "        print(f\"   Status: {model_versions[0].status}\")\n",
    "        \n",
    "        # Load model using pyfunc (this is what serving endpoint uses)\n",
    "        model_uri = f\"models:/{registered_model_name}/{latest_version}\"\n",
    "        serving_model = mlflow.pyfunc.load_model(model_uri)\n",
    "        print(f\"[OK] MLflow pyfunc model loaded successfully\\n\")\n",
    "        \n",
    "        # Get test images (skip first 3 used in test_samples evaluation)\n",
    "        test_images = glob.glob(f\"{project_location}data/coco128/images/test/*.jpg\")\n",
    "        if test_images:\n",
    "            # Use images 5-7 (different from test_samples which uses 1-3)\n",
    "            num_samples = min(3, len(test_images) - 3)\n",
    "            sample_images = test_images[10:10+num_samples]  # Skip first 3 test images \n",
    "            # sample_images = test_images[6:6+num_samples]  # \n",
    "            \n",
    "            print(f\"Testing with {num_samples} sample images (different from test_samples)\\n\")\n",
    "            \n",
    "            # Create color map for different classes\n",
    "            colors = plt.cm.tab20(np.linspace(0, 1, 20))  # 20 distinct colors\n",
    "            \n",
    "            # Create visualization\n",
    "            fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
    "            if num_samples == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i, test_image_path in enumerate(sample_images):\n",
    "                print(f\"Sample {i+1}/{num_samples}: {test_image_path.split('/')[-1]}\")\n",
    "                \n",
    "                # Encode image as base64\n",
    "                with open(test_image_path, 'rb') as f:\n",
    "                    image_bytes = f.read()\n",
    "                image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "                \n",
    "                # Test pyfunc wrapper with base64 input\n",
    "                input_df = pd.DataFrame({\"image_base64\": [image_base64]})\n",
    "                predictions = serving_model.predict(input_df)\n",
    "                \n",
    "                # Load and display image\n",
    "                image = Image.open(test_image_path)\n",
    "                axes[i].imshow(image)\n",
    "                axes[i].axis('off')\n",
    "                \n",
    "                # Draw bounding boxes from pyfunc predictions\n",
    "                if len(predictions) > 0:\n",
    "                    num_detections = len(predictions)\n",
    "                    \n",
    "                    # Draw each bounding box with class-specific color\n",
    "                    for idx, row in predictions.iterrows():\n",
    "                        # Use xyxy coordinates\n",
    "                        x1, y1, x2, y2 = row['bbox_x1'], row['bbox_y1'], row['bbox_x2'], row['bbox_y2']\n",
    "                        width = x2 - x1\n",
    "                        height = y2 - y1\n",
    "                        \n",
    "                        # Get color based on class number\n",
    "                        color = colors[int(row['class_num']) % len(colors)]\n",
    "                        \n",
    "                        # Draw rectangle\n",
    "                        rect = patches.Rectangle(\n",
    "                            (x1, y1), width, height,\n",
    "                            linewidth=2, edgecolor=color, facecolor='none'\n",
    "                        )\n",
    "                        axes[i].add_patch(rect)\n",
    "                        \n",
    "                        # Add label with matching color\n",
    "                        label = f\"{row['class_name']} {row['confidence']:.2f}\"\n",
    "                        axes[i].text(\n",
    "                            x1, y1 - 5, label,\n",
    "                            color='white', fontsize=8,\n",
    "                            bbox=dict(facecolor=color, alpha=0.8, pad=2)\n",
    "                        )\n",
    "                    \n",
    "                    axes[i].set_title(f\"{test_image_path.split('/')[-1]}\\n{num_detections} objects\", fontsize=10)\n",
    "                    print(f\"   [OK] Detections: {num_detections} objects\")\n",
    "                    \n",
    "                    for idx, row in predictions.head(3).iterrows():\n",
    "                        print(f\"      - {row['class_name']}: {row['confidence']:.3f}\")\n",
    "                else:\n",
    "                    axes[i].set_title(f\"{test_image_path.split('/')[-1]}\\nNo objects\", fontsize=10)\n",
    "                    print(f\"   [OK] No objects detected\")\n",
    "                print()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f\"Local Serving Test - MLflow PyFunc with Base64 (v{latest_version})\", fontsize=14, y=1.02)\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"[OK] MODEL READY FOR DEPLOYMENT\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\n[Test Summary]\")\n",
    "        print(f\"   - Model: {registered_model_name} (v{latest_version})\")\n",
    "        print(f\"   - Input format: Base64-encoded images ✓\")\n",
    "        print(f\"   - MLflow pyfunc wrapper: ✓\")\n",
    "        print(f\"   - Bounding boxes: ✓ (color-coded by class)\")\n",
    "        print(f\"   - Test images: Different from test_samples evaluation\")\n",
    "        print(f\"   - Status: Validated and ready\")\n",
    "        print(f\"\\n[Key Difference from Split Tests]\")\n",
    "        print(f\"   - Split tests: Native YOLO + file paths\")\n",
    "        print(f\"   - This test: MLflow pyfunc wrapper + base64\")\n",
    "        print(f\"   - This test validates actual serving endpoint format\")\n",
    "        print(f\"\\n   Next: Deploy to serving endpoint\")\n",
    "    else:\n",
    "        print(f\"[ERROR] No versions found for: {registered_model_name}\")\n",
    "        print(f\"\\nPlease register the model first.\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95019a81-1b93-446a-8536-008be1c8b4d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## UC Registered Model Deployment CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b50acd-ffbc-4acb-95ac-e11ca5f56d05",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "⚠️ Deployment Checkpoint - Manual Confirmation Required"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEPLOYMENT CHECKPOINT\n",
    "# ============================================================\n",
    "# This cell acts as a safety gate before deployment cells.\n",
    "# Set the 'Proceed with Deployment' widget to 'true' to continue.\n",
    "# ============================================================\n",
    "\n",
    "# Get deployment approval from widget (set in cell 10)\n",
    "PROCEED_WITH_DEPLOYMENT = dbutils.widgets.get(\"proceed_with_deployment\") == \"true\"\n",
    "\n",
    "if not PROCEED_WITH_DEPLOYMENT:\n",
    "    message = \"\"\"\n",
    "============================================================\n",
    "⚠️  DEPLOYMENT PAUSED - MANUAL CONFIRMATION REQUIRED\n",
    "============================================================\n",
    "\n",
    "This checkpoint prevents accidental execution of deployment cells.\n",
    "\n",
    "[To Proceed]\n",
    "   1. Review the model validation results above\n",
    "   2. Verify the model is ready for deployment\n",
    "   3. Set 'Proceed with Deployment' widget to 'true' (top of notebook)\n",
    "   4. Re-run this cell\n",
    "\n",
    "[What Happens Next]\n",
    "   - Cell 40: Create/update serving endpoint (AI Gateway enabled automatically)\n",
    "   - Cell 42: Test deployed endpoint\n",
    "\n",
    "[Safety Note]\n",
    "   This checkpoint ensures you don't accidentally deploy\n",
    "   an unvalidated model or overwrite a production endpoint.\n",
    "\n",
    "[For 'Run All']\n",
    "   Deployment cells will skip execution if not approved.\n",
    "   No errors will be raised.\n",
    "\n",
    "============================================================\n",
    "⏸️  DEPLOYMENT PAUSED - AWAITING APPROVAL\n",
    "============================================================\n",
    "\"\"\"\n",
    "    dbutils.notebook.exit(message)\n",
    "else:\n",
    "    message = \"\"\"\n",
    "============================================================\n",
    "✓ DEPLOYMENT CHECKPOINT PASSED\n",
    "============================================================\n",
    "\n",
    "[Confirmation]\n",
    "   User has manually approved deployment\n",
    "   Execution will stop here for manual control\n",
    "\n",
    "[Next Steps - Run Manually]\n",
    "   1. Run cell 40: Create/update serving endpoint (AI Gateway enabled automatically)\n",
    "   2. Wait for endpoint to be ready (10-20 minutes)\n",
    "   3. Run cell 42: Test deployed endpoint\n",
    "\n",
    "[Why Manual Execution?]\n",
    "   - Endpoint provisioning takes 10-20 minutes\n",
    "   - You can monitor progress in the UI\n",
    "   - Each step requires verification before proceeding\n",
    "   - Prevents accidental 'Run All' through deployment\n",
    "\n",
    "============================================================\n",
    "⏸️  STOPPING HERE - RUN DEPLOYMENT CELLS MANUALLY\n",
    "============================================================\n",
    "\"\"\"\n",
    "    dbutils.notebook.exit(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49600690-8757-4f30-a2ba-d1014633c3cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Deployment"
    }
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "Deploy model to Databricks Model Serving endpoint with AI Gateway and inference table logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b4c6a64-aeee-4f4b-9d09-ca5a2932bf41",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Model Serving Endpoint"
    }
   },
   "outputs": [],
   "source": [
    "# Check deployment approval\n",
    "if 'PROCEED_WITH_DEPLOYMENT' not in dir() or not PROCEED_WITH_DEPLOYMENT:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"⚠️  DEPLOYMENT SKIPPED - NOT APPROVED\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n[Reason]\")\n",
    "    print(\"   PROCEED_WITH_DEPLOYMENT flag is not set to True\")\n",
    "    print(\"\\n[To Enable Deployment]\")\n",
    "    print(\"   1. Set 'Proceed with Deployment' widget to 'true' (top of notebook)\")\n",
    "    print(\"   2. Re-run deployment checkpoint cell and this cell\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    # Deployment approved - proceed with endpoint creation\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    from databricks.sdk.service.serving import (\n",
    "        ServedEntityInput, \n",
    "        EndpointCoreConfigInput,\n",
    "        AiGatewayConfig,\n",
    "        AiGatewayInferenceTableConfig\n",
    "    )\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    import time\n",
    "\n",
    "    w = WorkspaceClient()\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # Get latest model version\n",
    "    if 'registered_model_name' not in dir():\n",
    "        registered_model_name = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "\n",
    "    model_versions = client.search_model_versions(f\"name='{registered_model_name}'\")\n",
    "    if not model_versions:\n",
    "        raise ValueError(f\"No model versions found for {registered_model_name}. Run training cell to register the model.\")\n",
    "\n",
    "    model_version = model_versions[0].version\n",
    "\n",
    "    # Derive endpoint name from model name\n",
    "    model_name_only = registered_model_name.split('.')[-1]\n",
    "    endpoint_name = f\"{model_name_only}_endpoint\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CREATING MODEL SERVING ENDPOINT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\nEndpoint configuration:\")\n",
    "    print(f\"   Name: {endpoint_name}\")\n",
    "    print(f\"   Model: {registered_model_name}\")\n",
    "    print(f\"   Version: {model_version}\")\n",
    "    print(f\"   Workload size: Small\")\n",
    "    print(f\"   Scale to zero: Enabled\")\n",
    "    print(f\"   AI Gateway: Enabled with inference tables\")\n",
    "    print(f\"   Inference table: {catalog_name}.{schema_name}.{endpoint_name}_payload\")\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        # Check if endpoint already exists\n",
    "        endpoint_exists = False\n",
    "        needs_update = True\n",
    "        needs_ai_gateway_update = False\n",
    "        \n",
    "        try:\n",
    "            existing_endpoint = w.serving_endpoints.get(endpoint_name)\n",
    "            endpoint_exists = True\n",
    "            print(f\"[INFO] Endpoint '{endpoint_name}' already exists\")\n",
    "            \n",
    "            # Check if endpoint is currently being updated\n",
    "            if existing_endpoint.state.config_update.value != \"NOT_UPDATING\":\n",
    "                print(f\"[INFO] Endpoint is currently being updated (status: {existing_endpoint.state.config_update.value})\")\n",
    "                print(f\"   Checking status briefly (will timeout after 2 minutes)...\\n\")\n",
    "                \n",
    "                # Brief check for current update status\n",
    "                max_wait_time = 120  # Only wait 2 minutes\n",
    "                poll_interval = 10\n",
    "                elapsed_time = 0\n",
    "                \n",
    "                while elapsed_time < max_wait_time:\n",
    "                    endpoint = w.serving_endpoints.get(endpoint_name)\n",
    "                    \n",
    "                    if endpoint.state.config_update.value == \"NOT_UPDATING\":\n",
    "                        print(f\"\\n[OK] Current update completed (took {elapsed_time}s)\")\n",
    "                        existing_endpoint = endpoint\n",
    "                        break\n",
    "                    elif endpoint.state.config_update.value == \"UPDATE_FAILED\":\n",
    "                        print(f\"\\n[WARNING] Current update failed\")\n",
    "                        existing_endpoint = endpoint\n",
    "                        break\n",
    "                    else:\n",
    "                        if elapsed_time % 30 == 0:\n",
    "                            print(f\"   Status: {existing_endpoint.state.config_update.value} ({elapsed_time}s elapsed)\")\n",
    "                        time.sleep(poll_interval)\n",
    "                        elapsed_time += poll_interval\n",
    "                \n",
    "                if elapsed_time >= max_wait_time:\n",
    "                    print(f\"\\n[INFO] Endpoint update still in progress after {max_wait_time}s\")\n",
    "                    print(f\"   This cell will complete now to avoid blocking\")\n",
    "                    print(f\"\\n[NEXT STEP]\")\n",
    "                    print(f\"   1. Wait a few minutes for the update to complete\")\n",
    "                    print(f\"   2. Re-run this cell to check status\")\n",
    "                    print(f\"   3. Once ready, proceed to testing\")\n",
    "                    \n",
    "                    # Get final status and exit\n",
    "                    endpoint = w.serving_endpoints.get(endpoint_name)\n",
    "                    workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "                    endpoint_url = f\"https://{workspace_url}/ml/endpoints/{endpoint_name}\"\n",
    "                    \n",
    "                    print(f\"\\n[View Endpoint]\")\n",
    "                    print(f\"   {endpoint_url}\")\n",
    "                    print(f\"\\n[Current Status]\")\n",
    "                    print(f\"   - Config State: {endpoint.state.config_update.value}\")\n",
    "                    print(f\"   - Ready State: {endpoint.state.ready.value}\")\n",
    "                    \n",
    "                    # Exit early - don't proceed to update logic\n",
    "                    raise SystemExit(\"Endpoint update in progress - cell completed to avoid blocking\")\n",
    "            \n",
    "            # Check if it's already serving the same model version\n",
    "            current_config = existing_endpoint.config\n",
    "            if current_config and current_config.served_entities:\n",
    "                current_entity = current_config.served_entities[0]\n",
    "                current_model = current_entity.entity_name\n",
    "                current_version = current_entity.entity_version\n",
    "                \n",
    "                if current_model == registered_model_name and current_version == str(model_version):\n",
    "                    print(f\"   Already serving {registered_model_name} version {model_version}\")\n",
    "                    print(f\"   No model update needed\")\n",
    "                    needs_update = False\n",
    "                    \n",
    "                    # Check if AI Gateway inference tables are enabled\n",
    "                    ai_gateway = existing_endpoint.ai_gateway\n",
    "                    if ai_gateway and ai_gateway.inference_table_config and ai_gateway.inference_table_config.enabled:\n",
    "                        print(f\"   AI Gateway inference tables already enabled\")\n",
    "                        print(f\"   No AI Gateway update needed\\n\")\n",
    "                    else:\n",
    "                        print(f\"   AI Gateway inference tables not enabled\")\n",
    "                        print(f\"   Will enable AI Gateway\\n\")\n",
    "                        needs_ai_gateway_update = True\n",
    "                else:\n",
    "                    print(f\"   Currently serving: {current_model} v{current_version}\")\n",
    "                    print(f\"   Updating to: {registered_model_name} v{model_version}\")\n",
    "                    print(f\"   Note: AI Gateway must be configured separately for updates\\n\")\n",
    "                    needs_ai_gateway_update = True\n",
    "        except Exception as e:\n",
    "            if \"does not exist\" in str(e).lower() or \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "                print(f\"[INFO] Endpoint '{endpoint_name}' does not exist\")\n",
    "                print(f\"   Creating new endpoint with AI Gateway enabled...\\n\")\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # Create/update endpoint if needed\n",
    "        if needs_update:\n",
    "            if endpoint_exists:\n",
    "                # Update existing endpoint using SDK method\n",
    "                # Note: update_config() doesn't support ai_gateway parameter\n",
    "                w.serving_endpoints.update_config(\n",
    "                    name=endpoint_name,\n",
    "                    served_entities=[\n",
    "                        ServedEntityInput(\n",
    "                            entity_name=registered_model_name,\n",
    "                            entity_version=str(model_version),\n",
    "                            workload_size=\"Small\",\n",
    "                            scale_to_zero_enabled=True\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "                print(f\"[OK] Endpoint update submitted\")\n",
    "            else:\n",
    "                # Create new endpoint with AI Gateway enabled using SDK method\n",
    "                w.serving_endpoints.create(\n",
    "                    name=endpoint_name,\n",
    "                    config=EndpointCoreConfigInput(\n",
    "                        served_entities=[\n",
    "                            ServedEntityInput(\n",
    "                                entity_name=registered_model_name,\n",
    "                                entity_version=str(model_version),\n",
    "                                workload_size=\"Small\",\n",
    "                                scale_to_zero_enabled=True\n",
    "                            )\n",
    "                        ]\n",
    "                    ),\n",
    "                    ai_gateway=AiGatewayConfig(\n",
    "                        inference_table_config=AiGatewayInferenceTableConfig(\n",
    "                            catalog_name=catalog_name,\n",
    "                            schema_name=schema_name,\n",
    "                            table_name_prefix=endpoint_name,\n",
    "                            enabled=True\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                print(f\"[OK] Endpoint creation submitted (with AI Gateway enabled)\")\n",
    "            \n",
    "            # Brief initial wait with shorter timeout to avoid stuck state\n",
    "            print(f\"\\n[INFO] Checking initial status (endpoint provisioning may take 10-20+ minutes)...\")\n",
    "            max_wait_time = 120  # Only wait 2 minutes here\n",
    "            poll_interval = 10   # Check every 10 seconds\n",
    "            elapsed_time = 0\n",
    "            \n",
    "            while elapsed_time < max_wait_time:\n",
    "                endpoint = w.serving_endpoints.get(endpoint_name)\n",
    "                \n",
    "                if endpoint.state.config_update.value == \"NOT_UPDATING\" and endpoint.state.ready.value == \"READY\":\n",
    "                    print(f\"\\n[OK] Endpoint is ready! (took {elapsed_time}s)\")\n",
    "                    break\n",
    "                elif endpoint.state.config_update.value == \"UPDATE_FAILED\":\n",
    "                    print(f\"\\n[ERROR] Endpoint update failed!\")\n",
    "                    print(f\"   Check the endpoint UI for error details\")\n",
    "                    break\n",
    "                else:\n",
    "                    if elapsed_time % 30 == 0:  # Print status every 30 seconds\n",
    "                        print(f\"   Status: {endpoint.state.config_update.value} ({elapsed_time}s elapsed)\")\n",
    "                    time.sleep(poll_interval)\n",
    "                    elapsed_time += poll_interval\n",
    "            \n",
    "            if elapsed_time >= max_wait_time:\n",
    "                print(f\"\\n[INFO] Endpoint is still initializing (this may take several more minutes)\")\n",
    "                print(f\"   This cell will complete now to avoid blocking\")\n",
    "                print(f\"\\n[NEXT STEP]\")\n",
    "                print(f\"   1. Wait for endpoint to finish provisioning (check UI)\")\n",
    "                print(f\"   2. Re-run this cell to verify status\")\n",
    "                print(f\"   3. Once ready, proceed to testing\")\n",
    "        \n",
    "        # Enable AI Gateway if needed (for existing endpoints that were updated)\n",
    "        if needs_ai_gateway_update and endpoint_exists:\n",
    "            print(f\"\\n[INFO] Enabling AI Gateway inference tables...\")\n",
    "            \n",
    "            # First verify endpoint is ready\n",
    "            endpoint = w.serving_endpoints.get(endpoint_name)\n",
    "            if endpoint.state.ready.value != \"READY\":\n",
    "                print(f\"[WARNING] Endpoint not ready yet (status: {endpoint.state.ready.value})\")\n",
    "                print(f\"   AI Gateway will be configured in the next cell once endpoint is ready\")\n",
    "            else:\n",
    "                # Enable AI Gateway (table will be created automatically by AI Gateway)\n",
    "                w.serving_endpoints.put_ai_gateway(\n",
    "                    name=endpoint_name,\n",
    "                    inference_table_config=AiGatewayInferenceTableConfig(\n",
    "                        catalog_name=catalog_name,\n",
    "                        schema_name=schema_name,\n",
    "                        table_name_prefix=endpoint_name,\n",
    "                        enabled=True\n",
    "                    )\n",
    "                )\n",
    "                print(f\"[OK] AI Gateway configuration submitted\")\n",
    "                \n",
    "                # Brief wait for configuration\n",
    "                time.sleep(5)\n",
    "                max_wait = 60\n",
    "                elapsed = 0\n",
    "                while elapsed < max_wait:\n",
    "                    endpoint = w.serving_endpoints.get(endpoint_name)\n",
    "                    if endpoint.state.config_update.value == \"NOT_UPDATING\":\n",
    "                        break\n",
    "                    time.sleep(5)\n",
    "                    elapsed += 5\n",
    "        \n",
    "        # Get final status\n",
    "        endpoint = w.serving_endpoints.get(endpoint_name)\n",
    "        \n",
    "        # Get workspace URL for link\n",
    "        workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "        endpoint_url = f\"https://{workspace_url}/ml/endpoints/{endpoint_name}\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        if endpoint.state.config_update.value == \"NOT_UPDATING\" and endpoint.state.ready.value == \"READY\":\n",
    "            print(\"[OK] SERVING ENDPOINT READY\")\n",
    "        else:\n",
    "            print(\"[INFO] SERVING ENDPOINT INITIALIZING\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\n[Endpoint Details]\")\n",
    "        print(f\"   - Name: {endpoint_name}\")\n",
    "        print(f\"   - Model: {registered_model_name} (v{model_version})\")\n",
    "        print(f\"   - Config State: {endpoint.state.config_update.value}\")\n",
    "        print(f\"   - Ready State: {endpoint.state.ready.value}\")\n",
    "        \n",
    "        # Check AI Gateway status\n",
    "        if endpoint.ai_gateway and endpoint.ai_gateway.inference_table_config:\n",
    "            ai_config = endpoint.ai_gateway.inference_table_config\n",
    "            if ai_config.enabled:\n",
    "                print(f\"   - AI Gateway: Enabled\")\n",
    "                print(f\"   - Inference Table: {ai_config.catalog_name}.{ai_config.schema_name}.{ai_config.table_name_prefix}_payload\")\n",
    "            else:\n",
    "                print(f\"   - AI Gateway: Disabled\")\n",
    "        else:\n",
    "            print(f\"   - AI Gateway: Not configured\")\n",
    "        \n",
    "        print(f\"\\n[View Endpoint]\")\n",
    "        print(f\"   {endpoint_url}\")\n",
    "        \n",
    "        if endpoint.state.config_update.value == \"NOT_UPDATING\" and endpoint.state.ready.value == \"READY\":\n",
    "            if endpoint.ai_gateway and endpoint.ai_gateway.inference_table_config and endpoint.ai_gateway.inference_table_config.enabled:\n",
    "                print(f\"\\n[Next Step]\")\n",
    "                print(f\"   Skip to 'Test Deployed Endpoint' cell\")\n",
    "            else:\n",
    "                print(f\"\\n[Next Step]\")\n",
    "                print(f\"   Run 'Enable AI Gateway Inference Tables' cell\")\n",
    "        else:\n",
    "            print(f\"\\n[Next Step]\")\n",
    "            print(f\"   Wait for endpoint to be ready, then re-run this cell\")\n",
    "        \n",
    "    except SystemExit as e:\n",
    "        # Clean exit when endpoint is still updating\n",
    "        print(f\"\\n[INFO] Cell completed (endpoint update in progress)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to create/update endpoint: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9960b23c-6837-405c-b4fd-089f5cf33be3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "AI Gateway Inference Tables - Important Notes"
    }
   },
   "source": [
    "## AI Gateway Inference Tables - Important Notes\n",
    "\n",
    "**AI Gateway is configured automatically** when creating new endpoints (cell 40). For existing endpoints being updated, AI Gateway is enabled separately after the model update.\n",
    "\n",
    "**Key behaviors:**\n",
    "\n",
    "1. **Table Creation**: AI Gateway creates the inference table automatically AFTER the first request is made to the endpoint, not when AI Gateway is configured. The table structure is created immediately, but remains empty until requests are logged.\n",
    "\n",
    "2. **Logging Delay**: There is typically a delay (usually 2-5 minutes) between when an inference request is made and when the request/response data appears in the payload table. This is normal behavior - the data is being processed and written asynchronously.\n",
    "\n",
    "3. **Verification**: After running the test endpoint cell below, wait a few minutes then query the table to see logged requests:\n",
    "   ```sql\n",
    "   SELECT * FROM `{catalog_name}`.`{schema_name}`.`{endpoint_name}_payload`\n",
    "   ORDER BY timestamp_ms DESC LIMIT 10\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e5ab396-8301-4133-9b1e-1af2c52e67d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test Deployed Endpoint"
    }
   },
   "outputs": [],
   "source": [
    "# Check deployment approval \n",
    "if 'PROCEED_WITH_DEPLOYMENT' not in dir() or not PROCEED_WITH_DEPLOYMENT:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"⚠️  DEPLOYMENT SKIPPED - NOT APPROVED\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n[Reason]\")\n",
    "    print(\"   PROCEED_WITH_DEPLOYMENT flag is not set to True\")\n",
    "    print(\"\\n[To Enable Deployment]\")\n",
    "    print(\"   1. Set 'Proceed with Deployment' widget to 'true' (top of notebook)\")\n",
    "    print(\"   2. Re-run cell 38 (deployment checkpoint)\")\n",
    "    print(\"   3. Then run deployment cells 40 and 42\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    # Deployment approved - proceed with endpoint testing\n",
    "    import json\n",
    "    import base64\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING DEPLOYED ENDPOINT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\nEndpoint: {endpoint_name}\\n\")\n",
    "\n",
    "    try:\n",
    "        test_images = glob.glob(f\"{project_location}data/coco128/images/test/*.jpg\")\n",
    "        \n",
    "        if test_images:\n",
    "            test_image_path = test_images[-3]\n",
    "            print(f\"Test image: {test_image_path.split('/')[-1]}\")\n",
    "            \n",
    "            # Encode image as base64\n",
    "            print(f\"[INFO] Encoding image as base64...\")\n",
    "            with open(test_image_path, 'rb') as f:\n",
    "                image_bytes = f.read()\n",
    "            image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "            \n",
    "            # Test endpoint with base64 input\n",
    "            print(f\"[INFO] Testing endpoint with base64 input...\\n\")\n",
    "            input_data = {\"dataframe_records\": [{\"image_base64\": image_base64}]}\n",
    "            \n",
    "            response = w.serving_endpoints.query(\n",
    "                name=endpoint_name,\n",
    "                dataframe_records=input_data[\"dataframe_records\"]\n",
    "            )\n",
    "            \n",
    "            print(f\"[OK] Endpoint test successful!\\n\")\n",
    "            print(f\"Response preview:\")\n",
    "            response_dict = response.as_dict()\n",
    "            print(json.dumps(response_dict, indent=2)[:500])\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"[OK] DEPLOYMENT COMPLETE\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            print(f\"\\n[Deployment Summary]\")\n",
    "            print(f\"   - Endpoint: {endpoint_name}\")\n",
    "            print(f\"   - Model: {registered_model_name} (v{model_version})\")\n",
    "            print(f\"   - Status: Ready and tested\")\n",
    "            print(f\"   - AI Gateway: Enabled\")\n",
    "            print(f\"   - Input format: Base64-encoded images\")\n",
    "            print(f\"   - Inference table: {catalog_name}.{schema_name}.{endpoint_name}_payload\")\n",
    "            \n",
    "            # Get workspace URL for links\n",
    "            workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "            endpoint_url = f\"https://{workspace_url}/ml/endpoints/{endpoint_name}\"\n",
    "            table_url = f\"https://{workspace_url}/explore/data/{catalog_name}/{schema_name}/{endpoint_name}_payload\"\n",
    "            \n",
    "            print(f\"\\n[Links]\")\n",
    "            print(f\"   - Endpoint: {endpoint_url}\")\n",
    "            print(f\"   - Inference table: {table_url}\")\n",
    "            \n",
    "            print(f\"\\n[Usage Example]\")\n",
    "            print(f\"   import base64\")\n",
    "            print(f\"   with open('image.jpg', 'rb') as f:\")\n",
    "            print(f\"       img_b64 = base64.b64encode(f.read()).decode('utf-8')\")\n",
    "            print(f\"   \")\n",
    "            print(f\"   w.serving_endpoints.query(\")\n",
    "            print(f\"       name='{endpoint_name}',\")\n",
    "            print(f\"       dataframe_records=[{{'image_base64': img_b64}}]\")\n",
    "            print(f\"   )\")\n",
    "            \n",
    "            print(f\"\\n[Monitor Inference]\")\n",
    "            print(f\"   SELECT * FROM {catalog_name}.{schema_name}.{endpoint_name}_payload\")\n",
    "            print(f\"   ORDER BY timestamp_ms DESC LIMIT 10\")\n",
    "            \n",
    "        else:\n",
    "            print(\"[WARNING] No test images found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Endpoint test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"\\n[INFO] Verify endpoint is ready and AI Gateway is configured\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "databricks_ai_v4",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2221573099134134,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "sgc-yolo11n-detect-coco128-multiA10",
   "widgets": {
    "catalog_name": {
     "currentValue": "main",
     "nuid": "4ec9ba90-c989-45dd-bab7-83408fd14b88",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main",
      "label": "Catalog Name",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "main",
      "label": "Catalog Name",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "model_name": {
     "currentValue": "yolo11n_coco128_sgc_distributed",
     "nuid": "57047854-13f6-40ce-8203-8fc606e6c7d6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "yolo11n_coco128_sgc_distributed",
      "label": "Model Name",
      "name": "model_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "yolo11n_coco128_sgc_distributed",
      "label": "Model Name",
      "name": "model_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "proceed_with_deployment": {
     "currentValue": "false",
     "nuid": "b83279e7-9720-414e-a8cc-5d91cf0965dc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Proceed with Deployment",
      "name": "proceed_with_deployment",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "false",
        "true"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": "Proceed with Deployment",
      "name": "proceed_with_deployment",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "false",
        "true"
       ]
      }
     }
    },
    "schema_name": {
     "currentValue": "sgc-nightly",
     "nuid": "c331f3ef-a112-40a4-ad5f-a3eaabd45a09",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sgc-nightly",
      "label": "Schema Name",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sgc-nightly",
      "label": "Schema Name",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "volume_name": {
     "currentValue": "yolo_sgc_distributed",
     "nuid": "03e7f4dc-0691-4c47-83c8-510b43d6edaa",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "yolo_sgc_distributed",
      "label": "Volume Name",
      "name": "volume_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "yolo_sgc_distributed",
      "label": "Volume Name",
      "name": "volume_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
