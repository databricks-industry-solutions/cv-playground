{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dca7979-9176-49ed-8ada-dfbb4eb2274f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "An example for distributing the Pytorch training with  Hyperparameter tuning optimizations using [Optuna](https://optuna.org/)\n",
    "\n",
    "The cluster used is the same as before -- you are welcome to test other cluster configs. \n",
    "\n",
    "```\n",
    "\"spark_version\": \"16.4.x-scala2.13\",\n",
    "\"node_type_id\": \"g5.12xlarge\",\n",
    "\"autoscale\": {\n",
    "    \"min_workers\": 2, ## you can fix this instead\n",
    "    \"max_workers\": 4\n",
    "    }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7189f5-7e13-4941-bcb5-bba786d41083",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "install dependencies"
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow[skinny]>=3 optuna nvidia-ml-py3 --upgrade\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70dc111c-3fa7-48c7-b9d2-55707c5e35b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "for reference"
    }
   },
   "outputs": [],
   "source": [
    "# MLflow version: 3.4.0\n",
    "# Optuna version: 4.5.0\n",
    "# PyTorch version: 2.6.0+cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71fb8064-e02c-463b-89f7-db7ebf3df9c2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "import dependencies"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import shutil\n",
    "import tempfile\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import mlflow\n",
    "import optuna\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "from streaming import StreamingDataset, StreamingDataLoader\n",
    "from streaming.base.util import clean_stale_shared_memory\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe9cf74-a27c-45f6-a8a8-6c87bab234d2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configs"
    }
   },
   "outputs": [],
   "source": [
    "# Unity Catalog Configuration -- update the Catalog and Schema & Volume names here for what you use\n",
    "CATALOG = \"mmt\"\n",
    "SCHEMA = \"pytorch\"\n",
    "VOLUME_NAME = \"torch_data\"\n",
    "\n",
    "# Dataset Configuration\n",
    "mds_train_dir = 'imagenet_tiny200_mds_train'\n",
    "mds_val_dir = 'imagenet_tiny200_mds_val'\n",
    "data_storage_location = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}\"\n",
    "\n",
    "# Training Configuration -- \n",
    "num_classes = 200\n",
    "NUM_EPOCHS = 2\n",
    "num_workers = 2\n",
    "\n",
    "# MLflow Configuration\n",
    "USER_NAME = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "experiment_name = f\"/Users/{USER_NAME}/mlflow_experiments/pytorch_imagenet_mobilenetv2_hpt\" ## a shared workspace folder is an alternative common path to use -- here we specify a user folder\n",
    "ARTIFACT_PATH = f\"dbfs:/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}\"\n",
    "\n",
    "# Setup MLflow\n",
    "import mlflow \n",
    "\n",
    "# Enable system metrics logging globally -- this tracks cpu/gpu metrics\n",
    "mlflow.enable_system_metrics_logging()\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "if mlflow.get_experiment_by_name(experiment_name) is None:\n",
    "    mlflow.create_experiment(name=experiment_name, artifact_location=ARTIFACT_PATH)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"Data storage location: {data_storage_location}\")\n",
    "print(f\"Training data: {data_storage_location}/{mds_train_dir}\")\n",
    "print(f\"Validation data: {data_storage_location}/{mds_val_dir}\")\n",
    "print(f\"MLflow experiment: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333f7b44-2d24-4d58-9f8e-373ed9087703",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dataloader for Multi-Shard Structure"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloader_with_mosaic(remote_path, local_cache_path, batch_size, rank=0):\n",
    "    \"\"\"Get MosaicML DataLoader with  settings for larger batch sizes\"\"\"\n",
    "    \n",
    "    import os\n",
    "    import shutil\n",
    "    from streaming import StreamingDataset\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    print(f\"Rank {rank}: Getting  MDS data from {remote_path}\")\n",
    "    \n",
    "    # Create unique cache directory\n",
    "    import time\n",
    "    import uuid\n",
    "    cache_suffix = f\"{int(time.time())}_{rank}_{str(uuid.uuid4())[:8]}\"\n",
    "    unique_cache_path = f\"{local_cache_path}_{cache_suffix}\"\n",
    "    \n",
    "    try:\n",
    "        # StreamingDataset configuration\n",
    "        dataset = StreamingDataset(\n",
    "            remote=remote_path,\n",
    "            local=unique_cache_path,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "            # Key optimizations for Mosaic streaming\n",
    "            download_retry=3,                # Retry failed downloads\n",
    "            download_timeout=120,            # Longer timeout for large files\n",
    "            keep_zip=False,                  # Don't keep compressed files\n",
    "            cache_limit=\"50gb\",              # Limit cache size\n",
    "            predownload=min(batch_size * 4, 1000),  # Predownload samples\n",
    "            partition_algo='relaxed'         # Better load balancing\n",
    "        )\n",
    "        \n",
    "        print(f\"Rank {rank}: Created StreamingDataset with {len(dataset)} samples\")\n",
    "        \n",
    "        # Calculate optimal num_workers based on batch size and system\n",
    "        import multiprocessing\n",
    "        max_workers = min(multiprocessing.cpu_count(), 16)\n",
    "        optimal_workers = min(max_workers, max(2, batch_size // 8))\n",
    "        \n",
    "        # Optimized DataLoader settings\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,                   # StreamingDataset handles shuffling\n",
    "            num_workers=optimal_workers,     # Dynamic worker count\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=max(2, min(8, batch_size // 16)),  # Dynamic prefetch\n",
    "            drop_last=True,\n",
    "            multiprocessing_context='spawn' if os.name == 'nt' else 'fork'  # OS-specific\n",
    "        )\n",
    "        \n",
    "        print(f\"Rank {rank}: Created optimized dataloader with {len(dataloader)} batches, {optimal_workers} workers\")\n",
    "        \n",
    "        return dataloader, unique_cache_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Rank {rank}: Error creating dataloader: {e}\")\n",
    "        if os.path.exists(unique_cache_path):\n",
    "            shutil.rmtree(unique_cache_path, ignore_errors=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b82505-9204-4a15-9da7-0b01f04b2196",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Loading Functions for Multi-Shard Structure"
    }
   },
   "outputs": [],
   "source": [
    "### your data structure may be different -->  verify_mds_dataset will need to be updated \n",
    "\n",
    "import io\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "from streaming import StreamingDataset\n",
    "\n",
    "def create_comprehensive_label_mapping(remote_path):\n",
    "    \"\"\"Extract class names using DataLoader approach\"\"\"\n",
    "    print(f\"Creating comprehensive label mapping from: {remote_path}\")\n",
    "    \n",
    "    temp_cache = tempfile.mkdtemp(prefix=\"label_mapping_\")\n",
    "    \n",
    "    try:\n",
    "        dataset = StreamingDataset(\n",
    "            remote=remote_path,\n",
    "            local=temp_cache,\n",
    "            shuffle=True,\n",
    "            batch_size=32  # Use a reasonable batch size\n",
    "        )\n",
    "        \n",
    "        # Create DataLoader\n",
    "        from torch.utils.data import DataLoader\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=32,\n",
    "            num_workers=0  # Use 0 to avoid multiprocessing issues during class extraction\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset created with {len(dataset)} total samples\")\n",
    "        \n",
    "        unique_classes = set()\n",
    "        sample_count = 0\n",
    "        max_batches = 50  # Process 50 batches to get class names\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "                \n",
    "            # batch['class_name'] is now a list of class names\n",
    "            class_names = batch['class_name']\n",
    "            unique_classes.update(class_names)\n",
    "            sample_count += len(class_names)\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Processed {batch_idx} batches, {sample_count} samples, found {len(unique_classes)} unique classes\")\n",
    "            \n",
    "            if len(unique_classes) >= 200:\n",
    "                break\n",
    "        \n",
    "        classes = sorted(list(unique_classes))\n",
    "        print(f\"Found {len(classes)} unique classes\")\n",
    "        print(f\"Sample classes: {classes[:5]}\")\n",
    "        \n",
    "        label_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        \n",
    "        print(f\"\\n=== Label Mapping Summary ===\")\n",
    "        print(f\"Total classes: {len(label_to_idx)}\")\n",
    "        \n",
    "        return label_to_idx\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating label mapping: {e}\")\n",
    "        import traceback\n",
    "        print(f\"Full error: {traceback.format_exc()}\")\n",
    "        return create_imagenet_tiny200_mapping()\n",
    "    \n",
    "    finally:\n",
    "        if os.path.exists(temp_cache):\n",
    "            shutil.rmtree(temp_cache, ignore_errors=True)\n",
    "\n",
    "def create_imagenet_tiny200_mapping():\n",
    "    \"\"\"Fallback mapping\"\"\"\n",
    "    print(\"Creating fallback ImageNet Tiny-200 mapping...\")\n",
    "    classes = [f\"class_{i:03d}\" for i in range(200)]\n",
    "    return {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "def verify_mds_dataset(remote_path):\n",
    "    \"\"\"Simple verification for multi-shard MDS dataset\"\"\"\n",
    "    print(f\"Verifying multi-shard MDS dataset at: {remote_path}\")\n",
    "    \n",
    "    if not os.path.exists(remote_path):\n",
    "        print(f\"ERROR: Remote path does not exist: {remote_path}\")\n",
    "        return False\n",
    "    \n",
    "    files_in_dir = os.listdir(remote_path)\n",
    "    print(f\"Items in root directory: {len(files_in_dir)}\")\n",
    "    \n",
    "    # Check for main index.json\n",
    "    has_main_index = 'index.json' in files_in_dir\n",
    "    if has_main_index:\n",
    "        print(\"Found main index.json\")\n",
    "    \n",
    "    # Check for numbered shard directories\n",
    "    shard_dirs = [f for f in files_in_dir if f.isdigit() and os.path.isdir(os.path.join(remote_path, f))]\n",
    "    shard_dirs.sort(key=int)\n",
    "    \n",
    "    print(f\"Found {len(shard_dirs)} shard directories\")\n",
    "    if len(shard_dirs) > 0:\n",
    "        print(f\"  Shard range: {shard_dirs[0]} to {shard_dirs[-1]}\")\n",
    "        print(f\"  Sample shards: {shard_dirs[:5]}{'...' if len(shard_dirs) > 5 else ''}\")\n",
    "        \n",
    "        # Check first shard\n",
    "        if len(shard_dirs) > 0:\n",
    "            shard_path = os.path.join(remote_path, shard_dirs[0])\n",
    "            shard_contents = os.listdir(shard_path)\n",
    "            print(f\"Shard {shard_dirs[0]} contents: {shard_contents}\")\n",
    "            \n",
    "            # Check for shard files\n",
    "            shard_files = [f for f in shard_contents if f.endswith('.mds') or f.endswith('.mds.zstd')]\n",
    "            if shard_files:\n",
    "                shard_file_path = os.path.join(shard_path, shard_files[0])\n",
    "                file_size = os.path.getsize(shard_file_path)\n",
    "                print(f\"Shard file size: {file_size:,} bytes\")\n",
    "        \n",
    "        print(f\"Verified sample shards\")\n",
    "        return True\n",
    "    \n",
    "    print(\"No valid shard directories found\")\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6fd907a-8786-4fbe-af91-8d44ad689f18",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "convert_batch_to_tensors"
    }
   },
   "outputs": [],
   "source": [
    "def convert_batch_to_tensors(batch, device=None, class_to_idx=None, rank=None, \n",
    "                           use_augmentation=False, augmentation_strength='medium'):\n",
    "    \"\"\"Convert MDS batch to PyTorch tensors with optional data augmentation\"\"\"\n",
    "    import io\n",
    "    from PIL import Image\n",
    "    import torchvision.transforms as transforms\n",
    "    import torch\n",
    "    \n",
    "    # Auto-detect device if not provided\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Handle rank parameter - get from distributed training if available\n",
    "    if rank is None:\n",
    "        try:\n",
    "            if torch.distributed.is_initialized():\n",
    "                rank = torch.distributed.get_rank()\n",
    "            else:\n",
    "                rank = 0\n",
    "        except:\n",
    "            rank = 0\n",
    "    \n",
    "    # Define augmentation presets\n",
    "    augmentation_presets = {\n",
    "        'light': {\n",
    "            'rotation': 10,\n",
    "            'brightness': 0.1,\n",
    "            'contrast': 0.1,\n",
    "            'saturation': 0.1,\n",
    "            'hue': 0.05,\n",
    "            'horizontal_flip': 0.3,\n",
    "            'vertical_flip': 0.0,\n",
    "            'gaussian_blur': 0.0,\n",
    "            'random_erasing': 0.0\n",
    "        },\n",
    "        'medium': {\n",
    "            'rotation': 15,\n",
    "            'brightness': 0.2,\n",
    "            'contrast': 0.2,\n",
    "            'saturation': 0.2,\n",
    "            'hue': 0.1,\n",
    "            'horizontal_flip': 0.5,\n",
    "            'vertical_flip': 0.1,\n",
    "            'gaussian_blur': 0.1,\n",
    "            'random_erasing': 0.1\n",
    "        },\n",
    "        'heavy': {\n",
    "            'rotation': 25,\n",
    "            'brightness': 0.3,\n",
    "            'contrast': 0.3,\n",
    "            'saturation': 0.3,\n",
    "            'hue': 0.15,\n",
    "            'horizontal_flip': 0.5,\n",
    "            'vertical_flip': 0.2,\n",
    "            'gaussian_blur': 0.2,\n",
    "            'random_erasing': 0.2\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Build transform pipeline\n",
    "    transform_list = [transforms.Resize((224, 224))]\n",
    "    \n",
    "    if use_augmentation:\n",
    "        # Get augmentation parameters\n",
    "        if isinstance(augmentation_strength, str):\n",
    "            aug_params = augmentation_presets.get(augmentation_strength, augmentation_presets['medium'])\n",
    "        elif isinstance(augmentation_strength, dict):\n",
    "            aug_params = augmentation_strength\n",
    "        else:\n",
    "            aug_params = augmentation_presets['medium']\n",
    "        \n",
    "        # Print augmentation info only once per function and only from rank 0\n",
    "        if not hasattr(convert_batch_to_tensors, '_printed_aug_info'):\n",
    "            if rank == 0:\n",
    "                print(f\"Using {augmentation_strength} augmentation with parameters: {aug_params}\")\n",
    "            convert_batch_to_tensors._printed_aug_info = True\n",
    "\n",
    "        # Add augmentation transforms\n",
    "        if aug_params.get('rotation', 0) > 0:\n",
    "            transform_list.append(\n",
    "                transforms.RandomRotation(degrees=aug_params['rotation'])\n",
    "            )\n",
    "        \n",
    "        if aug_params.get('horizontal_flip', 0) > 0:\n",
    "            transform_list.append(\n",
    "                transforms.RandomHorizontalFlip(p=aug_params['horizontal_flip'])\n",
    "            )\n",
    "        \n",
    "        if aug_params.get('vertical_flip', 0) > 0:\n",
    "            transform_list.append(\n",
    "                transforms.RandomVerticalFlip(p=aug_params['vertical_flip'])\n",
    "            )\n",
    "        \n",
    "        # Color jittering\n",
    "        color_params = [\n",
    "            aug_params.get('brightness', 0),\n",
    "            aug_params.get('contrast', 0),\n",
    "            aug_params.get('saturation', 0),\n",
    "            aug_params.get('hue', 0)\n",
    "        ]\n",
    "        if any(p > 0 for p in color_params):\n",
    "            transform_list.append(\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=color_params[0],\n",
    "                    contrast=color_params[1],\n",
    "                    saturation=color_params[2],\n",
    "                    hue=color_params[3]\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Gaussian blur\n",
    "        if aug_params.get('gaussian_blur', 0) > 0:\n",
    "            transform_list.append(\n",
    "                transforms.RandomApply([\n",
    "                    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    "                ], p=aug_params['gaussian_blur'])\n",
    "            )\n",
    "    \n",
    "    # Add normalization (always last before tensor conversion)\n",
    "    transform_list.extend([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Add random erasing after tensor conversion if specified\n",
    "    if use_augmentation and aug_params.get('random_erasing', 0) > 0:\n",
    "        transform_list.append(\n",
    "            transforms.RandomErasing(\n",
    "                p=aug_params['random_erasing'],\n",
    "                scale=(0.02, 0.33),\n",
    "                ratio=(0.3, 3.3),\n",
    "                value=0\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    transform = transforms.Compose(transform_list)\n",
    "    \n",
    "    # Debug info only for rank 0 and only occasionally\n",
    "    debug_output = (rank == 0)\n",
    "    \n",
    "    # REMOVED THE DUPLICATE PRINT STATEMENT HERE\n",
    "    \n",
    "    # Process images\n",
    "    images = []\n",
    "    for i, img_bytes in enumerate(batch['image_data']):\n",
    "        try:\n",
    "            img = Image.open(io.BytesIO(img_bytes)).convert('RGB')\n",
    "            img_tensor = transform(img)\n",
    "            images.append(img_tensor)\n",
    "        except Exception as e:\n",
    "            if debug_output and i < 3:  # Only show first few errors\n",
    "                print(f\"Error processing image {i}: {e}\")\n",
    "            images.append(torch.zeros((3, 224, 224)))\n",
    "    \n",
    "    images_tensor = torch.stack(images).to(device)\n",
    "    \n",
    "    # Process labels \n",
    "    class_names = batch['class_name']\n",
    "    \n",
    "    # Ensure class_to_idx is a dictionary, not an integer\n",
    "    if class_to_idx is None or not isinstance(class_to_idx, dict):\n",
    "        unique_classes = sorted(set(class_names))\n",
    "        class_to_idx = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "        if debug_output:\n",
    "            print(f\"Created temporary class mapping with {len(class_to_idx)} classes\")\n",
    "    \n",
    "    labels = []\n",
    "    for class_name in class_names:\n",
    "        # Use isinstance check instead of 'in' operator with potential int\n",
    "        if isinstance(class_to_idx, dict) and class_name in class_to_idx:\n",
    "            labels.append(class_to_idx[class_name])\n",
    "        else:\n",
    "            labels.append(0)  # Default to class 0 for unknown classes\n",
    "            if debug_output:\n",
    "                print(f\"Warning: Unknown class '{class_name}', using default\")\n",
    "    \n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "    \n",
    "    return images_tensor, labels_tensor\n",
    "\n",
    "\n",
    "def create_custom_augmentation_config(\n",
    "    rotation=15,\n",
    "    brightness=0.2,\n",
    "    contrast=0.2,\n",
    "    saturation=0.2,\n",
    "    hue=0.1,\n",
    "    horizontal_flip=0.5,\n",
    "    vertical_flip=0.1,\n",
    "    gaussian_blur=0.1,\n",
    "    random_erasing=0.1\n",
    "):\n",
    "    \"\"\"Create custom augmentation configuration\n",
    "    \n",
    "    Args:\n",
    "        rotation: Degrees of random rotation (0 to disable)\n",
    "        brightness: Brightness jitter factor (0 to disable)\n",
    "        contrast: Contrast jitter factor (0 to disable)\n",
    "        saturation: Saturation jitter factor (0 to disable)\n",
    "        hue: Hue jitter factor (0 to disable)\n",
    "        horizontal_flip: Probability of horizontal flip (0 to disable)\n",
    "        vertical_flip: Probability of vertical flip (0 to disable)\n",
    "        gaussian_blur: Probability of gaussian blur (0 to disable)\n",
    "        random_erasing: Probability of random erasing (0 to disable)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Custom augmentation configuration\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'rotation': rotation,\n",
    "        'brightness': brightness,\n",
    "        'contrast': contrast,\n",
    "        'saturation': saturation,\n",
    "        'hue': hue,\n",
    "        'horizontal_flip': horizontal_flip,\n",
    "        'vertical_flip': vertical_flip,\n",
    "        'gaussian_blur': gaussian_blur,\n",
    "        'random_erasing': random_erasing\n",
    "    }\n",
    "\n",
    "\n",
    "## Usage examples:\n",
    "# def example_usage():\n",
    "#     \"\"\"Examples of how to use the enhanced convert_batch_to_tensors function\"\"\"\n",
    "    \n",
    "#     # Example 1: No augmentation (default behavior)\n",
    "#     images, labels = convert_batch_to_tensors(batch, device, class_to_idx)\n",
    "    \n",
    "#     # Example 2: Light augmentation\n",
    "#     images, labels = convert_batch_to_tensors(\n",
    "#         batch, device, class_to_idx, \n",
    "#         use_augmentation=True, \n",
    "#         augmentation_strength='light'\n",
    "#     )\n",
    "    \n",
    "#     # Example 3: Heavy augmentation\n",
    "#     images, labels = convert_batch_to_tensors(\n",
    "#         batch, device, class_to_idx,\n",
    "#         use_augmentation=True,\n",
    "#         augmentation_strength='heavy'\n",
    "#     )\n",
    "    \n",
    "#     # Example 4: Custom augmentation\n",
    "#     custom_aug = create_custom_augmentation_config(\n",
    "#         rotation=20,\n",
    "#         brightness=0.3,\n",
    "#         horizontal_flip=0.7,\n",
    "#         gaussian_blur=0.2,\n",
    "#         random_erasing=0.15\n",
    "#     )\n",
    "    \n",
    "#     images, labels = convert_batch_to_tensors(\n",
    "#         batch, device, class_to_idx,\n",
    "#         use_augmentation=True,\n",
    "#         augmentation_strength=custom_aug\n",
    "#     )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e870bd84-a85c-4f84-bb68-83f0edacbaaf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "quick check | Dataset Verification for Multi-Shard Structure"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. FIRST: Define device globally\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Create class mapping\n",
    "print(\"Creating class mapping...\")\n",
    "label_to_idx = create_comprehensive_label_mapping(f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}/{mds_train_dir}\")\n",
    "\n",
    "# 3. Verify datasets\n",
    "print(\"Verifying datasets...\")\n",
    "train_valid = verify_mds_dataset(f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}/{mds_train_dir}\")\n",
    "val_valid = verify_mds_dataset(f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}/{mds_val_dir}\")\n",
    "\n",
    "print(\"Setup complete! Ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315979ff-98c4-46b7-b465-e2388f88441b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Label Mapping"
    }
   },
   "outputs": [],
   "source": [
    "# Label Mapping\n",
    "print(\"=== Creating Comprehensive Label Mapping ===\")\n",
    "train_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}/{mds_train_dir}\"\n",
    "\n",
    "# Global variables that need to be defined\n",
    "label_to_idx = {}\n",
    "\n",
    "try:\n",
    "    label_to_idx = create_comprehensive_label_mapping(train_path)\n",
    "    num_classes = len(label_to_idx)\n",
    "    \n",
    "    # If we didn't get enough classes, use the fallback\n",
    "    if num_classes < 50:\n",
    "        print(\"Insufficient classes found, using fallback mapping...\")\n",
    "        label_to_idx = create_imagenet_tiny200_mapping()\n",
    "        num_classes = len(label_to_idx)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in comprehensive mapping: {e}\")\n",
    "    print(\"Using fallback mapping...\")\n",
    "    label_to_idx = create_imagenet_tiny200_mapping()\n",
    "    num_classes = len(label_to_idx)\n",
    "\n",
    "print(f\"\\nFinal label mapping created with {num_classes} classes\")\n",
    "\n",
    "# Test the mapping with a few samples\n",
    "print(\"\\n=== Testing Label Mapping ===\")\n",
    "test_classes = ['barrel, cask', 'school bus', 'pizza, pizza pie']\n",
    "for test_class in test_classes:\n",
    "    idx = label_to_idx.get(test_class, -1)\n",
    "    print(f\"'{test_class}' -> index {idx}\")\n",
    "\n",
    "# Show some actual mappings that exist\n",
    "print(f\"\\nFirst 10 actual mappings:\")\n",
    "for i, (class_name, idx) in enumerate(list(label_to_idx.items())[:10]):\n",
    "    print(f\"  {idx}: {class_name}\")\n",
    "\n",
    "print(f\"\\nLabel mapping setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43edb6dc-a3bc-497c-9523-56416f28987f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Definition"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(lr=0.001):\n",
    "    \"\"\"Create MobileNetV2 model for ImageNet Tiny-200\"\"\"\n",
    "    from torchvision.models import MobileNet_V2_Weights\n",
    "    \n",
    "    model = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Freeze feature extraction layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace classifier for num_classes\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier[1] = torch.nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    # Only train the classifier\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c852aa06-424b-40ba-a6fd-10c5ae17d6a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loss function"
    }
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    ## import torch.nn as nn ## moved up in dependencies import\n",
    "\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(1)\n",
    "        log_preds = torch.log_softmax(pred, dim=1)\n",
    "        smooth_target = torch.zeros_like(log_preds).scatter_(1, target.unsqueeze(1), 1)\n",
    "        smooth_target = smooth_target * (1 - self.smoothing) + self.smoothing / n_classes\n",
    "        return (-smooth_target * log_preds).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f119673-71ec-49ad-9df0-fc0334075d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0b07a59-fe4c-4821-b7ce-12f7c3bd8521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### IF needed -- some checks before training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0bb5a11a-3e12-4f71-ab83-dceeb95c03c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check cuda"
    }
   },
   "outputs": [],
   "source": [
    "# # Before training starts:\n",
    "# if torch.cuda.is_available():\n",
    "#     for i in range(torch.cuda.device_count()):\n",
    "#         total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "#         print(f\"GPU {i}: {total_memory:.1f}GB total memory\")\n",
    "#         torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9be67f7e-8116-463b-bae9-b7e74aa30592",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check data_loading if bottleneck"
    }
   },
   "outputs": [],
   "source": [
    "# def debug_data_loading_bottleneck():\n",
    "#     \"\"\"Debug the specific data loading steps\"\"\"\n",
    "#     import time\n",
    "    \n",
    "#     print(\"=== DEBUGGING DATA LOADING BOTTLENECK ===\")\n",
    "    \n",
    "#     # Test 1: Check if it's the MDS dataset creation\n",
    "#     print(\"1. Testing MDS dataset creation...\")\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     try:\n",
    "#         from streaming import StreamingDataset\n",
    "#         import tempfile\n",
    "        \n",
    "#         cache_dir = tempfile.mkdtemp(prefix=\"debug_cache_\")\n",
    "#         print(f\"   Cache dir: {cache_dir}\")\n",
    "        \n",
    "#         print(\"   Creating StreamingDataset...\")\n",
    "#         dataset = StreamingDataset(\n",
    "#             remote=\"/Volumes/mmt/pytorch/torch_data/imagenet_tiny200_mds_train\",\n",
    "#             local=cache_dir,\n",
    "#             shuffle=True,\n",
    "#             batch_size=4,  # Small batch for testing\n",
    "#             predownload=8,  # Minimal predownload\n",
    "#             download_timeout=120  # 2 minute timeout\n",
    "#         )\n",
    "        \n",
    "#         creation_time = time.time() - start_time\n",
    "#         print(f\"✓ Dataset created in {creation_time:.1f} seconds\")\n",
    "        \n",
    "#         # Test 2: Check if it's the first sample access\n",
    "#         print(\"2. Testing first sample access...\")\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         first_sample = dataset[0]\n",
    "#         sample_time = time.time() - start_time\n",
    "#         print(f\"✓ First sample loaded in {sample_time:.1f} seconds\")\n",
    "#         print(f\"   Sample keys: {list(first_sample.keys())}\")\n",
    "        \n",
    "#         # Test 3: Check if it's the DataLoader iteration\n",
    "#         print(\"3. Testing DataLoader iteration...\")\n",
    "#         from streaming import StreamingDataLoader\n",
    "        \n",
    "#         dataloader = StreamingDataLoader(\n",
    "#             dataset,\n",
    "#             batch_size=4,\n",
    "#             num_workers=0,  # No multiprocessing for debugging\n",
    "#             pin_memory=False\n",
    "#         )\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         batch = next(iter(dataloader))\n",
    "#         batch_time = time.time() - start_time\n",
    "#         print(f\"✓ First batch loaded in {batch_time:.1f} seconds\")\n",
    "        \n",
    "#         # Test 4: Check if it's the tensor conversion\n",
    "#         print(\"4. Testing tensor conversion...\")\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         device = torch.device('cuda:0')\n",
    "#         inputs, labels = convert_batch_to_tensors(batch, device, label_to_idx, 0)\n",
    "#         conversion_time = time.time() - start_time\n",
    "#         print(f\"✓ Tensor conversion completed in {conversion_time:.1f} seconds\")\n",
    "#         print(f\"   Inputs shape: {inputs.shape}, Labels shape: {labels.shape}\")\n",
    "        \n",
    "#         print(\"=== ALL DATA LOADING TESTS PASSED ===\")\n",
    "        \n",
    "#         # Cleanup\n",
    "#         import shutil\n",
    "#         shutil.rmtree(cache_dir, ignore_errors=True)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         elapsed = time.time() - start_time\n",
    "#         print(f\"✗ FAILED after {elapsed:.1f} seconds: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# # Run this test\n",
    "# debug_data_loading_bottleneck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9da81a31-d7d6-4a98-859c-3e8a7a9b4870",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check (non)distributed_training"
    }
   },
   "outputs": [],
   "source": [
    "# def debug_distributed_training():\n",
    "#     \"\"\"Debug the distributed training components specifically\"\"\"\n",
    "#     import time\n",
    "    \n",
    "#     print(\"=== DEBUGGING DISTRIBUTED TRAINING ===\")\n",
    "    \n",
    "#     # Test 1: Model creation and DDP wrapping\n",
    "#     print(\"1. Testing model creation...\")\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     try:\n",
    "#         model = get_model(lr=0.001)\n",
    "#         device = torch.device('cuda:0')\n",
    "#         model = model.to(device)\n",
    "#         print(f\"✓ Model created and moved to GPU in {time.time() - start_time:.1f} seconds\")\n",
    "        \n",
    "#         # Test 2: Simple forward pass\n",
    "#         print(\"2. Testing simple forward pass...\")\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         # Create dummy input\n",
    "#         dummy_input = torch.randn(4, 3, 224, 224).to(device)\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             output = model(dummy_input)\n",
    "        \n",
    "#         print(f\"✓ Forward pass completed in {time.time() - start_time:.1f} seconds\")\n",
    "#         print(f\"   Output shape: {output.shape}\")\n",
    "        \n",
    "#         # Test 3: Training step without distributed\n",
    "#         print(\"3. Testing training step (non-distributed)...\")\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         model.train()\n",
    "#         criterion = torch.nn.CrossEntropyLoss()\n",
    "#         optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "        \n",
    "#         # Dummy labels\n",
    "#         dummy_labels = torch.randint(0, 200, (4,)).to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(dummy_input)\n",
    "#         loss = criterion(outputs, dummy_labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         print(f\"✓ Training step completed in {time.time() - start_time:.1f} seconds\")\n",
    "#         print(f\"   Loss: {loss.item():.4f}\")\n",
    "        \n",
    "#         print(\"=== NON-DISTRIBUTED TRAINING WORKS FINE ===\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ FAILED: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# # Run this test\n",
    "# debug_distributed_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec143d1-79d9-44a2-be9c-cee38dc8f3c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test_training"
    }
   },
   "outputs": [],
   "source": [
    "# def simple_objective_function(trial):\n",
    "#     \"\"\"[simple] version to isolate the distributed issue\"\"\"\n",
    "    \n",
    "#     lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [8, 16])  # Smaller batches\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['AdamW']) \n",
    "    \n",
    "#     print(f\"Trial {trial.number}: lr={lr}, batch_size={batch_size}, optimizer={optimizer_name}\")\n",
    "    \n",
    "#     with mlflow.start_run(run_name=f\"simplified_trial_{trial.number}\", nested=True):\n",
    "#         mlflow.log_params({\n",
    "#             'lr': lr,\n",
    "#             'batch_size': batch_size,\n",
    "#             'optimizer': optimizer_name,\n",
    "#             'test_mode': 'simplified'\n",
    "#         })\n",
    "        \n",
    "#         try:\n",
    "#             # Test with SINGLE PROCESS first (no distribution)\n",
    "#             distributor = TorchDistributor(\n",
    "#                 num_processes=1,  # ← Single process to test\n",
    "#                 local_mode=True,   # ← Local mode\n",
    "#                 use_gpu=True\n",
    "#             )\n",
    "            \n",
    "#             result = distributor.run(\n",
    "#                 simplified_train_function,  # ← We'll create this\n",
    "#                 lr=lr,\n",
    "#                 batch_size=batch_size,\n",
    "#                 optimizer_name=optimizer_name\n",
    "#             )\n",
    "            \n",
    "#             trial_metric = result.get('val_acc', 0.0) if isinstance(result, dict) else 0.0\n",
    "#             mlflow.log_metric(\"validation_accuracy\", trial_metric)\n",
    "            \n",
    "#             print(f\"Simplified trial {trial.number} completed: {trial_metric:.4f}\")\n",
    "#             return trial_metric\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Simplified trial {trial.number} failed: {e}\")\n",
    "#             mlflow.log_param(\"error\", str(e))\n",
    "#             return 0.0\n",
    "\n",
    "# def simplified_train_function(lr, batch_size, optimizer_name):\n",
    "#     \"\"\"Simplified training function - single process\"\"\"\n",
    "#     print(\"Starting simplified training (single process)...\")\n",
    "    \n",
    "#     try:\n",
    "#         device = torch.device('cuda:0')\n",
    "        \n",
    "#         # Create model\n",
    "#         model = get_model(lr=lr)\n",
    "#         model = model.to(device)\n",
    "        \n",
    "#         # Create single dataloader (no distributed)\n",
    "#         train_loader, train_cache = get_dataloader_with_mosaic(\n",
    "#             \"/Volumes/mmt/pytorch/torch_data/imagenet_tiny200_mds_train\",\n",
    "#             None, batch_size, rank=0\n",
    "#         )\n",
    "        \n",
    "#         val_loader, val_cache = get_dataloader_with_mosaic(\n",
    "#             \"/Volumes/mmt/pytorch/torch_data/imagenet_tiny200_mds_val\",\n",
    "#             None, batch_size, rank=0\n",
    "#         )\n",
    "        \n",
    "#         # Setup training\n",
    "#         criterion = torch.nn.CrossEntropyLoss()\n",
    "#         optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "        \n",
    "#         print(\"Starting training loop...\")\n",
    "        \n",
    "#         # Train for just 1 epoch with limited steps\n",
    "#         model.train()\n",
    "#         for step, batch in enumerate(train_loader):\n",
    "#             if step >= 10:  # Only 10 steps for testing\n",
    "#                 break\n",
    "                \n",
    "#             print(f\"Step {step}/10...\")\n",
    "            \n",
    "#             inputs, labels = convert_batch_to_tensors(batch, device, label_to_idx, 0)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             if step % 5 == 0:\n",
    "#                 print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
    "        \n",
    "#         # Quick validation\n",
    "#         model.eval()\n",
    "#         val_acc = 0.1  # Placeholder\n",
    "        \n",
    "#         print(f\"Simplified training completed. Val accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "#         # Cleanup\n",
    "#         import shutil\n",
    "#         for cache in [train_cache, val_cache]:\n",
    "#             if os.path.exists(cache):\n",
    "#                 shutil.rmtree(cache)\n",
    "        \n",
    "#         return {\"val_acc\": val_acc, \"status\": \"completed\"}\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in simplified training: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return {\"val_acc\": 0.0, \"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "# # Test with simplified version\n",
    "# print(\"Testing simplified single-process version...\")\n",
    "# study_simple = optuna.create_study(direction=\"maximize\")\n",
    "# study_simple.optimize(simple_objective_function, n_trials=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "517ba964-745d-402e-ab2c-d6b632d6784a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efdd5457-cfef-4afa-97a9-56351bdbb4b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab77d7d-de79-42e2-803d-8f1d4e363951",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Training + Evaluation + Metrics"
    }
   },
   "outputs": [],
   "source": [
    "def distributed_train_and_evaluate(lr=0.001, batch_size=256, optimizer_name='AdamW',\n",
    "                                 weight_decay=1e-4, step_size=7, gamma=0.1,\n",
    "                                 dropout_rate=0.2, label_smoothing=0.1,\n",
    "                                 momentum=0.9, nesterov=False, beta1=0.9, beta2=0.999, eps=1e-8,\n",
    "                                 data_storage_location=None, mds_train_dir=None, \n",
    "                                 mds_val_dir=None, num_epochs=2,\n",
    "                                 mlflow_run_id=None, mlflow_tracking_uri=None,\n",
    "                                 mlflow_experiment_name=None,\n",
    "                                 use_augmentation=True, augmentation_strength='medium'):\n",
    "    \n",
    "    \"\"\"Define distributed training with essential model and system metrics and data augmentation\"\"\"\n",
    "    \n",
    "    import os\n",
    "    import time\n",
    "    import shutil\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.distributed as dist\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "    from datetime import timedelta\n",
    "    import numpy as np\n",
    "    \n",
    "    # Initialize core variables\n",
    "    cache_paths = []\n",
    "    training_start_time = time.time()\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "    global_rank = int(os.environ.get(\"RANK\", 0))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    \n",
    "    # Core metrics storage\n",
    "    epoch_metrics = []\n",
    "    essential_model_metrics = {\n",
    "        'parameter_count': 0,\n",
    "        'trainable_parameters': 0,\n",
    "        'model_size_mb': 0.0,\n",
    "        'gradient_norms': [],\n",
    "        'weight_norms': [],\n",
    "        'learning_curves': {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    }\n",
    "    \n",
    "    # Calculate effective batch size\n",
    "    effective_batch_size = batch_size * world_size\n",
    "    \n",
    "    try:\n",
    "        print(f\"Rank {global_rank}/{world_size}: Starting training...\")\n",
    "        print(f\"Per-GPU batch size: {batch_size}, Effective batch size: {effective_batch_size}\")\n",
    "        \n",
    "        if global_rank == 0:\n",
    "            print(f\" MEMORY OPTIMIZATION:\")\n",
    "            print(f\"   - Large batch size: {batch_size} per GPU\")\n",
    "            print(f\"   - Effective batch size: {effective_batch_size}\")\n",
    "            print(f\"   - Augmentation: {augmentation_strength if use_augmentation else 'disabled'}\")\n",
    "        \n",
    "        # Initialize distributed training\n",
    "        if world_size > 1:\n",
    "            dist.init_process_group(\"nccl\", timeout=timedelta(seconds=1800))\n",
    "        \n",
    "        ### \n",
    "        device = torch.device(f'cuda:{local_rank}' if torch.cuda.is_available() else 'cpu')\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_device(local_rank)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Create enhanced model\n",
    "        model = get_model(lr=lr)\n",
    "        \n",
    "        # Add dropout to classifier\n",
    "        if hasattr(model, 'classifier') and isinstance(model.classifier, nn.Sequential):\n",
    "            if len(model.classifier) > 1 and hasattr(model.classifier[1], 'in_features'):\n",
    "                num_features = model.classifier[1].in_features\n",
    "                model.classifier = nn.Sequential(\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(num_features, 512),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(dropout_rate * 0.5),\n",
    "                    nn.Linear(512, 200)\n",
    "                )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Enable channels_last for better memory efficiency with larger batches\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "        \n",
    "        # Calculate essential model metrics\n",
    "        if global_rank == 0:\n",
    "            essential_model_metrics['parameter_count'] = sum(p.numel() for p in model.parameters())\n",
    "            essential_model_metrics['trainable_parameters'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            essential_model_metrics['model_size_mb'] = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
    "            print(f\"Model: {essential_model_metrics['parameter_count']:,} total params, \"\n",
    "                  f\"{essential_model_metrics['trainable_parameters']:,} trainable\")\n",
    "        \n",
    "        if world_size > 1:\n",
    "            model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n",
    "        \n",
    "        # Create data loaders with larger batch sizes\n",
    "        train_input_remote_path = os.path.join(data_storage_location, mds_train_dir)\n",
    "        val_input_remote_path = os.path.join(data_storage_location, mds_val_dir)\n",
    "        \n",
    "        # Use larger batch sizes with optimized DataLoader\n",
    "        train_dataloader, train_cache_path = get_dataloader_with_mosaic(\n",
    "            train_input_remote_path, f\"/local_disk0/tmp/train_cache\", batch_size, global_rank\n",
    "        )\n",
    "        cache_paths.append(train_cache_path)\n",
    "        \n",
    "        # Use even larger batch size for validation (no gradients needed)\n",
    "        val_batch_size = min(batch_size * 2, 1024)  # 2x larger for validation\n",
    "        val_dataloader, val_cache_path = get_dataloader_with_mosaic(\n",
    "            val_input_remote_path, f\"/local_disk0/tmp/val_cache\", val_batch_size, global_rank\n",
    "        )\n",
    "        cache_paths.append(val_cache_path)\n",
    "        \n",
    "        if world_size > 1:\n",
    "            dist.barrier()\n",
    "        \n",
    "        # Setup training components\n",
    "        criterion = LabelSmoothingCrossEntropy(label_smoothing) if label_smoothing > 0 else nn.CrossEntropyLoss()\n",
    "        \n",
    "        model_params = [p for p in (model.module if world_size > 1 else model).parameters() if p.requires_grad]\n",
    "        \n",
    "        # Adjust learning rate for larger batch size (linear scaling rule)\n",
    "        adjusted_lr = lr * max(1.0, effective_batch_size / 256)  # Scale LR with batch size\n",
    "        \n",
    "        if optimizer_name == 'SGD':\n",
    "            optimizer = torch.optim.SGD(model_params, lr=adjusted_lr, momentum=momentum, \n",
    "                                      weight_decay=weight_decay, nesterov=nesterov)\n",
    "        elif optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(model_params, lr=adjusted_lr, weight_decay=weight_decay, \n",
    "                                       betas=(beta1, beta2), eps=eps)\n",
    "        else:  # AdamW\n",
    "            optimizer = torch.optim.AdamW(model_params, lr=adjusted_lr, weight_decay=weight_decay, \n",
    "                                        betas=(beta1, beta2), eps=eps)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "        \n",
    "        # Add mixed precision scaler for memory efficiency | torch.amp.GradScaler('cuda', args...)\n",
    "        scaler = torch.cuda.amp.GradScaler() \n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        \n",
    "        # Log memory usage before training\n",
    "        if global_rank == 0 and torch.cuda.is_available():\n",
    "            memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            memory_util = memory_allocated/memory_total*100\n",
    "            print(f\"Pre-training GPU memory: {memory_allocated:.2f} GB / {memory_total:.2f} GB ({memory_util:.1f}%)\")\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Rank {global_rank}: Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Training phase with memory optimization\n",
    "            train_results = train_one_epoch(\n",
    "                model, criterion, optimizer, scheduler, train_dataloader, \n",
    "                epoch, device, global_rank, label_to_idx,\n",
    "                use_augmentation=use_augmentation,\n",
    "                augmentation_strength=augmentation_strength,\n",
    "                scaler=scaler  # Pass scaler for mixed precision\n",
    "            )\n",
    "            \n",
    "            # Validation phase\n",
    "            val_results = evaluate(\n",
    "                model, criterion, val_dataloader, epoch, device, global_rank, label_to_idx,\n",
    "                use_augmentation=False,\n",
    "                scaler=scaler  # Pass scaler for mixed precision\n",
    "            )\n",
    "            \n",
    "            train_loss, train_acc = train_results['loss'], train_results['accuracy']\n",
    "            val_loss, val_acc = val_results['loss'], val_results['accuracy']\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "            \n",
    "            # Store essential metrics\n",
    "            if global_rank == 0:\n",
    "                essential_model_metrics['gradient_norms'].append(train_results.get('gradient_norm', 0.0))\n",
    "                essential_model_metrics['weight_norms'].append(train_results.get('weight_norm', 0.0))\n",
    "                essential_model_metrics['learning_curves']['train_loss'].append(float(train_loss))\n",
    "                essential_model_metrics['learning_curves']['val_loss'].append(float(val_loss))\n",
    "                essential_model_metrics['learning_curves']['train_acc'].append(float(train_acc))\n",
    "                essential_model_metrics['learning_curves']['val_acc'].append(float(val_acc))\n",
    "                \n",
    "                epoch_data = {\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': float(train_loss),\n",
    "                    'train_acc': float(train_acc),\n",
    "                    'val_loss': float(val_loss),\n",
    "                    'val_acc': float(val_acc),\n",
    "                    'learning_rate': float(optimizer.param_groups[0]['lr']),\n",
    "                    'gradient_norm': train_results.get('gradient_norm', 0.0),\n",
    "                    'weight_norm': train_results.get('weight_norm', 0.0)\n",
    "                }\n",
    "                epoch_metrics.append(epoch_data)\n",
    "                \n",
    "                # Log memory usage during training\n",
    "                if torch.cuda.is_available():\n",
    "                    memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "                    memory_util = memory_allocated/memory_total*100\n",
    "                    print(f'Epoch {epoch+1}: Train {train_acc:.4f}, Val {val_acc:.4f}, '\n",
    "                          f'Loss {train_loss:.4f}/{val_loss:.4f}, LR {optimizer.param_groups[0][\"lr\"]:.2e}, '\n",
    "                          f'Memory {memory_util:.1f}%')\n",
    "                else:\n",
    "                    print(f'Epoch {epoch+1}: Train {train_acc:.4f}, Val {val_acc:.4f}, '\n",
    "                          f'Loss {train_loss:.4f}/{val_loss:.4f}, LR {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "        \n",
    "        training_time = time.time() - training_start_time\n",
    "        \n",
    "        return {\n",
    "            \"val_acc\": float(best_val_acc),\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"train_acc\": float(train_acc),\n",
    "            \"best_val_acc\": float(best_val_acc),\n",
    "            \"status\": \"completed\",\n",
    "            \"epochs_completed\": num_epochs,\n",
    "            \"training_time\": training_time,\n",
    "            \"epoch_metrics\": epoch_metrics,\n",
    "            \"model_metrics\": essential_model_metrics,\n",
    "            \"hyperparameters\": {\n",
    "                \"lr\": lr, \"adjusted_lr\": adjusted_lr, \"batch_size\": batch_size, \n",
    "                \"effective_batch_size\": effective_batch_size, \"optimizer\": optimizer_name,\n",
    "                \"weight_decay\": weight_decay, \"dropout_rate\": dropout_rate, \n",
    "                \"label_smoothing\": label_smoothing, \"use_augmentation\": use_augmentation,\n",
    "                \"augmentation_strength\": augmentation_strength, \"memory_optimized\": True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Rank {global_rank}: Training error: {e}\")\n",
    "        return {\"val_acc\": 0.0, \"status\": \"failed\", \"error\": str(e)}\n",
    "    \n",
    "    finally:\n",
    "        for cache_path in cache_paths:\n",
    "            if os.path.exists(cache_path):\n",
    "                shutil.rmtree(cache_path, ignore_errors=True)\n",
    "        if world_size > 1 and dist.is_initialized():\n",
    "            dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, criterion, optimizer, scheduler, train_dataloader, \n",
    "                   epoch, device, global_rank, label_to_idx,\n",
    "                   use_augmentation=True, augmentation_strength='medium',\n",
    "                   scaler=None):\n",
    "    \"\"\"Training loop \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    successful_batches = 0\n",
    "    gradient_norms = []\n",
    "    \n",
    "    # Enable mixed precision training\n",
    "    use_mixed_precision = scaler is not None\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        try:\n",
    "            # Apply augmentation during training\n",
    "            inputs, labels = convert_batch_to_tensors(\n",
    "                batch, device, label_to_idx, global_rank,\n",
    "                use_augmentation=use_augmentation,\n",
    "                augmentation_strength=augmentation_strength\n",
    "            )\n",
    "            \n",
    "            # Convert to channels_last for better memory efficiency\n",
    "            inputs = inputs.to(memory_format=torch.channels_last)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Use mixed precision if available\n",
    "            if use_mixed_precision:\n",
    "                # with torch.cuda.amp.autocast():\n",
    "                with torch.amp.autocast('cuda'):    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Scale loss and backward pass\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Calculate gradient norm before unscaling\n",
    "                if global_rank == 0 and step % 50 == 0:\n",
    "                    # Unscale gradients for norm calculation\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    grad_norm = calculate_gradient_norm(model, 1 if torch.distributed.get_world_size() == 1 else torch.distributed.get_world_size())\n",
    "                    gradient_norms.append(grad_norm)\n",
    "                \n",
    "                # Update with scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard precision training\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Calculate gradient norm\n",
    "                if global_rank == 0 and step % 50 == 0:\n",
    "                    grad_norm = calculate_gradient_norm(model, 1 if torch.distributed.get_world_size() == 1 else torch.distributed.get_world_size())\n",
    "                    gradient_norms.append(grad_norm)\n",
    "                \n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            successful_batches += 1\n",
    "            \n",
    "            # Log memory usage periodically\n",
    "            if global_rank == 0 and step % 100 == 0:\n",
    "                current_acc = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "                aug_status = f\"(aug: {augmentation_strength})\" if use_augmentation else \"(no aug)\"\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "                    memory_util = memory_allocated/memory_total*100\n",
    "                    print(f\"  Step {step}: Loss {loss.item():.4f}, Acc {current_acc:.4f} {aug_status}, \"\n",
    "                          f\"Memory {memory_util:.1f}%\")\n",
    "                else:\n",
    "                    print(f\"  Step {step}: Loss {loss.item():.4f}, Acc {current_acc:.4f} {aug_status}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            if step < 5:\n",
    "                print(f\"Training step {step} error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_loss = running_loss / successful_batches if successful_batches > 0 else 0.0\n",
    "    epoch_acc = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "    avg_gradient_norm = np.mean(gradient_norms) if gradient_norms else 0.0\n",
    "    \n",
    "    return {\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': epoch_acc,\n",
    "        'gradient_norm': avg_gradient_norm,\n",
    "        'weight_norm': calculate_weight_norm(model, 1 if not torch.distributed.is_initialized() else torch.distributed.get_world_size()),\n",
    "        'mixed_precision_used': use_mixed_precision\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, val_dataloader, epoch, device, global_rank, label_to_idx,\n",
    "            use_augmentation=False, augmentation_strength='light', scaler=None):\n",
    "    \"\"\"Evaluation loop \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    successful_batches = 0\n",
    "    \n",
    "    # Enable mixed precision for evaluation too\n",
    "    use_mixed_precision = scaler is not None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_dataloader):\n",
    "            try:\n",
    "                # Typically no augmentation during validation\n",
    "                inputs, labels = convert_batch_to_tensors(\n",
    "                    batch, device, label_to_idx, global_rank,\n",
    "                    use_augmentation=use_augmentation,\n",
    "                    augmentation_strength=augmentation_strength\n",
    "                )\n",
    "                \n",
    "                # Convert to channels_last for consistency\n",
    "                inputs = inputs.to(memory_format=torch.channels_last)\n",
    "                \n",
    "                # Use mixed precision if available\n",
    "                if use_mixed_precision:\n",
    "                    #with torch.cuda.amp.autocast():\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                successful_batches += 1\n",
    "                \n",
    "                # Log memory usage periodically during validation\n",
    "                if global_rank == 0 and step % 50 == 0 and torch.cuda.is_available():\n",
    "                    memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "                    memory_util = memory_allocated/memory_total*100\n",
    "                    if step == 0:  # Only log once per epoch\n",
    "                        print(f\"  Validation memory usage: {memory_util:.1f}%\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    val_loss = running_loss / successful_batches if successful_batches > 0 else 0.0\n",
    "    val_acc = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'loss': val_loss, \n",
    "        'accuracy': val_acc,\n",
    "        'mixed_precision_used': use_mixed_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24039e51-3eab-4e97-b9b1-1e2492f466df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "utils / helper functions"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_gradient_norm(model, world_size):\n",
    "    \"\"\"Calculate L2 norm of gradients\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    total_norm = 0.0\n",
    "    param_count = 0\n",
    "    \n",
    "    if world_size > 1:\n",
    "        parameters = model.module.parameters()\n",
    "    else:\n",
    "        parameters = model.parameters()\n",
    "    \n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "            param_count += 1\n",
    "    \n",
    "    return (total_norm ** 0.5) if param_count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_weight_norm(model, world_size):\n",
    "    \"\"\"Calculate L2 norm of model weights\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    total_norm = 0.0\n",
    "    param_count = 0\n",
    "    \n",
    "    if world_size > 1:\n",
    "        parameters = model.module.parameters()\n",
    "    else:\n",
    "        parameters = model.parameters()\n",
    "    \n",
    "    for p in parameters:\n",
    "        param_norm = p.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "        param_count += 1\n",
    "    \n",
    "    return (total_norm ** 0.5) if param_count > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb1bdab-f483-4528-bd40-5c5e635d2a62",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Objective Function"
    }
   },
   "outputs": [],
   "source": [
    "def objective_function(trial):\n",
    "    \"\"\"Define the objective function with essential hyperparameters\"\"\"\n",
    "    \n",
    "    # Core hyperparameters with UPDATED LARGER BATCH SIZES\n",
    "    lr = trial.suggest_float('lr', 5e-5, 5e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [128, 256, 384, 512])  # MUCH LARGER cf [32, 64, 128]\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['AdamW', 'SGD', 'Adam'])\n",
    "    \n",
    "    # Optimizer-specific weight decay\n",
    "    if optimizer_name == 'AdamW':\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-1, log=True)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
    "    else:\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "    \n",
    "    # Additional key hyperparameters\n",
    "    step_size = trial.suggest_int('step_size', 5, 15)\n",
    "    gamma = trial.suggest_float('gamma', 0.1, 0.7)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    label_smoothing = trial.suggest_float('label_smoothing', 0.0, 0.2)\n",
    "    \n",
    "    # Data augmentation hyperparameters\n",
    "    use_augmentation = trial.suggest_categorical('use_augmentation', [True, False])\n",
    "    \n",
    "    if use_augmentation:\n",
    "        augmentation_strength = trial.suggest_categorical('augmentation_strength', ['light', 'medium', 'heavy'])\n",
    "    else:\n",
    "        augmentation_strength = None\n",
    "    \n",
    "    # Optimizer-specific parameters\n",
    "    momentum = trial.suggest_float('momentum', 0.85, 0.99) if optimizer_name == 'SGD' else 0.9\n",
    "    nesterov = trial.suggest_categorical('nesterov', [True, False]) if optimizer_name == 'SGD' else False\n",
    "    beta1 = trial.suggest_float('beta1', 0.85, 0.95) if optimizer_name == 'AdamW' else 0.9\n",
    "    beta2 = trial.suggest_float('beta2', 0.95, 0.999) if optimizer_name == 'AdamW' else 0.999\n",
    "    eps = trial.suggest_float('eps', 1e-9, 1e-7, log=True) if optimizer_name == 'AdamW' else 1e-8\n",
    "    \n",
    "    # Calculate effective batch size for logging\n",
    "    effective_batch_size = batch_size * num_workers\n",
    "    \n",
    "    aug_info = f\"aug_{augmentation_strength}\" if use_augmentation else \"no_aug\"\n",
    "    print(f\"LARGE BATCH Trial {trial.number}: {optimizer_name}, lr={lr:.2e}, bs={batch_size}, \"\n",
    "          f\"eff_bs={effective_batch_size}, wd={weight_decay:.2e}, {aug_info}\")\n",
    "    \n",
    "    run_name = f\"trial_{trial.number}_{optimizer_name}_bs{batch_size}_lr{lr:.2e}_{aug_info}\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name, nested=True) as trial_run:\n",
    "        trial_start_time = time.time()\n",
    "        trial_run_id = trial_run.info.run_id\n",
    "        \n",
    "        # Log essential parameters including large batch info\n",
    "        trial_params = {\n",
    "            'trial_number': trial.number,\n",
    "            'lr': lr, 'batch_size': batch_size, 'effective_batch_size': effective_batch_size,\n",
    "            'optimizer': optimizer_name,\n",
    "            'weight_decay': weight_decay, 'step_size': step_size, 'gamma': gamma,\n",
    "            'dropout_rate': dropout_rate, 'label_smoothing': label_smoothing,\n",
    "            'momentum': momentum, 'nesterov': nesterov, 'beta1': beta1, 'beta2': beta2, 'eps': eps,\n",
    "            'use_augmentation': use_augmentation,\n",
    "            'augmentation_strength': str(augmentation_strength) if augmentation_strength else 'none',\n",
    "            'num_epochs': NUM_EPOCHS, 'num_workers': num_workers,\n",
    "            'model_architecture': 'mobilenetv2',\n",
    "            'large_batch_optimization': True,  # Flag for large batch training\n",
    "            'memory_optimized': True\n",
    "        }\n",
    "        mlflow.log_params(trial_params)\n",
    "        \n",
    "        try:\n",
    "            distributor = TorchDistributor(num_processes=num_workers, local_mode=False, use_gpu=True)\n",
    "            result = distributor.run(\n",
    "                distributed_train_and_evaluate,  # Using updated function with large batch support\n",
    "                lr=lr, batch_size=batch_size, optimizer_name=optimizer_name,\n",
    "                weight_decay=weight_decay, step_size=step_size, gamma=gamma,\n",
    "                dropout_rate=dropout_rate, label_smoothing=label_smoothing,\n",
    "                momentum=momentum, nesterov=nesterov, beta1=beta1, beta2=beta2, eps=eps,\n",
    "                data_storage_location=data_storage_location,\n",
    "                mds_train_dir=mds_train_dir, mds_val_dir=mds_val_dir,\n",
    "                num_epochs=NUM_EPOCHS, mlflow_run_id=trial_run_id,\n",
    "                use_augmentation=use_augmentation,\n",
    "                augmentation_strength=augmentation_strength\n",
    "            )\n",
    "            \n",
    "            trial_end_time = time.time()\n",
    "            trial_duration = trial_end_time - trial_start_time\n",
    "            \n",
    "            if isinstance(result, dict):\n",
    "                trial_metric = result.get('val_acc', 0.0)\n",
    "                \n",
    "                # Log essential model metrics\n",
    "                model_metrics = result.get('model_metrics', {})\n",
    "                if model_metrics:\n",
    "                    mlflow.log_metric(\"model_parameter_count\", model_metrics.get('parameter_count', 0))\n",
    "                    mlflow.log_metric(\"model_trainable_parameters\", model_metrics.get('trainable_parameters', 0))\n",
    "                    mlflow.log_metric(\"model_size_mb\", model_metrics.get('model_size_mb', 0.0))\n",
    "                \n",
    "                    # Log gradient and weight norms\n",
    "                    gradient_norms = model_metrics.get('gradient_norms', [])\n",
    "                    weight_norms = model_metrics.get('weight_norms', [])\n",
    "                    \n",
    "                    if gradient_norms:\n",
    "                        mlflow.log_metric(\"final_gradient_norm\", gradient_norms[-1])\n",
    "                        mlflow.log_metric(\"mean_gradient_norm\", np.mean(gradient_norms))\n",
    "                        mlflow.log_metric(\"gradient_stability\", 1.0 / (np.var(gradient_norms) + 1e-8))\n",
    "                    \n",
    "                    if weight_norms:\n",
    "                        mlflow.log_metric(\"final_weight_norm\", weight_norms[-1])\n",
    "                        mlflow.log_metric(\"mean_weight_norm\", np.mean(weight_norms))\n",
    "                \n",
    "                # Log epoch-by-epoch metrics\n",
    "                epoch_metrics = result.get('epoch_metrics', [])\n",
    "                if epoch_metrics:\n",
    "                    for epoch_data in epoch_metrics:\n",
    "                        epoch = epoch_data['epoch']\n",
    "                        mlflow.log_metric(\"train_loss_epoch\", epoch_data['train_loss'], step=epoch)\n",
    "                        mlflow.log_metric(\"train_accuracy_epoch\", epoch_data['train_acc'], step=epoch)\n",
    "                        mlflow.log_metric(\"val_loss_epoch\", epoch_data['val_loss'], step=epoch)\n",
    "                        mlflow.log_metric(\"val_accuracy_epoch\", epoch_data['val_acc'], step=epoch)\n",
    "                        mlflow.log_metric(\"learning_rate_epoch\", epoch_data['learning_rate'], step=epoch)\n",
    "                        mlflow.log_metric(\"gradient_norm_epoch\", epoch_data['gradient_norm'], step=epoch)\n",
    "                \n",
    "                # Log large batch training metrics\n",
    "                training_metrics = {\n",
    "                    'final_validation_accuracy': trial_metric,\n",
    "                    'final_train_loss': result.get('train_loss', 0.0),\n",
    "                    'final_val_loss': result.get('val_loss', 0.0),\n",
    "                    'final_train_accuracy': result.get('train_acc', 0.0),\n",
    "                    'best_validation_accuracy': result.get('best_val_acc', trial_metric),\n",
    "                    'epochs_completed': result.get('epochs_completed', NUM_EPOCHS),\n",
    "                    'training_time_seconds': result.get('training_time', 0.0),\n",
    "                    'trial_duration_seconds': trial_duration,\n",
    "                    # Large batch specific metrics\n",
    "                    'large_batch_size': float(batch_size),\n",
    "                    'effective_batch_size': float(effective_batch_size),\n",
    "                    'memory_efficiency_score': trial_metric / (batch_size / 100),  # Accuracy per 100 batch units\n",
    "                    'throughput_samples_per_second': (50000 * num_workers * NUM_EPOCHS) / trial_duration if trial_duration > 0 else 0.0\n",
    "                }\n",
    "                mlflow.log_metrics(training_metrics)\n",
    "                \n",
    "                # Log augmentation-specific metrics\n",
    "                if use_augmentation:\n",
    "                    mlflow.log_metric(\"augmentation_enabled\", 1.0)\n",
    "                    mlflow.log_metric(\"augmentation_impact_score\", trial_metric)\n",
    "                else:\n",
    "                    mlflow.log_metric(\"augmentation_enabled\", 0.0)\n",
    "                \n",
    "                # Log efficiency metrics with large batch considerations\n",
    "                if trial_duration > 0:\n",
    "                    total_samples = 50000 * num_workers * NUM_EPOCHS\n",
    "                    samples_per_second = total_samples / trial_duration\n",
    "                    mlflow.log_metric('samples_per_second', samples_per_second)\n",
    "                    mlflow.log_metric('time_per_epoch_seconds', trial_duration / NUM_EPOCHS)\n",
    "                    mlflow.log_metric('samples_per_second_per_batch_size', samples_per_second / batch_size)\n",
    "                \n",
    "                # Log convergence metrics\n",
    "                if epoch_metrics and len(epoch_metrics) > 1:\n",
    "                    val_accs = [ep['val_acc'] for ep in epoch_metrics]\n",
    "                    final_acc = val_accs[-1]\n",
    "                    initial_acc = val_accs[0]\n",
    "                    improvement = final_acc - initial_acc\n",
    "                    \n",
    "                    mlflow.log_metric(\"total_accuracy_improvement\", improvement)\n",
    "                    mlflow.log_metric(\"accuracy_improvement_rate\", improvement / len(val_accs))\n",
    "                    \n",
    "                    # Training stability\n",
    "                    if len(val_accs) >= 3:\n",
    "                        recent_stability = 1.0 / (np.var(val_accs[-3:]) + 1e-8)\n",
    "                        mlflow.log_metric(\"validation_stability\", recent_stability)\n",
    "                        \n",
    "            else:\n",
    "                trial_metric = 0.0\n",
    "                mlflow.log_metric(\"final_validation_accuracy\", trial_metric)\n",
    "            \n",
    "            # Set essential tags including large batch info\n",
    "            performance_tier = \"excellent\" if trial_metric > 0.7 else \"good\" if trial_metric > 0.5 else \"fair\" if trial_metric > 0.3 else \"poor\"\n",
    "            augmentation_tag = f\"aug_{augmentation_strength}\" if use_augmentation else \"no_augmentation\"\n",
    "            \n",
    "            mlflow.set_tags({\n",
    "                \"trial_number\": str(trial.number),\n",
    "                \"optimizer\": optimizer_name,\n",
    "                \"performance_tier\": performance_tier,\n",
    "                \"augmentation_strategy\": augmentation_tag,\n",
    "                \"trial_status\": \"completed\",\n",
    "                \"large_batch_training\": \"true\",\n",
    "                \"batch_size\": str(batch_size),\n",
    "                \"effective_batch_size\": str(effective_batch_size),\n",
    "                \"memory_optimized\": \"true\"\n",
    "            })\n",
    "            \n",
    "            print(f\"Large Batch Trial {trial.number} completed: Accuracy = {trial_metric:.4f}, \"\n",
    "                  f\"Batch Size = {batch_size}, Duration = {trial_duration:.1f}s, Aug = {aug_info}\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(f\"OOM Error with large batch_size={batch_size}: {e}\")\n",
    "                trial_metric = 0.0\n",
    "                mlflow.log_param(\"oom_error\", f\"batch_size_{batch_size}\")\n",
    "                mlflow.set_tag(\"trial_status\", \"oom_failed\")\n",
    "            else:\n",
    "                raise\n",
    "                \n",
    "        except Exception as e:\n",
    "            trial_end_time = time.time()\n",
    "            trial_duration = trial_end_time - trial_start_time\n",
    "            \n",
    "            print(f\"Large Batch Trial {trial.number} failed: {e}\")\n",
    "            trial_metric = 0.0\n",
    "            \n",
    "            mlflow.log_param(\"trial_status\", \"failed\")\n",
    "            mlflow.log_param(\"error_message\", str(e)[:200])\n",
    "            mlflow.log_metric(\"final_validation_accuracy\", trial_metric)\n",
    "            mlflow.log_metric(\"trial_duration_seconds\", trial_duration)\n",
    "            \n",
    "            augmentation_tag = f\"aug_{augmentation_strength}\" if use_augmentation else \"no_augmentation\"\n",
    "            mlflow.set_tags({\n",
    "                \"trial_number\": str(trial.number),\n",
    "                \"trial_status\": \"failed\",\n",
    "                \"optimizer\": optimizer_name,\n",
    "                \"augmentation_strategy\": augmentation_tag,\n",
    "                \"large_batch_training\": \"true\"\n",
    "            })\n",
    "    \n",
    "    return trial_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08523cc7-f120-4b09-b789-3b136afe1b55",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "[optional] update Configs to test"
    }
   },
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "# num_classes = 200\n",
    "NUM_EPOCHS =  5  # Test with just 1 epoch first if needed\n",
    "num_workers = 2  # Reduce to 2 workers to decrease I/O contention\n",
    "\n",
    "# n_trials = 5 ## can be increased \n",
    "# batch_sizes = [8, 16] -- update parameter-ranges/etc\n",
    "# optimizers = ['AdamW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5386290-7278-4991-a483-fc917b6b231b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "### spark config auto-detect?"
    }
   },
   "outputs": [],
   "source": [
    "def setup_spark_for_optuna():\n",
    "    \"\"\"Setup Spark with appropriate GPU configuration for Optuna\"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "    except ImportError:\n",
    "        gpu_available = False\n",
    "    \n",
    "    gpu_amount = \"1\" if gpu_available else \"0\"\n",
    "    \n",
    "    # Try to update existing session first\n",
    "    try:\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark:\n",
    "            spark.conf.set(\"spark.task.resource.gpu.amount\", gpu_amount)\n",
    "            return spark\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Create new session if needed\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"OptunaTuning\") \\\n",
    "        .config(\"spark.task.resource.gpu.amount\", gpu_amount) \\\n",
    "        .config(\"spark.executor.resource.gpu.amount\", gpu_amount) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark = setup_spark_for_optuna()\n",
    "# study.optimize(objective_function, n_trials=5, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3639dff9-ec85-49ef-9679-9ee2fe6b3ea9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optimization Run"
    }
   },
   "outputs": [],
   "source": [
    "## Run optimization with data augmentation tracking\n",
    "start_time = time.time()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    # study_name=f\"pytorch_mobilenetv2_aug_{int(time.time())}\",  \n",
    "    study_name = f\"pytorch_mobilenetv2_aug_{datetime.now().strftime('%Y%m%d_%H%M%S')}\" ,\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "print(\"Starting Optuna optimization with  BATCH SIZES and data augmentation...\")\n",
    "print(f\"Trials: 5, Workers: {num_workers}, Epochs: {NUM_EPOCHS}\")\n",
    "print(f\" BATCH SIZES: [128, 256, 384, 512] \") #cf [32, 64, 128]\n",
    "\n",
    "with mlflow.start_run(run_name=f\"optuna_optimization_{study.study_name}\") as parent_run:\n",
    "    parent_run_id = parent_run.info.run_id\n",
    "    \n",
    "    # Log optimization configuration including large batch info\n",
    "    mlflow.log_params({\n",
    "        \"study_name\": study.study_name,\n",
    "        \"n_trials\": 5,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"model_architecture\": \"mobilenetv2\",\n",
    "        \"optimization_type\": \"large_batch_with_augmentation\",\n",
    "        \"augmentation_enabled\": False,  # You can change this to True\n",
    "        \"large_batch_optimization\": True, # update as appropriate\n",
    "        \"batch_size_range\": \"[128, 256, 384, 512]\",\n",
    "        \"memory_optimization\": True\n",
    "    })\n",
    "    \n",
    "    print(f\"Parent run ID: {parent_run_id}\")\n",
    "    \n",
    "    # Run optimization with updated objective function ## alternative way to detect?\n",
    "    study.optimize(objective_function, n_trials=5, n_jobs=1)\n",
    "    \n",
    "    # Log results\n",
    "    end_time = time.time()\n",
    "    optimization_duration = end_time - start_time\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value\n",
    "    \n",
    "    completed_trials = [t for t in study.trials if t.state.name == \"COMPLETE\" and t.value is not None]\n",
    "    trial_values = [t.value for t in completed_trials]\n",
    "    \n",
    "    print(f\"\\nBest parameters: {best_params}\")\n",
    "    print(f\"Best validation accuracy: {best_value:.4f}\")\n",
    "    print(f\"Best batch size: {best_params.get('batch_size', 'N/A')}\")\n",
    "    print(f\"Completed trials: {len(completed_trials)}/{len(study.trials)}\")\n",
    "    \n",
    "    # Analyze batch size impact\n",
    "    batch_size_analysis = {}\n",
    "    for trial in completed_trials:\n",
    "        bs = trial.params.get('batch_size', 'unknown')\n",
    "        if bs not in batch_size_analysis:\n",
    "            batch_size_analysis[bs] = []\n",
    "        batch_size_analysis[bs].append(trial.value)\n",
    "    \n",
    "    if batch_size_analysis:\n",
    "        print(f\"\\nBatch Size Performance Analysis:\")\n",
    "        for bs, values in batch_size_analysis.items():\n",
    "            avg_acc = np.mean(values)\n",
    "            print(f\"Batch size {bs}: {len(values)} trials, avg accuracy: {avg_acc:.4f}\")\n",
    "    \n",
    "    # Analyze augmentation impact\n",
    "    aug_trials = [t for t in completed_trials if t.params.get('use_augmentation', False)]\n",
    "    no_aug_trials = [t for t in completed_trials if not t.params.get('use_augmentation', True)]\n",
    "    \n",
    "    aug_values = [t.value for t in aug_trials] if aug_trials else []\n",
    "    no_aug_values = [t.value for t in no_aug_trials] if no_aug_trials else []\n",
    "    \n",
    "    print(f\"\\nAugmentation Analysis:\")\n",
    "    print(f\"Trials with augmentation: {len(aug_trials)}\")\n",
    "    print(f\"Trials without augmentation: {len(no_aug_trials)}\")\n",
    "    \n",
    "    if aug_values:\n",
    "        print(f\"Avg accuracy with augmentation: {np.mean(aug_values):.4f}\")\n",
    "    if no_aug_values:\n",
    "        print(f\"Avg accuracy without augmentation: {np.mean(no_aug_values):.4f}\")\n",
    "    \n",
    "    # Analyze augmentation strength impact\n",
    "    strength_analysis = {}\n",
    "    for trial in aug_trials:\n",
    "        strength = trial.params.get('augmentation_strength', 'unknown')\n",
    "        if strength not in strength_analysis:\n",
    "            strength_analysis[strength] = []\n",
    "        strength_analysis[strength].append(trial.value)\n",
    "    \n",
    "    if strength_analysis:\n",
    "        print(f\"\\nAugmentation Strength Analysis:\")\n",
    "        for strength, values in strength_analysis.items():\n",
    "            print(f\"{strength}: {len(values)} trials, avg accuracy: {np.mean(values):.4f}\")\n",
    "    \n",
    "    # Log essential optimization results with batch metrics\n",
    "    optimization_results = {\n",
    "        \"best_validation_accuracy\": float(best_value),\n",
    "        \"best_batch_size\": float(best_params.get('batch_size', 0)),\n",
    "        \"best_effective_batch_size\": float(best_params.get('batch_size', 0) * num_workers),\n",
    "        \"total_trials\": float(len(study.trials)),\n",
    "        \"completed_trials\": float(len(completed_trials)),\n",
    "        \"optimization_duration_minutes\": float(optimization_duration / 60.0),\n",
    "        \"success_rate\": float(len(completed_trials) / len(study.trials)) if len(study.trials) > 0 else 0.0,\n",
    "        \"large_batch_optimization\": 1.0,\n",
    "        \"memory_optimization_enabled\": 1.0\n",
    "    }\n",
    "    \n",
    "    # Log batch size effectiveness\n",
    "    if batch_size_analysis:\n",
    "        for bs, values in batch_size_analysis.items():\n",
    "            if isinstance(bs, (int, float)):\n",
    "                mlflow.log_metric(f\"avg_accuracy_batch_size_{bs}\", float(np.mean(values)))\n",
    "                mlflow.log_metric(f\"trials_batch_size_{bs}\", float(len(values)))\n",
    "                mlflow.log_metric(f\"memory_efficiency_batch_size_{bs}\", float(np.mean(values) / (bs / 100)))\n",
    "    \n",
    "    # Log augmentation-specific results with proper type conversion\n",
    "    augmentation_results = {\n",
    "        \"trials_with_augmentation\": float(len(aug_trials)),\n",
    "        \"trials_without_augmentation\": float(len(no_aug_trials)),\n",
    "        \"augmentation_usage_rate\": float(len(aug_trials) / len(completed_trials)) if completed_trials else 0.0\n",
    "    }\n",
    "    \n",
    "    if aug_values:\n",
    "        augmentation_results.update({\n",
    "            \"avg_accuracy_with_augmentation\": float(np.mean(aug_values)),\n",
    "            \"std_accuracy_with_augmentation\": float(np.std(aug_values)),\n",
    "            \"best_accuracy_with_augmentation\": float(np.max(aug_values))\n",
    "        })\n",
    "    \n",
    "    if no_aug_values:\n",
    "        augmentation_results.update({\n",
    "            \"avg_accuracy_without_augmentation\": float(np.mean(no_aug_values)),\n",
    "            \"std_accuracy_without_augmentation\": float(np.std(no_aug_values)),\n",
    "            \"best_accuracy_without_augmentation\": float(np.max(no_aug_values))\n",
    "        })\n",
    "    \n",
    "    # Calculate augmentation effectiveness with proper type conversion\n",
    "    if aug_values and no_aug_values:\n",
    "        aug_improvement = np.mean(aug_values) - np.mean(no_aug_values)\n",
    "        augmentation_results[\"augmentation_improvement\"] = float(aug_improvement)\n",
    "        augmentation_results[\"augmentation_effective\"] = 1.0 if aug_improvement > 0 else 0.0\n",
    "        print(f\"Augmentation improvement: {aug_improvement:+.4f}\")\n",
    "    \n",
    "    # Log augmentation strength effectiveness - handle string values properly\n",
    "    if strength_analysis:\n",
    "        best_strength = max(strength_analysis.items(), key=lambda x: np.mean(x[1]))\n",
    "        \n",
    "        # Log the string value as a parameter, not a metric\n",
    "        mlflow.log_param(\"best_augmentation_strength\", str(best_strength[0]))\n",
    "        \n",
    "        # Log the numeric accuracy as a metric\n",
    "        augmentation_results[\"best_strength_accuracy\"] = float(np.mean(best_strength[1]))\n",
    "        \n",
    "        for strength, values in strength_analysis.items():\n",
    "            # Use safe metric names (replace special characters)\n",
    "            safe_strength = str(strength).replace(' ', '_').replace('-', '_')\n",
    "            mlflow.log_metric(f\"avg_accuracy_strength_{safe_strength}\", float(np.mean(values)))\n",
    "            mlflow.log_metric(f\"trials_strength_{safe_strength}\", float(len(values)))\n",
    "    \n",
    "    # Log best hyperparameters including batch size info\n",
    "    for param_name, param_value in best_params.items():\n",
    "        mlflow.log_param(f\"best_{param_name}\", param_value)\n",
    "    \n",
    "    # Log effective batch size separately\n",
    "    if 'batch_size' in best_params:\n",
    "        mlflow.log_param(\"best_effective_batch_size\", best_params['batch_size'] * num_workers)\n",
    "    \n",
    "    # Now safe to log metrics (all values are guaranteed to be numeric)\n",
    "    mlflow.log_metrics(optimization_results)\n",
    "    mlflow.log_metrics(augmentation_results)\n",
    "    \n",
    "    # Log trial statistics with proper type conversion\n",
    "    if trial_values:\n",
    "        trial_stats = {\n",
    "            \"mean_trial_accuracy\": float(np.mean(trial_values)),\n",
    "            \"std_trial_accuracy\": float(np.std(trial_values)),\n",
    "            \"min_trial_accuracy\": float(np.min(trial_values)),\n",
    "            \"max_trial_accuracy\": float(np.max(trial_values))\n",
    "        }\n",
    "        mlflow.log_metrics(trial_stats)\n",
    "    \n",
    "    # Log parameter importance including batch size\n",
    "    try:\n",
    "        if len(completed_trials) > 1:\n",
    "            importance = optuna.importance.get_param_importances(study)\n",
    "            for param_name, importance_value in importance.items():\n",
    "                mlflow.log_metric(f\"param_importance_{param_name}\", float(importance_value))\n",
    "            \n",
    "            most_important = max(importance.items(), key=lambda x: x[1])\n",
    "            mlflow.log_param(\"most_important_param\", str(most_important[0]))\n",
    "            mlflow.log_metric(\"most_important_param_score\", float(most_important[1]))\n",
    "            \n",
    "            # Check batch size importance\n",
    "            batch_size_importance = importance.get('batch_size', 0.0)\n",
    "            if batch_size_importance > 0:\n",
    "                mlflow.log_metric(\"batch_size_importance\", float(batch_size_importance))\n",
    "                print(f\"Batch size parameter importance: {batch_size_importance:.4f}\")\n",
    "            \n",
    "            # Check if augmentation parameters are important\n",
    "            aug_params = ['use_augmentation', 'augmentation_strength']\n",
    "            aug_importance = {k: v for k, v in importance.items() if k in aug_params}\n",
    "            if aug_importance:\n",
    "                total_aug_importance = sum(aug_importance.values())\n",
    "                mlflow.log_metric(\"total_augmentation_importance\", float(total_aug_importance))\n",
    "                print(f\"Augmentation parameter importance: {total_aug_importance:.4f}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        mlflow.log_param(\"importance_error\", str(e))\n",
    "    \n",
    "    # Set final tags including large batch and augmentation info\n",
    "    best_aug_strategy = \"no_augmentation\"\n",
    "    if best_params.get('use_augmentation', False):\n",
    "        best_aug_strategy = f\"aug_{best_params.get('augmentation_strength', 'unknown')}\"\n",
    "    \n",
    "    # Ensure augmentation_effective is numeric before using in tags\n",
    "    aug_effective_value = augmentation_results.get(\"augmentation_effective\", -1.0)\n",
    "    if isinstance(aug_effective_value, str):\n",
    "        aug_effective_str = aug_effective_value\n",
    "    else:\n",
    "        aug_effective_str = \"true\" if aug_effective_value > 0 else \"false\"\n",
    "    \n",
    "    mlflow.set_tags({\n",
    "        \"optimization_type\": \"large_batch_with_augmentation\",\n",
    "        \"model_architecture\": \"mobilenetv2\",\n",
    "        \"best_optimizer\": best_params.get('optimizer', 'unknown'),\n",
    "        \"best_batch_size\": str(best_params.get('batch_size', 'unknown')),\n",
    "        \"best_effective_batch_size\": str(best_params.get('batch_size', 0) * num_workers),\n",
    "        \"best_augmentation_strategy\": best_aug_strategy,\n",
    "        \"augmentation_in_best\": str(best_params.get('use_augmentation', False)),\n",
    "        \"large_batch_optimization\": \"true\",\n",
    "        \"memory_optimized\": \"true\",\n",
    "        \"optimization_status\": \"completed\",\n",
    "        \"total_trials\": str(len(study.trials)),\n",
    "        \"best_accuracy\": f\"{best_value:.4f}\",\n",
    "        \"augmentation_effective\": aug_effective_str\n",
    "    })\n",
    "\n",
    "print(f\"\\nOptimization completed in {optimization_duration:.2f} seconds\")\n",
    "print(f\"Parent run ID: {parent_run_id}\")\n",
    "\n",
    "# Print final summary including batch size info\n",
    "print(f\"\\n OPTIMIZATION SUMMARY:\")\n",
    "print(f\"Best batch size: {best_params.get('batch_size', 'N/A')}\")\n",
    "print(f\"Effective batch size: {best_params.get('batch_size', 0) * num_workers}\")\n",
    "print(f\"Best optimizer: {best_params.get('optimizer', 'N/A')}\")\n",
    "\n",
    "if best_params.get('use_augmentation', False):\n",
    "    print(f\"Best trial used augmentation: {best_params.get('augmentation_strength', 'unknown')}\")\n",
    "else:\n",
    "    print(\"Best trial did not use augmentation\")\n",
    "\n",
    "if 'augmentation_improvement' in augmentation_results:\n",
    "    improvement = augmentation_results['augmentation_improvement']\n",
    "    if improvement > 0:\n",
    "        print(f\"Augmentation provided {improvement:.4f} average improvement\")\n",
    "    else:\n",
    "        print(f\"Augmentation showed {improvement:.4f} average impact (negative)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac26115c-f9f9-44e3-a9b5-5c5be8164961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e0e941-433e-4839-86fa-6836ae986df0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: \n",
    "# - `{top-1} validation accuracy` is typically standard for Tiny ImageNet classification. Here we are using validation accuracy for this example. If your use case requires a different metric (e.g., `top-5 accuracy` or `F1`), do feel free to modify the objective_function to return and log that metric instead.\n",
    "\n",
    "# - `Encountering py4j errors and failures` when scaling to more workers (e.g. 4 that the compute can scale up to): these are likely due to several factors specific to distributed PyTorch with Optuna on Databricks:\n",
    "\n",
    "#     1. **Resource Contention**: Each worker creates its own MDS cache directories and data loaders. With 4 workers, you're creating 4x the I/O operations, leading to:\n",
    "#         - File system contention on `/local_disk0/tmp/mds_cache_*` directories\n",
    "#         - Network bandwidth saturation when accessing MDS data\n",
    "#         - Memory pressure from multiple concurrent data loading processes\n",
    "\n",
    "#     2. **Spark Context Conflicts**: Optuna trials run in parallel, and each trial attempts to initialize TorchDistributor, which can cause Spark context conflicts when multiple trials try to claim the same executor resources.\n",
    "\n",
    "#     3. **Port/Resource Binding**: Distributed PyTorch requires specific ports for inter-process communication. With more workers, port conflicts become more likely.\n",
    "\n",
    "# - **Some Heuristics**        \n",
    "#     1. **Start Conservative**: Begin with 2 workers, validate stability, then scale incrementally    \n",
    "#     2. **Monitor Resources**: Track CPU, memory, and I/O utilization during trials    \n",
    "#     3. **Use Pruning**: Implement early stopping to free resources for promising trials    \n",
    "#     4. **Stagger Trial Starts**: Add small delays between trial initiations to reduce resource conflicts\n",
    "#     5. **implementing proper resource isolation and coordination strategies.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd7fd2a0-504f-4142-a674-55124ff34d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_65bc13ea-276c-4905-a728-9fe2fb1780e2",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "imgnet_mbnetv2_TorchDistr_Optuna_mlflow_v0.1 (dataAug)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
